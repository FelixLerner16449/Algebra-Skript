\documentclass[twoside = false,	%a4paper, 		% paper size
		%12pt, 			% font size
% 		BCOR = \usrBCOR,	% binding correction
% 		DIV=15, 		% used for typearea calculation
		headsepline,		% add head line to border calculation
% 		oneside, 		% one- or twosided document
% 		footnotes = multiple,	% visually separate two consecutive footnotes
% 		toc = index,		% add index to table of contents
% 		numbers = auto,		% automatic placing of end dot in numbering
% 		pagesize,		% used for flexibility
		%twocolumn		
		parskip = true,
 		%draft
		]{scrbook}						% KOMA book class
\title{Algebra Skript}	
\author{Elias Cleusters; Felix Lerner}					
\input{settings/preambleBook}	% general settings and packages
\input{settings/abbreviations}		    % predefined macros
\input{settings/environmentsbook:eng}		    % predefined environments 
\usepackage[top=3cm,bottom=3cm,left=2cm,right=2cm,marginparwidth=1cm]{geometry}

\usepackage[english]{babel}             %language declaration
\setlength{\parskip}{5pt}
\addbibresource{literatur.bib}

\nc\rk{\text{rank}}

\begin{document}

%titlepage
\thispagestyle{plain}
\begin{titlepage}
	\centering
	
    {\rule{15cm}{3pt}\par}
    \vspace*{-5.5mm}
    {\rule{15cm}{1pt} \par}
    
	{\scshape\Huge Algebra \par}
	
    \vspace*{-2.5mm}
    {\rule{15cm}{1pt}\par}
    \vspace*{-4.8mm}
    {\rule{15cm}{3pt}\par}
    
	\vspace{2cm}
	
	{\scshape\Large Dr. Xin Fang \par}
	
	\vspace{1.5cm}
	
	{\Large read during \\
    Summer Term 2023 \par}
    
    \vspace{3cm}
    {\Large written by \\
    \itshape Elias Cleusters \\
    and Felix Lerner}
    
	\vfill
	
	\begin{figure}[ht]
        \centering
        \includegraphics[scale=0.27]{img/rwth_art_en_cmyk.jpg}
    \end{figure}
\end{titlepage}
\clearpage

\setcounter{page}{1}                           %to not count the titlepage
\tableofcontents                               %creates a table of contents 
\chapter{Galois theory}
    The goal of this chapter can be summarized as follows:
    
    Given a polynomial $f(x) = x^n + a_{n-1}x^{n-1} + \dots + a_1x + x_0 \in K[x]$, $K \in \{\Q,\R\}$, determine whether it can be solved by radicals. That means all roots of $f(x)$ can expressed in terms of its coefficients only using $+,-,\cdot,$ \textdiv, $\sqrt[k]{\cdot}$.

    For $\deg f(x) = 2$, we have the $pq$-formula and for $\deg f(x) \in \{3,4\}$ there are the Cardano-Ferrari formulae. But if $\deg f(x) \geq 5$ for a general $f(x)$ the answer is \textit{no} (Abel-Ruffini theorem). Later Galois gave a criterion on the solvability by radicals using the solvability of the Galois group of the polynomial.
    
    Although nowadays people call it Galois' theorem, the notion of groups is introduced only 40 years after Galois' work. It took people about $100$ years to write Galois' work in the current form.
    We state and prove Galois' theorem in this chapter.
    
\section{Galois extensions}
    \subsection{Fields and extensions}
        We briefly recall what we have done in the Cobra lecture \cite{Cobra} last semester.
        
        In Algebra people study algebraic structures (like groups, rings, fields, vector spaces) and the maps between them preserving these structures (grouphom., ringhom., etc.). For fields, such maps are field extensions.
        
        A \textbf{field} $K$ is a commutative ring with exactly two ideals ($\{0\}$ and $K$).
        Given two fields $K$ and $L$, a \textbf{field homomorphism} between them is a ring homomorphism $\varphi: K \rightarrow L$. The kernel of such a field homomorphism is a ($K$-)ideal in the ring $K$, hence either $\ker(\varphi) = \{0\}$ or $\ker(\varphi) = K$. That is to say, either $\varphi$ is the zero map, or $\varphi$ is injective.
        
        To summarize: For $\varphi \neq 0$, $\varphi(K)$ is a subfield of $L$ and $L/\varphi(K)$ is a field extension.
        
        Let $L/K$ be a field extension. The $L$ is naturally a $K$-vector space from the inclusion $K \subseteq L$. Such an extension is called \textbf{finite} if $\dim_K L < \infty$. Moreover, $L$ is also a $K$-algebra.
        
        Let $L_1/K$ and $L_2/K$ be two extensions of $K$. The homomorphisms between the two extensions are exactly the $K$-algebra homomorphisms $\varphi: L_1 \rightarrow L_2$. We denote by $\Hom_K(L_1,L_2)$ the set of all such homomorphisms, and we will call them simply $K$-homomorphisms. Similarly, we have the notion of $K$-isomorphisms.
        
        \begin{exercise}{}{1}
                Let $L_1/K, \, L_2/K$ be two field extensions of $K$. Show that:
                $\varphi \in \Hom_K(L_1,L_2)$ iff (if and only if) $\varphi:L_1 \rightarrow L_2$ is a field homomorphism such that $\forall \alpha \in K: \varphi(\alpha) = \alpha$.
        \end{exercise}
        \begin{proof}
            If $\varphi$ is a $K$-algebra homomorphism, then $\varphi$ is also $K$-linear which means $\varphi(k) = k\varphi(1) = k$ for all $k \in K$. Conversely if $\varphi(\alpha)=\alpha$ for all $\alpha \in K$, then $\varphi$ is $K$-linear since $\varphi(\alpha l+l') = \varphi(\alpha) \varphi(l) + \varphi(l') = \alpha \varphi(l) + \varphi(l')$. Thus $\varphi$ is a $K$-algebra homomorphism.
        \end{proof}
        
        \begin{definition*}{Galois group}
                For a field extension $L/K$ we denote the \textbf{Galois group} of the field extension $L/K$ as $\Gal{L/K}:= \{\varphi \in \Hom_K(L,L) \ | \ \varphi \ K\text{-isomorphism}\}$. Together with the composition of maps $\Gal{L/K}$ is a group.
        \end{definition*}
        \textbf{Examples:}
            \begin{enumerate}
                \item $\Gal{\C/\R} = \{\id,\sigma\} \left(\cong \Z/2\Z\right)$, where $\sigma(z) = \overline{z}$
                \item $\Gal{\Q\round{\sqrt[3]{2}}/\Q} = \{\id\}$
            \end{enumerate}
        
    \subsection{Algebraic extensions}
        Given a polynomial $f(x) \in K[x]$ we are interested in the symmetries between the roots of $f(x)$. Therefore we need a field containing roots of $f(x)$. Such a field can be obtained from adding all roots of $f(x)$, one after the other to the field $K$.
        
        We describe the procedure of "adjoining a root":
        Let $L/K$ be a field extension and $\alpha \in L$. The point is to consider the evaluation map
        \begin{align*}
            \mathrm{ev}_\alpha: K[x] &\rightarrow L \\
                                f(x) &\mapsto f(\alpha)
        \end{align*}
        The map $\mathrm{ev}_\alpha$ is a ring homomorphism. By the isomorphism theorem, we get:
        \begin{equation*}
            \faktor{K[x]}{\Ker(\ev_\alpha)} \cong \Img(\ev_\alpha)
        \end{equation*}
        The kernel $\ker(\ev_\alpha)$ is a $K-$ideal in $K[x]$, which is a PID (principal ideal domain), hence either there exists a monic polynomial $p_\alpha(x) \in K[x]$ (monic means $\LK(p_\alpha(x))=1$) of positive degree such that $\Ker(\ev_\alpha) = (p_\alpha(x))$ or $\ker(\ev_\alpha) = \{0\}$.
        
        In the second case $\alpha$ is not a root of any polynomial in $K[x]$, hence the set $\{1,\alpha,\alpha^2,\dots\}$ is linearly independent over $K$ and thus the $K$-vector space $L$ is of infinite dimension. In this case, we say $\alpha$ is \textbf{transcendental} over $K$.
        
        In the first case, the image $\Img(\ev_\alpha)$ is a subring of the field $L$, so it is an integral domain (i.e. nonzero, commutative and zero divisor free) and the ideal $(p_\alpha(x))$ is prime, which means that the polynomial $p_\alpha(x)$ is prime and hence irreducible. This monic irreducible polynomial is called the \textbf{minimal polynomial} of $\alpha$ over $K$, which we used to denote by $\Irr(\alpha,K)$ in Cobra.
        
        Moreover, in a principal ideal ring prime ideals are maximal. So $\Img(\ev_\alpha)$ is a field. It is the smallest (w.r.t. set inclusion) field in $L$ containing $K$ and $\alpha$. We denote it by $K(\alpha)$ or $K[\alpha]$. In this case, $\alpha$ is called algebraic over $K$. The $K$-vector space $K(\alpha)$ is finite dimensional with basis $1,\alpha,\alpha^2,\dots,\alpha^{m-1}$, where $m=\deg\left(p_\alpha(x)\right)$. That is to say $\dim_K K(\alpha) = m$.
        
        Generalising this notation: for a subset $S \subseteq L$ we will denote $K(S)$ as the smallest (w.r.t. set inclusion) field in $L$ containing $K$ and $S$. If $S$ is a finite set we call $K(S)$ finitely generated.
        
        For a finite extension $L/K$ any element $\alpha \in L$ is algebraic over $K$ since $\dim_K K(\alpha) \leq \dim_K(L) < \infty$, so we are never in the second case above.
        
        In general the extension $L/K$ is finite iff there exist $\alpha_1,\dots,\alpha_k \in L$ such that $L = K(\alpha_1,\dots,\alpha_k)$.
        
        A field extension $L/K$ is called algebraic, if all elements in $L$ are algebraic over $K$. Any finite extension is algebraic, but not vice versa. For example take $S = \left\{\sqrt{p} \ | \ p \text{ prime} \right\} \subseteq \R$ and consider the field extension $\Q(S)/\Q$.
        
        \begin{exercise}{}{2}
            Show that $\Q(S)/\Q$ is algebraic but not finite.
        \end{exercise}
        
        \begin{proof}
            Let $p_1,\dots,p_n$ be distinct prime numbers. We show that $[\Q(\sqrt{p_1},\dots,\sqrt{p_s}):\Q] = 2^s$. For this, we proceed with induction on $s$.
            
            If $s=1$, then $\sqrt{p_1}$ has minimal polynomial $x^2-p_1$, which is irreducible by the Eisenstein criterion. Thus $[\Q(\sqrt{p_1}):\Q] = 2$.
            
            Let us assume we proved the assertion for $s-1 \in \N$. We have:
            \begin{align*}
                [\Q(\sqrt{p_1},\dots,\sqrt{p_s}):\Q] &= [\Q(\sqrt{p_1},\dots,\sqrt{p_s}):\Q(\sqrt{p_1},\dots,\sqrt{p_{s-1}})] \cdot [\Q(\sqrt{p_1},\dots,\sqrt{p_{s-1}}):\Q] \\
                &\overset{\text{IH}}{=} [\Q(\sqrt{p_1},\dots,\sqrt{p_s}):\Q(\sqrt{p_1},\dots,\sqrt{p_{s-1}})] \cdot 2^{s-1}
            \end{align*}
            By induction, we get for all $1<i<s$:
            \begin{equation}\tag{$\star$}\label{*}
                [\Q(\sqrt{p_1},\dots,\sqrt{p_i}):\Q(\sqrt{p_1},\dots,\sqrt{p_{i-1}})] = 2
            \end{equation}
            
            What remains to show is that $[\Q(\sqrt{p_1},\dots,\sqrt{p_s}):\Q(\sqrt{p_1},\dots,\sqrt{p_{s-1}})] = 2$.
            For the sake of contradiction let us assume that $\sqrt{p_s} \in \Q(\sqrt{p_1},\dots,\sqrt{p_{s-1}})$. Because of \eqref{*} there exist $q,a_1,\dots,a_{s-1} \in \Q$ where we assume wlog (without loss of generality) that $a_{s-1} \neq 0$:
            \begin{align*}
                \sqrt{p_s} &= q + \sum_{i=1}^{s-1} a_i \sqrt{p_i} \\
                \implies p_s &= \left(q + \sum_{i=1}^{s-2} a_i \sqrt{p_i}\right)^2 + 2  \left(q + \sum_{i=1}^{s-2} a_i \sqrt{p_i}\right) \sqrt{p_{s-1}} + a_{s-1}^2 p_{s-1}
            \end{align*}
            Now either $\sqrt{p_{s-1}} \in \Q(\sqrt{p_1},\dots,\sqrt{p_{s-2}})$ or $q + \sum_{i=1}^{s-2} a_i \sqrt{p_i} = 0$. The first statement contradicts \eqref{*} and the second statement contradicts that $p_s$ and $p_{s-1}$ are distinct primes.
            
            Thus $\Q(S)/\Q$ is infinite. And since $S$ is algebraic $\Q(S)/\Q$ is also algebraic (since any element of $\Q(S)$ can be written as a $\Q$-rational function using only finitely many elements of $S$).
        \end{proof}
        
        For a finite extension $L/K$ we define the degree of the extension to be $[L:K] = \dim_K L$. For any intermediate field extension $K \subseteq M \subseteq L$, the extensions $L/M$ and $M/K$ are finite and we have the degree formula $[L:K] = [L:M] \cdot [M:K]$.
        
        \begin{exercise}{}{3}
            Let $L/K$ and $L'/K$ be two extensions of $K$.
            \begin{enumerate}
                \item Let $\varphi \in \Hom_K(L,L')$ and $\alpha \in L$ be algebraic over $K$. Show that: $\alpha' = \varphi(\alpha) \in L'$ is algebraic over $K$ with $p_\alpha(x) = p_{\alpha'}(x) \in K[x]$.
                \item Let $\alpha \in L$ and $\beta \in L'$ with $p_\alpha(x) = p_\beta(x) \in K[x]$. Show that: There exists unique $\varphi \in \Hom_K\left(K(\alpha),L'\right)$ such that $\varphi(\alpha) = \beta$. Moreover $\Img(\varphi) = K(\beta)$.
            \end{enumerate}
        \end{exercise}
        
        \begin{proof}
            Let $p_\alpha(x) = \sum_{i=0}^{n} a_i x^i \in K[x]$. We have:
            \begin{align*}
                p_\alpha(\varphi(\alpha)) = \sum_{i=0}^{n} a_i \varphi(\alpha)^i = \sum_{i=0}^{n} \varphi(a_i \alpha^i) =  \varphi\left( \sum_{i=0}^{n} a_i \alpha^i\right) = \varphi(p_\alpha(\alpha)) = 0
            \end{align*}
            Thus $\alpha' = \varphi(\alpha)$ is algebraic over $K$ with minimal polynomial $p_\alpha(x)$.
            
            For the second part notice that $\varphi(\alpha)$ completely determines the map $\varphi \in \Hom_K(K(\alpha),L')$, if it exists. Since $p_\alpha(x) = p_\beta(x)$ we have:
            \begin{alignat*}{3}
                K(\alpha) &\cong \faktor{K[x]}{\round{p_\alpha(x)}} &&= \faktor{K[x]}{\round{p_\beta(x)}} &&\cong K(\beta)\\
                 \alpha &\mapsto [x] &&=[x] &&\mapsto \beta
            \end{alignat*}
            Call this $K$-isomorphism $\psi: K(\alpha) \rightarrow K(\beta)$. Then $\varphi = \iota \circ \psi$, where $\iota: K(\beta) \hookrightarrow L'$. So $\Img(\varphi) = \Img(\iota) = K(\beta)$.
        \end{proof}
        
        \begin{exercise}{}{4}
            Let $L/K$ be an algebraic extension, show that $\Hom_K(L,L) = \Gal{L/K}$.
        \end{exercise}
        \begin{proof}
            We only need to show that any $\varphi \in \Hom_K(L,L)$ is surjective. Take $\alpha \in L$ and set $M:= \left\{\beta \ | \ \beta \in L, p_\alpha(\beta) = 0 \right\} \subseteq L$ and consider the map
            \begin{align*}
                \varphi': K(M) &\rightarrow K(M) \\
                            m &\mapsto \varphi(m)
            \end{align*}
        This is well defined because of Exercise \ref{exe:3}. Since $p_\alpha(x)$ has only finitely many roots $[K(M):K] < \infty$ and thus $\varphi'$ is surjective. Since $\alpha$ was arbitrary $\varphi$ is surjective.
        \end{proof}
            
    \subsection{Galois extensions}
        Our goal of this chapter is to study the solvability of algebraic equations of radicals, hence we will concentrate on the finite extensions.
        
        We have seen the following Lemma in Cobra \cite{Cobra} (Satz 1.3.33 with $M=L$ and its proof).
        \begin{lemma}{}{1.1}
            Let $L/K$ be a finite extension. Then:
            \begin{enumerate}
                \item $1 \leq \# \Hom_K(L,L) \leq [L:K]$
                \item if $\#\Hom_K(L,L) = [L:K]$, then for any intermediate field $L/N/K$ we have:\\ $\#\Hom_K(N,L) = [N:K], \, \#\Hom_N(L,L) = [L:N]$
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            We sketch the proof of (2) since it does not follow from the statement of \textit{Satz 1.3.33} but its proof. From that proof, we have the estimation for the extension $L/N/K$:
            \begin{align*}
                \#\Hom_K(L,L) &\leq \#\Hom_N(L,L) \cdot \# \Hom_K(N,L) \\
                &\leq [L:N] \cdot [N:K] = [L:K]
            \end{align*}
            If the equality holds, then $\#\Hom_N(L,L) = [L:N]$ and $\#\Hom_K(N,L) = [N:K]$.
        \end{proof}
        
        Applying the above exercise, we have a first estimation of the Galois group:
        \begin{align*}
            \# \Gal{L/K} \leq [L:K]
        \end{align*}
        
        \begin{definition}{Galois extension}{1.2}
            A finite field extension $L/K$ is called \textbf{Galois} if $\#\Gal{L/K} = [L:K]$.
        \end{definition}
        We can rephrase part of the Lemma above as:
        
        \begin{corollary}{}{1.3}
            Let $L/K$ be a Galois extension and $N$ be an intermediate field. Then $L/N$ is Galois.
        \end{corollary}
        
        Note that in this case the extension $N/K$ is not necessarily Galois (example comes later).
        
        \begin{equation*}
            \xymatrix
            {
            L\ar@{-}[d]\ar@/_1.5pc/@{-}[dd]_{\text{Galois}}\ar@/^1pc/@{-}[d]^{\text{Galois}} \\
            N\ar@{-}[d]\ar@/^1pc/@{-}[d]^{\text{not always Galois}} \\
            K
            }
        \end{equation*}
        
        For a better understanding of the definition, we try to figure out what obstructs an extension from being Galois?
        \begin{example}{}{1.4}
            We consider the extension $\Q\left(\sqrt[3]{2}\right)/\Q$. We have $p_{\sqrt[3]{2}}(x) = x^3-2$. Let $\sigma \in \Gal{\Q\left(\sqrt[3]{2}\right)/\Q }$. By Exercise \ref{exe:3}, $\sigma$ must send $\sqrt[3]{2}$ to an element in $\Q(\sqrt[3]{2})$ with minimal polynomial $x^3-2$. The only root of $x^3-2$ in $\Q(\sqrt[3]{2})$ is $\sqrt[3]{2}$ (the others are complex). Hence $\sigma$ must be the identity map. In this case $\Gal{ \Q(\sqrt[3]{2})/\Q} = \{\id\}$ and the extension is not Galois since $\left[\Q\left(\sqrt[3]{2}\right):\Q\right]=3$.
        \end{example}
        
        What goes wrong in this example? The point is that $\Q\left( \sqrt[3]{2}\right)$ does not contain all roots of $x^3-2$.
        
        So to get a Galois extension, we need that the minimal polynomial split into linear factors.
        
        Even if we assume that $L$ is a splitting field of $K$ the equality $\#\Gal{L/K} = [L:K]$ may still fail. We consider $K = \F_p(t)$ and $L= \F_p\left( \sqrt[p]{t}\right)$, where $\alpha:=\sqrt[p]{t}$ has minimal polynomial $p_\alpha = x^p - t \in K[x]$. This polynomial is irreducible but since $\F_p$ has characteristic $p$ we get $x^p-t = (x-\alpha)^p$. Let $\sigma \in \Gal {L/K}$ then $\sigma$ sends $\alpha$ to a root of $p_\alpha$ in $L$ which can only be $\alpha$. Therefore again $\#\Gal {L/K} = 1$.
        
        What goes wrong here is that the minimal polynomial has multiple, non-distinct roots, i.e. the extension is not separable.
        
        Luckily these are all the obstructions. Before starting this investigation we briefly recall normal and separable extensions.
        
        \begin{definition}{normal, separable}{1.5}
            A finite extension $L/K$ is called
            \begin{enumerate}
                \item \textbf{normal}, if any irreducible polynomial in $K[x]$ having a root in $L$ splits into linear factors in $L[x]$.
                \item \textbf{separable}, if every $\alpha \in L$ is separable over $K$ i.e. the roots of its minimal polynomial are pairwise nonequal.
            \end{enumerate}
        \end{definition}

        \begin{proposition}{}{1.6}
            Let $L/K$ be a finite extension.
            \begin{enumerate}
                \item The extension $L/K$ is normal iff there exists $f(x) \in K[x]$ such that $L$ is a splitting field of $f(x)$ over $K$.
                \item The following statements are equivalent:
                    \begin{enumerate}
                        \item $L/K$ is separable
                        \item There exists a field extension $N/K$ such that $\#\Hom_K(L,N) = [L:K]$
                        \item There exist separable elements $\alpha_1,\dots,\alpha_m \in L$ such that $L = K(\alpha_1,\dots,\alpha_m)$
                    \end{enumerate}
                \item (Primitive element) If $L/K$ is separable, there exists $\alpha \in L$ such that $L = K (\alpha)$.
                \item If $\cha(K)=0$, the extension $L/K$ is separable
            \end{enumerate}
        \end{proposition}
        
        The definition of the Galois extension is not easy to use when determining whether a given extension is Galois or not, because computing $\Gal{L/K}$ is usually a hard task. The following proposition gives a convenient way to determine Galois extensions.

        \begin{proposition}{}{1.7}
        	Let $L/K$ be a finite extension. The following statements are equivalent:
        	\begin{enumerate}
        		\item The extension $L/K$ is Galois.
        		\item The extension $L/K$ is normal and separable.
        		\item $L$ is a splitting field of a separable polynomial in $K[x]$.
        	\end{enumerate}
        \end{proposition}
        \begin{proof}
        	$1 \Rightarrow 2$: Assume that $\#\Gal{L/K} = [L:K]$.
        	
        	\underline{separable:} Take $N=L$ in Proposition \ref{prop:1.6}, the equality above gives us that $L/K$ is separable.
        	
        	\underline{normal:} We take an irreducible polynomial $f(x) \in K[x]$ which has a root $\alpha \in L$. Then $f(x)$ is the minimal polynomial $p_\alpha(x)$ of $\alpha$ over $K[x]$. We show that $f(x)$ splits into linear factors in $L[x]$.
        	From Cobra, Proposition 1.3.31, we have a bijection of sets
        	\begin{align*}
        			\Hom_K(K(\alpha),L) &\leftrightarrow \{\beta \in L \ | \ p_\alpha(\beta) = 0\} \\
        			\varphi &\mapsto \varphi(\alpha)
        	\end{align*}
        	Since $L/K$ is Galois, the assumption in \ref{lem:1.1} (2) is fulfilled. Then for $N=K(\alpha)$, we get:
        	
        	\begin{equation*}
        	    \#\Hom_K(K(\alpha),L) = [K(\alpha):K]
        	\end{equation*}
        	
        	Since $[K(\alpha):K] = \deg p_\alpha(x) $, combined with the bijection above gives: $p_\alpha(x)$ has $\deg p_\alpha(x)$ roots in $L$ and hence $p_\alpha(x)$ splits into linear factors in $L[x]$.
        	
        	$2 \Rightarrow 3$: Since $L/K$ is separable, from Proposition \ref{prop:1.6} (2c), we choose separable elements $\alpha_1,\dots,\alpha_k \in L$ such that $L = K(\alpha_1,\dots,\alpha_k)$. The minimal polynomials $p_{\alpha_1}(x),\dots,p_{\alpha_k}$ have roots in $L$. Since $L/K$ is normal, they split into linear factors in $L[x]$.
        	We denote $p(x) = \lcm(p_{\alpha_1}(x),\dots,p_{\alpha_k}(x))$. Then $p(x) \in K[x]$ and it has no multiple roots. Denote the roots of $p(x)$ in $L$ as $\beta_1, \dots, \beta_n$ . Then we have:
        	\begin{align*}
        		L = K(\alpha_1, \dots, \alpha_k) \subseteq K(\beta_1,\dots,\beta_n)\subseteq L
        	\end{align*}
        	Hence $L = K(\beta_1,\dots,\beta_n)$ and $L$ is a splitting field of $p(x)$ over $K$.
        	
        	$3 \Rightarrow 1:$ We immediately get that $L$ is normal and separable (since $L$ is generated by separable elements over $K$). Letting $N=L$ for theorem 1.3.46 of Cobra \cite{Cobra}, we get that $L/K$ is Galois. 
        \end{proof}
        
        \begin{corollary}{}{1.8}
            Assume that $\cha K = 0$ and $f(x) \in K[x]$. Let $K_f$ be the splitting field of $f(x)$ over $K$. Then the extension $K_f/K$ is Galois.
        \end{corollary}
        \begin{proof}
            Clearly, the extension is normal. Since $K$ is perfect the extension is also separable (because it is finite). Using \ref{prop:1.7} yields that $K_f/K$ is Galois.
        \end{proof}
        
        According to this corollary, if we want to study the symmetries of roots of a polynomial $f(x) \in K[x]$. $K \in \{\Q,\R\}$, we can always work inside $K_f$, where we have the Galois property.
        
        In the next corollary, we show that the Galois group encodes symmetries between roots.
        
        \begin{corollary}{}{1.9}
            Let $L/K$ be a Galois extension where $L$ is a splitting field of a separable polynomial $p(x) \in K[x]$ of degree $n$. Then there exists an invective group hom. $\varphi: \Gal{L/K} \rightarrow S_n$. In particular $[L:K]|n!$
        \end{corollary}
        
        \begin{proof}
            We first construct the map $\varphi: \Gal{L/K} \rightarrow S_n$. since $p(x)$ is separable, we let $r_1,\dots,r_n$ denote the distinct roots of $p(x)$. Since $L$ is the splitting field of $p(x)$, we have $r_1,\dots,r_n \in L$.
            
            Let $g \in \Gal{L/K}$. Then for a fixed $r_k, g(r_k)$ is also a root of $p(x)$. Hence there exists a map:
            \begin{align*}
                \sigma_g: [n] &\rightarrow [n], \text{ such that } g(r_k) = r_{\sigma_g(k)}
            \end{align*}
            Since $g$ is invertible $\sigma_g$ is invertible with inverse $\sigma_{g^{-1}}$. Hence $\sigma_g \in S_n$. We define $\varphi(g) = \sigma_g$.
            
            This is a group hom. because the action of $S_n$ on $L$ is well defined:
            \begin{align*}
                &r_{\sigma_{g \circ h}(k)} = (g \circ h)(r_k) = g(h(r_k)) = g(r_{\sigma_h(k)}) = r_{\sigma_g(\sigma_h(k))} \\
                \implies &\sigma_{g \circ h} = \sigma_g \circ \sigma_h \\
                \implies &\varphi(g \circ h) = \varphi(g) \circ \varphi(h)
            \end{align*}
            
            It remains to show that $\varphi$ is injective. Indeed if $\varphi(g) =\id$, the for any $1 \leq k \leq n$: $g(r_k) = r_k$. Since $L = K(r_1, \dots, r_n)$, we have $g = \id$.
        \end{proof}
        
        The idea in this Corollary is helpful in the computation of Galois groups since it realises an abstract group of automorphisms as permutations of roots. This is the fundamental idea of representation theory, which we will discuss in the next Chapter.
        
        Usually, $\varphi$ is not an isomorphism. But when $K = \Q$  we have the following theorem:
        
        \begin{theorem}{}{1.10}
        \begin{enumerate}
            \item Hilbert: For any $n \leq 2$, there exists a separable polynomial $p(x) \in \Q[x]$ of degree $n$ such that the Galois group $\Gal{L/\Q}$ with $L = \Q_p(x)$, is isomorphic to $S_n$.
            \item Nart, Vila: $p(x) = x^n - x -1$ does this job.
        \end{enumerate}
        \end{theorem}
        
        We will not prove the theorem in general, but we will see examples for $n=3,5$ (same idea for $n$ prime).
        
        We finish this section by the following examples. Computations of Galois groups will be discussed later.
        
        \begin{example}{}{1.11}
            \begin{enumerate}
                \item Let $K=\Q$ and $L = \Q(\zeta,\sqrt[3]{2})$, where $\zeta = \exp{\frac{2\pi i}{3}}$ is a root of unity. The field $L$ is a splitting field of the polynomial $x^3-2 \in \Q[x]$. The $L/K$ is Galois (since $\Q$ is perfect). We consider an intermediate field $N = \Q(\sqrt[3]{2})$. The $N/K$ is not Galois since it is not normal.
                \item Let $K$ be a finite field with $\cha K = p > 0$. We claim that any finite extension $L/K$ is Galois. Indeed there exists an $n$ such that $[K:\F_p] = n$. Let $m = [L:K]$. Then $[L:\F_p] = mn$ and from Cobra \cite{Cobra} Lemma 1.3.26, $L$ is a splitting field of the separable polynomial $x^{p^{mn}}-x\in \F_p[x]$. By Proposition \ref{prop:1.7} $L/\F_p$ is Galois and by Corollary \ref{cor:1.3} $L/K$ is Galois. %$\}ether$ 
            \end{enumerate}
        \end{example}
        
\section{Galois correspondence}
        In this section, we will state and prove the fundamental theorem of Galois theory. Roughly speaking it gives a correspondence (called Galois correspondence) between an intermediate field in a Galois extension and the subgroups of the Galois group. This correspondence translates problems in field theory to group theory, which is the key idea of Galois.
    
    \subsection{Fundamental theorem}
        We need to introduce certain notations to state the main result.
        
        For a field $L$ let $Aut(L)$ be the \textbf{group of automorphisms} of $L$, together with the composition of maps. For a finite extension $L/K$ the Galois group $\Gal{L/K}$ is a subgroup of $\Aut(L)$.
        
        \begin{definition}{}{1.12}
            Let $H \leq \Aut(L)$ be a subgroup. We set $L^H = \{\alpha \in L \ | \ \forall \sigma \in H: \sigma(\alpha) = \alpha \}$ and call it the \textbf{fixed field} of $L$ under $H$.
        \end{definition}
        
        \begin{exercise}{}{5}
            \begin{enumerate}
                \item The set $L^H$ is a subfield of $L$
                \item Let $L/K$ be a finite extension and $H \subseteq \Gal{L/K}$. Show that $K \subseteq L^H$
            \end{enumerate}
        \end{exercise}
        \begin{proof}
            \textit{Ad} $(1)$: Since the $\sigma$ are a field homomorphisms we get that $(L^H,+)$ is an subgroup of $L,+$, since:
            \begin{align*}
                \sigma(\alpha - \beta) = \sigma(\alpha) - \sigma(\beta) = \alpha - \beta
            \end{align*}
            and $0 \in L^H$. Furthermore:
            \begin{align*}
                1 = \sigma(\alpha) \sigma (\alpha^{-1}) \implies \alpha^{-1} = \sigma(\alpha)^{-1} = \sigma (\alpha^{-1})
            \end{align*}
            This means:
            \begin{align*}
                \sigma(\alpha \beta^{-1}) = \sigma(\alpha) \sigma(\beta^{-1}) = \alpha \beta^{-1}
            \end{align*}
            Because $1 \in L^H$, we also get that $L^H,\cdot$ is a subgroup of $(L,\cdot)$. The other properties are inherited.
            
            \textit{Ad} $(2)$: Since $\Gal{L/K}$ only contains $K$-automorphisms of $L$, for any $k \in K, \sigma \in H \subseteq \Gal{L/K}: \sigma(k) = k$ and thus $K \subseteq L^H$.
        \end{proof}
        
        Let $G$ be a group. We denote $\Gamma(G) = \{U \ | \ U \leq G\}$. The set $\Gamma(G)$ has a partial order:
        \begin{align*}
            H_1,H_2 \in \Gamma(G), \quad H_1 \prec H_2 :\Leftrightarrow H_1 \subseteq H_2
        \end{align*}
        
        Let $L/K$ be a field extension. We let $\Sigma(L/K) = \{M \ | \ L/M/K \text{ intermediate extension}\}$. The set $\Sigma(L/K)$ has a partial order:
        \begin{align*}
            M_1,M_2 \in \Sigma(L/K), \quad M_1 \succ M_2 :\Leftrightarrow M_1 \supseteq M_2
        \end{align*}
        
        If we consider a finite extension $L/K$ then the Galois group is finite (it is bounded by $[L:K]$). For a finite group, there exist only finitely many subgroups (i.e. $\Gamma(\Gal{L/K})$) is a finite set). If there were a correspondence between $\Gamma(\Gal{L/K})$ and $\Sigma(\Gal{L/K})$, then there should only be finitely many intermediate fields. According to Korollar 1.3.51 of Cobra \cite{Cobra}, the extension $L/K$ has to be primitive or we should at least assume that $L/K$ is separable.
        
        \begin{theorem}{Galois correspondence}{1.13}
            Let $L/K$ be a Galois extension and $G:= \Gal{L/K}$ be its Galois group.
            \begin{enumerate}
                \item The map \[\mathrm{Fix}: \Gamma(G) \rightarrow \Sigma(L/K), \quad H \mapsto L^H\] is an order reversing bijection with inverse \[\Gal{L/\_}: \Sigma(L/K) \rightarrow \Gamma(G), \quad M \mapsto \Gal{L/M}\] Moreover $[L^H:K] = \#G/\#H$.
                \item When restricted to the subset of normal subgroups in $G$, $\Fix$ gives a bijection
                \[\Fix|_{N(G)}: N(G):= \{ N \ | \ N \trianglelefteq G\} \longleftrightarrow \{M \in \Sigma(L/K) \ | \ M/K \text{ Galois}\}\]
                \item Given a normal subgroup $N$ of $G$ the restriction map $\Gal{L/K} \rightarrow \Gal{L^N/K}$ is well defined, surjective with kernel isomorphic to $N$. In particular:
                \begin{equation*}
                    G/N \cong_{\text{group}} \Gal{L^N/K}
                \end{equation*}
            \end{enumerate}
        \end{theorem}
        
        So (2) gives us (in comparison to \ref{cor:1.3}) that all of that also the lower extension is Galois.
        
        \begin{equation*}
            \xymatrix
            {
            L\ar@{-}[d]\ar@/_1.5pc/@{-}[dd]_{G\text{ Galois}}
            \ar@/^1pc/@{-}[d]^{N \text{ Galois}}\\
            L^N\ar@{-}[d]\ar@/^1pc/@{-}[d]^{G/N\text{ Galois}} \\
            K
            }
        \end{equation*}
        
        Before we prove this theorem we first look at a few consequences of it.
        
        \begin{corollary}{}{1.14}
            Let $L/K$ be a Galois extension and $M$ be an intermediate field such that $M/K$ is Galois. Then:
            \begin{equation*}
                \faktor{\Gal{L/K}}{\Gal{L/M}} \cong \Gal{M/K}
            \end{equation*}
        \end{corollary}
        
        \begin{proof}
            From $(2)$ of \ref{thm:1.13}, $\Gal{L/M}$ is a normal subgroup in $\Gal{L/K}$.\\ From $(1)$ of \ref{thm:1.13}, $L^{\Gal{L/M}} = M$. Now we take $N = \Gal{L/M}$ in $(3)$ of \ref{thm:1.13}, which yields the isomorphism of groups above.
        \end{proof}
        
        \begin{remark}{}{1.15}
            Usually determining $\Gamma(G)$ is not an easy task. But later we will use group representation theory to compute normal subgroups of a finite group.
        \end{remark}
        
        The poset $\Gamma(G)$ has the maximal element $G$ and the minimal element $\curly \id$. The poset $\Sigma(L/K)$ has the maximal Element $L$ and minimal element $K$. From the first part of the theorem, we obtain $\Fix(G)=L^G=K$. This special case has used the full power of the Galois condition. For example, for $L=\Q\round{\sqrt[3]2}$ and $K=\Q$, we have seen in \ref{exa:1.4} that $\Gal{L/K}=\curly \id$, hence $L^G=L$.

\separline{Week 2}

        \begin{proposition}{}{1.16}
            Let $L/K$ be a finite extension with $G:=\Gal{L/K}$. The following statements are equivalent:
            \begin{enumerate}
                \item $L/K$ is Galois
                \item $L^G=K$
            \end{enumerate}
        \end{proposition}

        This proposition will serve as motivation for Artins Lemma, that greatly simplifies many proofs in Galois-theory. 
        
        We introduce the following notation: For $\sigma \in \Aut(L)$ we extend $\sigma$ in the following way to $L[x]$: for $f(x)=\sum_{i=0}^ma_ix^i$, we set $\sigma(f(x)):=\sum_{i=0}^m\sigma(a_i)x^i$.

        \begin{lemma}{Artin}{artin}
            Let L be a field and $H\subseteq\Aut(L)$ be a finite subgroup. Then $L/L^H$ is Galois with $$\Gal{L/L^H} = H$$
        \end{lemma}
        \begin{proof}
            With the notation above, a polynomial $p\in L[x]$ is contained in $L^H[x]$ if and only if $\sigma(p(x))=p(x)\ \forall\sigma\in H$. We prove the lemma in steps:
            \begin{enumerate}
                \item We show that $L/L^H$ is separable. Take $\alpha\in L$, we show that $\alpha$ is algebraic over $L^H$ and the minimal polynomial $p_\alpha \in L^H[x]$ is separable.
                \begin{itemize}
                    \item $\alpha$ is algebraic over $L^H$: For this we consider the orbit $H.\alpha = \curly{\sigma(\alpha)|\sigma\in H}$ and let $\beta_1,\dots,\beta_r$ be the distinct elements in $H.\sigma$. We consider the polynomial $q_\alpha(x)=\prod_{i=1}^r(x-\beta_i)\in L^H[x]$.

                    Since $\id\in H$, $\alpha$ is a root of $q_\alpha$. To show that $q_\alpha\in K[x]$ notice, that $\sigma\in H$ implies $\sigma H=H$, hence $\curly{\sigma(\beta_1),\dots,\sigma(\beta_r)}=\curly{\beta_1,\dots,\beta_r}$ and thus
                    $q_\alpha(x)=\prod_{i=1}^r(x-\beta_i)=\prod_{i=1}^r(x-\sigma(\beta_i))=\sigma(q_\alpha)$ holds for all $\sigma\in H$. So because of the remark at the beginning of the proof, $q_\alpha\in L^H$ and $\alpha$ is algebraic.
                    \item $p_\alpha$ is separable: We will show that in fact $p_\alpha=q_\alpha$ hence $p_\alpha$ is separable (we chose the $\beta_i$ to be pairwise distinct). From $q_\alpha(\alpha)=0$ follows $p_\alpha|q_\alpha$. Now in $L[x]$ we have  $x-\alpha|p_\alpha$. Applying a $\sigma\in H$, we get $$(x-\alpha)h=p_\alpha=\sigma(p_\alpha)=\sigma(x-\alpha)\sigma(h)$$
                    where $\sigma(x-\alpha)=x-\beta_i$ for a $\beta_i\in H.\alpha$. By definition of the $\beta_i$ we get $x-\beta_i|p_\alpha\ \forall1\leq i\leq r$, therefore $q_\alpha|p_\alpha$ (and even equality, since both polynomials are monic).
                \end{itemize}
                \item We show that there exists $\alpha\in L$ such that $L=L^K(\alpha)$ (note that we cannot directly use the primitive element theorem since we do not know, whether $L/L^K$ is finite):
                We pick $\alpha\in L$ to be such that $\eckig{L^H(\alpha):L^H}$ is maximal. Such an element exists since every $\alpha$ is algebraic with degree bounded by $\eckig{L^H(\alpha):L^H}=\deg(p_\alpha)=\#H.\alpha\leq\#H<\infty$.
                
                It suffices to show that $L\subseteq L^H(\alpha)$. For this fix an arbitrary $\beta\in L$ and consider the extension $L^H(\alpha,\beta)/L^H$. This extension is finite and separable (it is generated by separable elements). Now we can apply the primitive element theorem (Proposition 16(iii) in \cite{Cobra}), which gives $\gamma\in L$ with $K(\alpha)=K(\alpha,\beta)$. Because of the maximality property of $\alpha$, we have that $\gamma$ is already in $L^H(\alpha)$. The same must then be true for $\beta$. As a consequence, the extension $L/K$ is finite.
                \item We complete the proof: It suffices to show that $\eckig{L:L^H}\leq\Gal{L/L^H}$. Indeed, by choosing $\alpha\in L$ like in (2), we get from (1) that
                 $$\eckig{L^H(\alpha):K}=\#H.\alpha\leq \#H\leq \#\Gal{L/L^H}$$
                where the last inequality follows from $H\subseteq \Gal{L/L^H}$. 
                
                It follows that all inequalities were in fact equalities, especially we obtain $H=\Gal{L/L^H}$.
            \end{enumerate}
        \end{proof}
        \begin{proof}[Proof of \ref{prop:1.16}]
            Note that $\Gal{L/K}$ is a finite group.
            \begin{enumerate}
                \item [$\Leftarrow$:] Since $K\subseteq L^{\Gal{L/K}}=L^G$, it suffices to show that $[L:L^G]=[L:K]$. From Artins Lemma, $L/G^L$ is Galois, hence $$[L:L^G]=\#\Gal{L/L^G}=\#\Gal{L/K}=[L:K]$$
                \item [$\Rightarrow$:] With Artins Lemma \ref{lem:artin}, $L/L^G$ is Galois, hence $L/G$ is Galois.
            \end{enumerate}
        \end{proof}
        \subsection{Proof of Theorem \ref{thm:1.13}}
        \begin{proof}[Proof of \ref{thm:1.13}]
        \begin{enumerate}
            \item [(1)]
            The maps $\Fix$ and $\Gal{L/\_}$ are well defined and order reversing. We show, that they are inverse to each other.
            \begin{itemize}
                \item $\Fix\circ\Gal{L/\_}=\id$:
                This means, that $L^{\Gal{L/M}}=M\ \forall M\in\Sigma(L/K)$. By \ref{cor:1.3} $L/M$ is Galois. Now we can use \ref{prop:1.16} and get the desired result.
                \item $\Gal{L/\_}\circ\Fix=\id$: that is $\Gal{L/L^H}=H\ \forall H\in\Gamma(G)$. Since $H$ is a finite group, it follows from Artins Lemma.
            \end{itemize}
            We compute $[L^H:K]$ for $H\in\Gamma(G)$. For this we consider the extensions $L/L^H/K$ and have $$\eckig{L:K}=\eckig{L:L^H}\eckig{L^H:K}$$
            Again we can use Artins Lemma and get $\eckig{L:L^H}=\#H$ and since $L/K$ is Galois $\eckig{L:K}=\#G$. Hence $\#G/\#H$.
            
            
            \begin{claim*}{}
            $N\in\Gamma(G)$ is a normal subgroup if and only if $\sigma\round{L^N}=L^N\ \forall \sigma\in N$
            \end{claim*}
            \begin{proof}
                By plugging in the definitions we see, that $\sigma\round{L^N}=L^{\sigma N\sigma^{-1}}$
                \begin{align*}
                    \alpha\in L^{\sigma N \sigma^{-1}} &\Leftrightarrow \forall g\in N: \sigma g \sigma^{-1}(\alpha)=\alpha \\
                    &\Leftrightarrow \forall g\in N: g\sigma^{-1}(\alpha)=\sigma^{-1}(\alpha)\\
                    &\Leftrightarrow \sigma^{-1}(\alpha)\in L^N \Leftrightarrow \alpha\in \sigma\round{L^N}
                \end{align*}
                Now, if $N$ is normal, then $\forall \sigma\in G: L^{\sigma N \sigma^{-1}}=L^N$. Artins Lemma then gives us $$\sigma N\sigma^{-1} = \Gal{L/L^{\sigma N \sigma\inv}} = \Gal{L/L^N} = N $$
                The other direction is clear.
            \end{proof}
            \item[(3)]    
            We are now ready for $(3)$: Let $N\trianglelefteq G$ be normal. We consider the restriction map
            $$\pi: \Gal{L/K} \to \Gal{L^N/K}: \sigma \mapsto \sigma_{|L^N}$$
            Since $N$ is normal, $\sigma(L^N)=L^N$, hence $\pi$ is well defined. Per construction $N\subseteq \Ker\pi$. Now take $\sigma\in \ker\pi$, then $\sigma\in\Gal{L/L^N}$. By Artins Lemma $\sigma\in N$, therefore $N\supseteq \ker\pi$.
            
            All in all, we have a group isomorphism $\overline\pi:G/N\to \Gal{L^N/K}$, which was to show for (3).
            
            \item[(2)]
            Let $N\trianglelefteq G$ be a normal subgroup. To see that $L^N/K$ is Galois, consider
            $$\#(G/N)= \frac{\#G}{\#H}\overset{(1)}=\eckig{L^N:K}\geq\#\Gal{L^N/K}\overset{(3)} = \#(G/N)$$
            
            Now let $M\in\Sigma\round{L^N/K}$ be a Galois intermediate field. From \ref{prop:1.7} $(1)\Rightarrow(3)$, there exists a separable polynomial $p\in K[x]$ such that $M$ is a splitting field of $p$. We write $p=\prod_{i=1}^r(x-\beta_i)$ (with the $\beta_i$ being pairwise distinct). We show that $\forall\sigma \in\Gal{L/K}: \sigma(M)=M$. Since $L^{\Gal{L/M}}=M$ (with (1)) this will prove that $M\trianglelefteq G$ (with the claim above).
            
            To show $\sigma(M)=M$, notice that $\sigma$ fixes every element in $K$, hence $\sigma(p)=p$, i.e. $\curly{\beta_1,\dots,\beta_r}=\curly{\sigma(\beta_1),\dots,\sigma(\beta_r)}$
            Then $\sigma(M)=K(\sigma(\beta_1),\dots,\sigma(\beta_r))=K(\beta_1,\dots,\beta_r)=M$
        \end{enumerate}
        \end{proof}
        
        
    \separline{Week $3$}
    \subsection{Inverse Galois problem}
        The inverse Galois problem is the following:
        
        Given a finite group $G$, does there exist a Galois extension $L/\Q$ such that $\Gal{L/\Q} \cong G$?
        
        In general, this question is very hard and widely open. Starting from the work of Galois, Hilbert, Shafarevich etc. it is known that symmetric, alternating, abelian, more general solvable groups and certain finite simple groups can be realised as Galois groups $\Gal{L/\Q}$ for a certain field $L$.
        
        When the base field is not necessarily $\Q$ we can prove that:
        
        \begin{corollary}{}{1.18}
            Given a finite group $G$ there exists a Galois extension with Galois group $G$.
        \end{corollary}
        
        \begin{proof}
            First, any finite group $G$ is a subgroup of a certain symmetric group $S_n$. The proof is similar to \ref{cor:1.9}. Take $h \in G$ and write $G = \{g_1, \dots, g_n\}$. then multiplication by $h$ gives $h.g_i = g_{\sigma_h(i)}$ for a unique $\sigma_h(i) \in [n]$.
            
            The map $\sigma_h: [n] \rightarrow [n]$ is a bijection with inverse $\sigma_{h^{-1}}$. Then the map $G \hookrightarrow S_n, \, h \mapsto \sigma_h$ gives the desired group embedding.
            
            Now we choose a field extension $L/K$ with Galois group $S_n$. Then by Galois correspondence \ref{thm:1.13}, $G \in \Gamma(S_n)$ implies that $L/L^G$ is Galois with Galois group $G$. 
        \end{proof}

\section{Examples}
    In this section, we discuss certain examples
    \subsection{An easy example and Frobenius}
    
        \begin{example}{}{1.19}
            We consider the Galois group of the polynomial $x^3-2$. Let $L = \Q(\sqrt[3]{2},\zeta)$, where $\zeta = \exp{\frac{2 \pi i}{3}}$ be a splitting field of $x^3 - 2$ over $\Q$.
            
            We first compute $L:\Q$. For this we consider the intermediate field $\Q(\zeta)$ and $\Q(\sqrt[3]{2})$. The minimal polynomial of $\zeta$ over $\Q$ is $x^2 + x + 1 \in \Q[x]$ and the minimal polynomial of $\sqrt[3]{2}$ over $\Q$ is $x^3 - 2 \in \Q[x]$. We set $a:= [L:\Q(\zeta)]$ and $b:=[L:\Q(\sqrt[3]{2})]$. Then by the degree formula $2a = 3b$ and hence $2 | b$. But $\zeta \not \in \Q(\sqrt[3]{2}) \subseteq \R$ so $[L:\Q(\sqrt[3]{2})] \geq 2$ hence $b = 2$, which means $[L:\Q] = 6$.
            
            \begin{equation*}
                \xymatrix@W=15pt
                {
                & L\ar@{-}_{a}[dl]\ar@{-}^{b}[dr] & \\
                \Q(\zeta)\ar@{-}_{2}[dr] & & \Q(\sqrt[3]{2})\ar@{-}^{3}[dl] \\
                & \Q &
                }
            \end{equation*}
        
            Since $L/\Q$ is Galois it follows from \ref{cor:1.9} that the group homomorphism $\varphi: \Gal{L/\Q} \rightarrow S_3$ is an isomorphism.
            
            To have a geometric intuition, we draw the three roots of $x^3 - 2$ on the plane: $r_1 = \sqrt[3]{2}, r_2 = r_1 \zeta, r_3 = r_1 \zeta^2$ and connect them to get an equilateral triangle.
            
            \begin{center}
            \begin{tikzpicture}[x=3cm,y=3cm]
                \draw[->] (-1.3,0)--(1.3,0);
                \draw[->] (0,-1.3)--(0,1.3);
            
                \draw (0,0) circle (1);
                \draw (0:3.3cm) node[below] {$\sqrt[3]{2}$};
                \draw (0:3.3cm) node[above] {$r_1$};
                \draw (120:3.1cm) node[above] {$r_2$};
                \draw (240:3.1cm) node[below] {$r_3$};
                \filldraw[black] (0:3cm) circle(1.337pt);
                \filldraw[black] (120:3cm) circle(1.337pt);
                \filldraw[black] (240:3cm) circle(1.337pt);
                \draw[gray] (3cm,0cm) -- (120:3cm);
                \draw[gray] (120:3cm) -- (240:3cm);
                \draw[gray] (3cm,0cm) -- (240:3cm);
    
            \end{tikzpicture}
            \end{center}
            
            The symmetric group elements $(12),(13),(23)$ correspond to permutations $r_1,r_2; \, r_1,r_3; \, r_2,r_3$ of the roots. They can be seen as symmetries of the triangle with respect to the lines orthogonal to the edges, i.e. the reflections.
            
            \begin{center}
            \begin{tikzpicture}[x=3cm,y=3cm]
        
                \draw (0:3.3cm) node[above] {$r_1$};
                \draw (120:3.1cm) node[above] {$r_2$};
                \draw (240:3.1cm) node[below] {$r_3$};
                \filldraw[black] (0:3cm) circle(1.337pt);
                \filldraw[black] (120:3cm) circle(1.337pt);
                \filldraw[black] (240:3cm) circle(1.337pt);
                \draw[gray] (3cm,0cm) -- (120:3cm);
                \draw[gray] (120:3cm) -- (240:3cm);
                \draw[gray] (3cm,0cm) -- (240:3cm);
                \draw[black,dotted] (240:4.5cm) -- (60:4cm);
                \path[<->,very thick,blue] (3cm,0cm) edge[bend right] node [left] {} (120:3cm);
                \draw[blue] (60:2.8cm) node[fill=Example_color!30] {$(12)$};
            
            \end{tikzpicture}
            \end{center}
            
            The elements $(123)$ and $(132)$ correspond to rotations with angles $\frac{2 \pi }{3}$ and $\frac{4 \pi}{3}$.
            
            \begin{center}
            \begin{tikzpicture}[x=3cm,y=3cm]
    
                \draw (0:3.3cm) node[above] {$r_1$};
                \draw (120:3.1cm) node[above] {$r_2$};
                \draw (240:3.1cm) node[below] {$r_3$};
                \filldraw[black] (0:3cm) circle(1.337pt);
                \filldraw[black] (120:3cm) circle(1.337pt);
                \filldraw[black] (240:3cm) circle(1.337pt);
                \draw[gray] (3cm,0cm) -- (120:3cm);
                \draw[gray] (120:3cm) -- (240:3cm);
                \draw[gray] (3cm,0cm) -- (240:3cm);
                \path[->,very thick,blue] (3cm,0cm) edge[bend right] node [left] {} (120:3cm);
                \path[->,very thick,blue] (120:3cm) edge[bend right] (240:3cm);
                \draw[blue] (60:2.9cm) node[fill=Example_color!30] {$(123)$};
                \path[<-,very thick, blue] (0:3cm) edge[bend left] (240:3cm);
            \end{tikzpicture}
            \end{center}
        
            The Galois group $\Gal{L/K}$ is the symmetric group of the triangle, also known as $D_3$.
            
        \end{example}
        
        But if we look at the splitting field of $x^3 - 1$ over $\Q$, which is $\Q(\zeta)$. Then the three roots of $x^3 - 1$ are $\zeta,\zeta^2$. Note that $1 \in \Q$, so every element in $\Gal{\Q(\zeta)/\Q}$ will fix $1$ and the group is isomorphic to $S_2$. The only non trivial element will swap $\zeta$ and $\zeta^2$. Note that the restriction of the complex conjugation map $\sigma:z \mapsto \overline{z}$ to $\Q(\zeta)$ is well defined and it gives the above non trivial element in $\Gal{\Q(\zeta)/\Q}$
        
        
        \begin{center}
        \begin{tikzpicture}[x=3cm,y=3cm]
            \draw[->] (-1.3,0)--(1.3,0);
            \draw[->] (0,-1.3)--(0,1.3);
        
            \draw (0,0) circle (1);
            \draw (0:3.3cm) node[above] {$1$};
            \draw (120:3.1cm) node[above] {$\zeta$};
            \draw (240:3.1cm) node[below] {$\zeta^2$};
            \filldraw[black] (0:3cm) circle(1.337pt);
            \filldraw[black] (120:3cm) circle(1.337pt);
            \filldraw[black] (240:3cm) circle(1.337pt);
            \draw[gray] (3cm,0cm) -- (120:3cm);
            \draw[gray] (120:3cm) -- (240:3cm);
            \draw[gray] (3cm,0cm) -- (240:3cm);
            \path[<->,very thick,blue] (120:3cm) edge[bend right] node [left] {} (240:3cm);
            \draw[blue] (180:2.6cm) node[fill=white] {$\sigma$};
        \end{tikzpicture}
        \end{center}
        Next we study the Galois correspondence for $L/\Q$. We have $L = \Q(\sqrt[3]{2},\zeta)$ from \ref{exa:1.19}. The non trivial subgroups of $S_3$ are of order $2$ or $3$. There are four of them, namely:
        \begin{equation*}
            H_1 = \spitz{(123)}, \quad H_2 = \spitz{(12)}, \quad H_3 = \spitz{(23)}, \quad H_4 = \spitz{(13)}
        \end{equation*}
        
        The fixed fields are:
        \begin{equation*}
            L^{H_1} = \Q(\zeta), \quad L^{H_2} = \Q\round{\sqrt[3]{2} \zeta^2}, \quad L^{H_3} = \Q\round{\sqrt[3]{2}}, \quad L^{H_4} = \Q\round{\sqrt[3]{2}\zeta}
        \end{equation*}
        
        We write out the first one and leave the rest as exercises. Since $\zeta = \frac{r_2}{r_1}$, we have:
	    \begin{equation*}
			(123): \sqrt[3]{2} \mapsto \sqrt[3]{2}\zeta, \, \zeta \mapsto \zeta \qquad (132): \sqrt[3]{2} \mapsto \sqrt[3]{2} \zeta^2, \, \zeta \mapsto \zeta
    	\end{equation*}
    	
    	Hence $\Q(\zeta) \subseteq L^{H_1}$. Moreover $[L^{H_1}:\Q] = \#S_3/\#H_1 = 2$ implies that $\Q(\zeta) = L^{H_1}$.
    	We summarize the Galois correspondence in the following diagrams, called lattices:
	
    	\begin{equation*}
    	    \begin{split}
        	    \xymatrix@R=0.5cm @C=0.5cm
        	    {
        	            & S_3\ar@{-}[dl] \ar@{-}[dd] \ar@{-}[ddr] \ar@{-}[ddrr] & \\
        	        H_1\ar@{-}[ddr] &     & \\
        	            & H_2\ar@{-}[d] & H_3\ar@{-}[dl] & H_4\ar@{-}[dll] \\
        	            & \{e\}
        	    } \qquad  \qquad
        	    \xymatrix@R=0.5cm @C=0.5cm
        	    {
        	            & \Q \ar@{-}[dl] \ar@{-}[dd] \ar@{-}[ddr] \ar@{-}[ddrr] & \\
        	        \Q(\zeta)\ar@{-}[ddr] &     & \\
        	            & \Q\round{\sqrt[3]{2}\zeta^2}\ar@{-}[d] & \Q\round{\sqrt[3]{2}}\ar@{-}[dl] & \Q\round{\sqrt[3]{2}\zeta}\ar@{-}[dll] \\
        	            & \Q\round{\sqrt[3]{2},\zeta}
        	    }	   
    	    \end{split}
    	\end{equation*}
	
    	\begin{example}{}{1.20}
    		Let $K= \F_p$ with a prime number $p$ and $L$ an extension of $K$ with $[L:K] = n$. We have seen in Example \ref{exa:1.11} that $L/K$ is Galois.
    		We show that $\Gal{L/K} \cong (\Z/n\Z.+)$ as groups.
    		Recall that $L$ is a splitting field of the polynomial $x^{p^n}-x \in \F_p[x]$. We define a map $\mathrm{Fr}: L \rightarrow L, \alpha \mapsto \alpha^p$. It follows from $\Fr^n = \id$ that $\Fr$ is a field automorphism. Since for any $\alpha \in \F_p, \alpha^p = \alpha$ by Fermats little theorem, $\Fr \in \Gal{L/K}$.
    		
    		We denote by $H:= \spitz{\Fr}$ the subgroup of $\Gal{L/K}$ generated by $\Fr$.
    		
    		We define a map:
    		\begin{equation*}
    			\begin{split}
    				\varphi: \Z &\rightarrow \Gal{L/K} \\
    						1 & \mapsto \Fr
    			\end{split}
    		\end{equation*}
    	which is a group homomorphism. If we can show that $H = \Gal{L/K}$, then it follows from $[L:K] = n$ that $\ker \varphi = n\Z$, for some $n \in \N$ and hence $\varphi$ would be surjective. This would mean that $\varphi$ induces an isomorphism of groups:
    	
    	\begin{equation*}
    		\overline{\varphi}: \Z/n\Z \xrightarrow{\cong} \Gal{L/K}, \, \overline{k} \mapsto \Fr^k
    	\end{equation*}
    	
    	To show: $H = \Gal{L/K}$. According to the Galois correspondence, it suffices to show that $L^H = \F_p$. For this, we show that $\#L^H \leq \# \F_p$. Indeed, take $\alpha \in L^H$, then $\Fr(\alpha) = a$ implies that $\alpha^p = \alpha$ and $\alpha$ is a root of the polynomial $x^p - x \in \F_p[x]$. But this polynomial has at most $p$ roots, hence $\# L^H \leq p$.
    	
    	In general, if $K$ is a finite field with $\cha K = p > 0$ and $L/K$ is a finite extension with $[L:K] = m$ and $[K:\F_p] = n$. Then from Example \ref{exa:1.11}, the extension $K/\F_p$ is Galois. Applying Corollary \ref{cor:1.14} gives: %dieser Satz ergab in den Notizen keinen Sinn. Muss ich nochmal nachschauen. $\Gal{L/K}$ is the kernel of
    	
    	\begin{equation*}
    	    \xymatrix@R=0.4cm @C=0.4cm
    	    {
    	        \Gal{L/\F_p}\ar[r]\ar@/_2pc/[ddd]_{\cong} & \Gal{K/\F_p}\ar@/^2pc/[ddd]^{\cong} \\
    			\Fr\ar@{|->}[r] \ar@{|->}[d] & \Fr|_K\ar@{|->}[d] \\
    			\overline{1}\ar@{|->}[r] & \overline{1} \\
    			\Z/mn\Z\ar[r] & \Z/n\Z
    	    }
    	\end{equation*}
    	The induced map is given by
    	\begin{equation*}
    	    \begin{split}
    	        \Z/mn\Z &\rightarrow \Z/n\Z \\
    	       \overline{k} &\mapsto \overline{k}
    	    \end{split}
    	\end{equation*}
    	Hence $\spitz{\Fr^n} = \Gal{L/K} \cong \Z/m\Z$.
    	\end{example}
    	
    	\begin{definition}{}{1.21}
    		A Galois extension $L/K$ is called
    		\begin{enumerate}
    			\item \textbf{cyclic} if $\Gal{L/K}$ is a cyclic group
    			\item \textbf{abelian} if $\Gal{L/K}$ is an abelian group
    		\end{enumerate}
    	\end{definition}
    
    	According to Example \ref{exa:1.20}, finite extensions of a finite field are cyclic and abelian.
        
    \subsection{Cyclotomic extensions}
        We study the Galois group of the polynomial $x^n-1 \in \Q[x]$. First, we describe a splitting field. For this, we need the notion of a primitive root of unity.
        
        The complex roots of $x^n -1 $ are $\exp{\frac{2 k \pi i}{n}}$ for $k \in \{0,1,\dots,n-1\}$. These roots of unity, form a group together multiplication of complex numbers $(\mu_n,\cdot)$. It is straightforward to see that:
        \begin{equation*}
            \begin{split}
                (\mu_n,\cdot) &\rightarrow (\Z/n\Z) \\
                    \exp{\frac{2 k \pi i}{n}} &\mapsto \overline{k}
            \end{split}
        \end{equation*}
        is an isomorphism of groups. Note that $\Z/n\Z$ is a ring so on $\Z/n\Z$ there is a multiplication $\overline{k} \cdot \overline{l}:= \overline{kl}$.
        
        \begin{definition}{}{1.22}
            An n-th root of unity is called \textbf{primitive}, if its image in $\Z/n\Z$ has a multiplicative inverse.
        \end{definition}
        Clearly $\exp{\frac{2 \pi k i}{n}}$ is a primitive root of unity if and only if $\gcd(k,n) = 1$.
        
        \textbf{Examples:}
        \begin{enumerate}
            \item In $\Z/6\Z$ the multiplicative invertible elements are $\overline{1}$ and $\overline{5}$, hence the primitive $6$-th roots of unity are $\exp{\frac{2 \pi i}{6}}$ and $\exp{\frac{10 \pi i}{6}}$.
            \item In general there are $\varphi(n)$ many primitive $n$-th roots of unity.
        \end{enumerate}
        
        The following definition was not written in the notes (hence no number), but we include it for the sake of completeness.
        
        \begin{definition*}{}{}
            Let $\zeta_n \in \C$ be a primitive $n$-th root of unity, then the $n$-th cyclotomic extension is $\Q(\zeta_n)$.
        \end{definition*}
        
        \begin{exercise}{}{6}
            \begin{enumerate}
                \item Show that the following statements are equivalent for $\zeta \in \mu_n$:
                \begin{enumerate}
                    \item $\zeta$ is a primitive $n$-th root of unity.
                    \item The group $\mu_n$ is generated by $\zeta$
                    \item $\zeta$ has order $n$.
                \end{enumerate}
                \item Deduce that, for a primitive $n$-th root of unity $\zeta_n$ the field $\Q(\zeta_n)$ is a splitting field of $x^n - 1$ over $\Q$, therefore $\Q(\zeta_n)/\Q$ is Galois.
            \end{enumerate}
        \end{exercise}
        
        \begin{proof}
            
            \textit{Ad (1):}
            For $a) \Rightarrow b)$ let $\zeta$ be a primitive $n$-th root of unity. Then $\zeta^i \neq \zeta^j$ for all $0 \leq i < j \leq n-1$, since otherwise $\zeta^{j - i} = 1$, which is a contradiction, since $0 < j - i < n$ and $\zeta$ was said to be primitive. This means the subgroup of $\mu_n$ generated by $\zeta$ has order $n$. But so does $\mu_n$, thus $\mu_n = \spitz{\zeta}$.
            
            For $b) \Rightarrow c)$ notice that the order of $\zeta$ is equal to $\#\spitz{\zeta}$, which is $n$.
            
            The implication $c) \Rightarrow a)$ follows directly from the definition of the order of $\zeta$ (the smallest number of self-compositions for it to be the identity).
            
            \textit{Ad (2):}
            The roots of $x^n - 1$ are precisely $\mu_n$. Since $\zeta_n$ is primitive, by \textit{1 b)}, we have $\mu_n = \spitz{\zeta}$ which means $\Q(\mu_n) = \Q(\zeta_n)$, which is a splitting field of $x^n - 1$, thus normal. Since $\Q$ is perfect, the algebraic extension $\Q(\zeta_n)/\Q$ is also separable thus Galois.
            
        \end{proof}
        
        We let $U_n$ denote the subset of multiplicative invertible elements in $\Z/n\Z$. So $\# U_n = \varphi(n)$. $U_n$ is a group with multiplication and is of order $\varphi(n)$.
        
        \begin{proposition}{}{1.23}
            Let $\zeta_n$ be a primitive $n$-root of unity. There exists an isomorphism of groups
            \begin{equation*}
                \Gal{\Q(\zeta_n)/\Q} \cong U_n
            \end{equation*}
        \end{proposition}
        
        Before going to the proof, we study the minimal polynomial of $\zeta_n$. We recall the $n$-th cyclotomic polynomial:
        
        \begin{equation*}
            \Phi_n(x) := \prod_{ \substack{\zeta \text{ primitive } \\
            n \text{-th root of unity}}
            } (x-\zeta) 
        \end{equation*}
        
        We have seen in \cite{Cobra} that $\Phi_n \in \Z[x]$ is a monic polynomial of degree $\varphi(n)$.
        
        \begin{lemma}{}{1.24}
            The minimal polynomial of $\zeta_n$ is $\Phi_n(x)$.
        \end{lemma}
        
        \begin{proof}
            We denote $p(x)$ as the minimal polynomial of $\zeta_n$ over $\Q$. Since $\zeta_n$ is a root of $x^n - 1$, there exists $q(x) \in \Q[x]$ such that $x^n - 1 = p(x)q(x)$. By Gauss Lemma, both $p(x)$ and $q(x)$ are in $\Z[x]$. We show that all primitive $n$-th root of unity are roots of $p(x)$. Since $p(x)$ is monic, we have $p(x) = \Phi_n(x)$.
            
            \underline{claim:}
                For any prime number $p$ and any primitive $n$-th root of unity $\zeta$, if $p(\zeta) = 0$ then $p\left(\zeta^p\right) = 0$.
                
            
            Assume that the claim is proven and we completed the proof. Let $\zeta$ be a primitive $n$-th root unity. From \ref{exe:6} there exists $r$ with $\gcd(r,n) = 1$ such that $\zeta = \zeta_n^r$. We write down the prime decomposition of $r$, say $r = p_1^{n_1} \dots p_k^{n_k}$ with $n_1,\dots,n_k > 0$. Then from $p(\zeta_n) = 0$ and the above claim, we know that $p\left(\zeta_n^{p_1}\right) = 0$ and hence $p\left(\zeta_n^{p_1^{n_1}}\right) = 0$ by applying the argument $n_1$ times. Repeating this argument gives $p\left(\zeta_n^{p_1^{n_1}p_2}\right) = 0$ and with induction $p\left(\zeta_n^r\right) = p(\zeta) = 0$. Then all primitive $n$-th roots of unity are roots of $p(x)$ and hence $\Phi_n(x) | p(x)$. Since $p(x)$ is the minimal polynomial, $p(x) | \Phi_n(x)$ and hence $p(x) = \Phi_n(x)$.
            
            \begin{proof}[Proof of claim]
                For this, we consider the canonical map
                \begin{equation*}
                    \begin{split}
                    \Z[x] &\rightarrow \F_p[x] \\        f(x) = \sum_{k=0}^{m} a_k x^k  &\mapsto \sum_{k=0}^m \overline{a_k}x^k =: \overline{f}(x)
                    \end{split} 
                \end{equation*}
                it is a ring homomorphism. We consider the image of $x^n - 1 = p(x)q(x)$ under this map, which is:
                \begin{equation*}
                    x^n - \overline{1} = \overline{p(x)} \cdot \overline{q(x)}
                \end{equation*}
                The polynomial $x^n - \overline{1} \in \F_p[x]$ is separable. Since $\zeta$ is a root of $\overline{p}(x), \overline{q}(\zeta) \neq 0$.
                In $\F_p[x]$ we have the identity: $(\alpha(x) + \beta(x))^p = \alpha(x)^p + \beta(x)^p$ since the non-trivial binomial numbers vanish and for any $\gamma \in \F_p, \gamma^p = \gamma$.
                
                Now we consider $\overline{q}(\zeta)^p: 0 \neq \overline{q}(\zeta)^p = \overline{q}(\zeta^p)$. This implies that $q(x) \in \Z[x], q(\zeta^p) \neq 0$. Since $\zeta^p$ is a root of $x^n - 1$, it has to be a root of $p(x)$.
            \end{proof}
        \end{proof}
        
        Now we can prove Proposition \ref{prop:1.23}
        
        \begin{proof}
            Proposition \ref{prop:1.23}
            
            From Exercise \ref{exe:6}, $\mu_n = \{1,\zeta_n,\dots,\zeta_n^{n-1}\}$. We take $\sigma \in \Gal{\Q(\zeta_n)/\Q}$. Such a $\Q$-automorphism of $\Q(\zeta_n)$ is uniquely determined by the image of $\zeta_n$. Since $\sigma$ is an automorphism, it preserves the order of $\zeta_n$. From Exercise \ref{exe:6}, $\sigma(\zeta_n)$ must be a primitive $n$-th root of unity. Then $\sigma(\zeta_n) = \zeta_n^r$ for some $r \in \N$ such that $\overline{r} \in U_n$. We define a map:
            \begin{equation*}
                \begin{split}
                    \Gal{\Q(\zeta_n)/\Q} &\rightarrow U_n \\
                    \sigma &\mapsto \overline{r}
                \end{split}
            \end{equation*}
            Such a map is an injective group homomorphism. From Lemma \ref{lem:1.24}:
            
            \begin{equation*}
                \#\Gal{\Q(\zeta_n)/\Q} = [\Q(\zeta_n) : \Q] = \deg \Phi_n(x) = \varphi(n)
            \end{equation*}
            Since $\#U_n = \varphi(n)$, the above map is an isomorphism of groups.
        \end{proof}
        \begin{exercise}{Kummer-Extension}{7}
            Let $K$ be a field with $\cha K = 0$ and $n\geq 2$. Assume $K$ contains a primitive $n$-th root of unity. Let $a\in K\setminus\curly{0}, p=x^n-a\in K[x]$ and $K_p$ be a splitting field of $p$ over $K$. Show that $\Gal{L/K}$ is isomorphic to a subgroup of $\Z/n\Z$. Show that $\Gal{L/K}\cong \Z/n\Z$ if and only if $p$ is irreducible in $K[x]$.
        \end{exercise}

%         \begin{proof}
%             
%         \end{proof}
    
\section{Solvable by radicals}
    \subsection{Radical extensions}
        The question we plan to answer is: Given a polynomial $f\in\Q[x]$ whether one can express all roots of $f$ by applying only $+,-,\cdot,\div$ and $\sqrt[k]\cdot$ to elements in $\Q$?
        
        When $f=x^2+px+q$ we have roots $\frac{-p\pm\sqrt{p^2-4q}}2$ and $f$ can be solved by radicals. In general, there might be roots of the form $$\sqrt[r]{a+\sqrt[s]{b+\sqrt[t]{c}}}$$
        for $a,b,c\in\Q, r,s,t\in\Z_{\geq 2}$.
        
        So our first task is to translate the notion "can be solved by radicals" to the language of fields and their extensions.
        \begin{definition}{radical extensions}{1.25}
            \begin{enumerate}
                \item A field extension $L/K$ is called \textbf{simple radical} if $L=K(\alpha)$ where there exists an integer $n\geq 0$ and $a\in K$ such that $\alpha^n=a$. We also denote such an extension by $K(\sqrt[n]a)$ for short (note that $\sqrt[n]a$ is not a single fixed element in our field, we differ here from the definition in Analysis).
                \item A field extension $L/K$ is called radical if there exists a chain of field extensions $$K=K_1\subseteq K_2\subseteq\dots\subseteq K_{r+1}= L$$
                such that each extension $K_{i+1}/K_i$ is simple radical.
            \end{enumerate}
        \end{definition}
        We expand this definition: Since $K_{i+1}/K$ is simple radical, we can find $u_i\in K_{i+1}$ and $n_i\in\N$ such that $K_{i+1}=K_i(u_i)$ and $u_i^{n_i}\in K_i$. Therefore the extension $L/K$ is radical if and only if there exists $u_1,\dots,u_r\in L$, $n_1,\dots,n_r\in \N$ such that $L=K(u_1,\dots,u_r)$ and $u_i^{n_i}\in K(u_1,\dots,u_{i-1})$.
        
        \textbf{Examples}:
        \begin{enumerate}
            \item $\Q \left( \sqrt[3]2, \sqrt[5]{7+\sqrt[3]{4}} \right)/\Q$ is radical
            \item any quadratic extension $L/K$ is radical, if $\cha(K) \neq 2$
        \end{enumerate}
    \subsection{Galois solvability theorem}
        With the help of radical extensions, we can give a rigorous definition of the notion "solvable by radicals". From now on, we will assume $\cha(K)=0$.
        \begin{definition}{solvable by radicals}{1.26}
            A polynomial $f\in K[x]$ is called \textbf{solvable by radicals} over $K$, if there exists a radical extension $L/K$ such that a splitting field $K_f$ of $f$ over $K$ is contained in $L$.
            
            Since $K$ is of characteristic 0, $K_f/K$ is a Galois extension. We denote $\Gal{f}:=\Gal{K_f/K}$ and call it the Galois group of $f\in K[x]$.
        \end{definition}
        We expand this definition. The roots of $f$ are contained in $K_f$. If $K_f$ is contained in  a radical extension $L=K(u_1,\dots,u_r)$ as above, then all roots of $f$ can be expressed in polynomials in $u_1,\dots,u_r$, which are iterated radicals of elements over $K$. Hence this definition coincides with our intuition of "is solvable by radicals".
        
        Before stating the main theorem, we quickly recall the notion of solvable groups in Cobra.
        
        \begin{definition}{solvable group}{1.27}
            A finite group $G$ is called solvable. if there exists a series of subgroups
            $$\curly e = G_n\subseteq G_{n-1}\subseteq \dots \subseteq G_2\subseteq G_1= G$$
            such that \begin{itemize}
                \item $G_{i+1}$ is a normal subgroup in $G_i$
                \item the quotient $G_i/G_{i+1}$ is a cyclic group.
            \end{itemize}
        \end{definition}
        \begin{remark}{}{1.27}
            In Cobra, the last condition reads: "$G_i/G_{i+1}$ is an abelian group. Since $G$ is finite, $G_i/G_{i+1}$ is a finite abelian group. Using the Smith normal form from LAII, we get
            $$G_i/G_{i+1} \cong \Z/n_1\Z \times \dots \times \Z/n_r\Z$$ with $n_1,\dots, n_r\in\N$ satisfying $n_1|\dots|n_r$ which means that the above series can be refined with cyclic subquotients. Details are left as an exercise.
        \end{remark}
        \begin{exercise}{Equivalence of the notions of solvability}{8}
            \begin{enumerate}
                \item Prove that for finite groups, the above definition of a solvable group and the one in Cobra are equivalent
                \item Show that any subgroup/quotient of a cyclic group is cyclic.
                \item Show that any subgroup/quotient of a solvable group is solvable.
                \item Let $N\subseteq G$ be a normal subgroup. Show that $G$ is solvable if and only if $N$ and $G/N$ are solvable.
            \end{enumerate}
        \end{exercise}

         \begin{proof}
             \underline{Ad (1):}
         \end{proof}
        
        The goal of this section is to prove the following:
        \begin{theorem}{Galois}{1.29}
            Let $f\in K[x]$. The following statements are equivalent:
            \begin{enumerate}
                \item The polynomial $f$ is solvable by radicals over $K$.
                \item The Galois group $\Gal{f}$ is solvable.
            \end{enumerate}
        \end{theorem}
        As a direct consequence, we obtain:
        \begin{corollary}{}{1.30}
            If there exists $n\geq 5$ such that $\Gal{f}\cong S_n$ then the polynomial $f$ is not solvable by radicals over $K$.
        \end{corollary}
        \begin{proof}
        From Cobra, the symmetric group $S_n$ is solvable if and only if $n<5$. It suffices to apply theorem \ref{thm:1.29}.
        \end{proof}
        it is natural to ask: Does there exist a polynomial $f\in K[x]$ with $\Gal{f}\cong S_5$?
        \begin{proposition}{}{1.31}
            Let $K=\Q$ and $f = x^5-6x^2+3\in\Q[x]$. Then $\Gal f \cong S_5$
        \end{proposition}
        \begin{proof}
            The polynomial $f$ has the following properties:
            \begin{enumerate}
                \item $f$ is irreducible by applying Eisenstein for $p=3$
                \item $f$ has three distinct real roots and two complex roots which are conjugate to each other.
            \end{enumerate}
            One can show this using a bit of Analysis 1: note that $f(0)>0, f(1)<0, \lim\limits_{n\to\infty}f(n)=\infty$ and $\lim\limits_{n\to-\infty}f(n) = -\infty$.
            by the intermediate value theorem, we know that there exist at least 3 distinct real roots.
            We can get by analytical means (such as a study of the extrema of the function) that these are in fact the only real roots. Since $f\in\Q[x]$ is invariant under complex conjugation, the complex roots have to be conjugate.
            
            We denote the complex roots by $r_1, r_2$ and the three real roots by $r_3, r_4, r_5$. Using \ref{cor:1.9} we look at $\Gal f$ as a subgroup of $S_5$ permuting the indices of the roots. The restriction of the complex conjugation to $K_f$ is well defined and is the element in $\Gal f$ corresponding to $(12)\in S_5$.
            
            Since $f$ is irreducible over $\Q$, the minimal polynomial of $r_1$ is $f$, hence $\eckig{\Q(r_1):\Q}=5$, which implies $5| \eckig{K_f:\Q} = \#\Gal f$. By Cauchy's Theorem (Cobra \cite{Cobra}, Korollar 1.5.22) there exists an element of order 5 in $\Gal f$. Since 5 is prime, such an element corresponds to a 5-cycle in $S_5$, say $\sigma=(1,k_2,k_3,k_4,k_5)$. if $j_l=2$, then $\sigma^l=(1,2,i_1,1_2,i_3)\in\Gal f$. If we reorder the real roots $r_3, r_4, r_5$, we can assume that $(12345)\in\Gal f$.
            
            \begin{claim*}{}
                Let $H\subseteq S_5$ be a subgroup containing $(12)$ and $(12345)$. Then $H=S_5$.
            \end{claim*}
            \begin{proof}
                From Cobra we know that $S_5$ is generated by the transpositions $(i,j)$ with $1\leq i < j \leq 5$.
                
                We set $s_i=(i,i+1)$ for $1\leq i \leq 4$, then $s_i=s_i^{-1}$. One verifies easily that $s_i(i,j)s_i=(i+1,j)$ and repeating it gives:
                $$(i,j)=s_i\dots s_{j-2}s_{j-1}s_{j-2}\dots s_i.$$
                We have shown that $S_5$ is generated by $s_1,s_2,s_3,s_4$. Let $\sigma=(12345)$. Then $s_i=\sigma s_{i-1} \sigma \ \forall 1<i<5$. This shows, that the $S_5$ is generated by $s_1,\sigma$, which proves the claim.
            \end{proof}
            And this completes the proof of the proposition.
        \end{proof}
        The proof generalises to prime numbers:
        \begin{exercise}{}{9}
            \begin{enumerate}
                \item Let $f\in \Q[x]$ be an irreducible polynomial of degree $p$ (for a prime $p$). Assume that $f$ has exactly two complex roots, show that $\Gal f\cong S_5$.
                \item (Harder) Let $p$ be an odd prime, $n_1<\dots<n_{p-2}$ are even numbers and $m$ even with $2m>\sum_{i=1}^{p-2}n_i^2$. Show that: for $f=(x^2+m)(x-n_1)\dots(x-n_{p-2}-2) \in \Q[x], \Gal f\cong S_p$.
            \end{enumerate}
        \end{exercise}
        
%         \begin{proof}
%             
%         \end{proof}
    
    \separline{Week $4$}
    
\section{Proof of Galois' theorem}
    \subsection*{Proof of $(1) \Rightarrow (2)$}
    The idea for this direction is to use Galois correspondence. From the assumption, there exists a radical extension $L/K$ such that $L_f \subseteq L$.
    
    To use Galois correspondence we need a Galois extension. The first problem is: a radical extension is not necessarily Galois.
    
    Since we work with the characteristic zero assumption, separable is not a problem. To get a Galois extension we take the smallest normal extension of $K$ containing $L$. We briefly recall the construction. As before we write $L = K(u_1,\dots,u_r)$ as in the definition. Let $p_1(x) \dots p_r(x)$ be the respective minimal polynomials of the $u_i$ over $K(u_1,\dots, u_{i-1})$. A splitting field of $p(x)$ over $K$ will do the job.
    
    We let $N$ denote this splitting field. Then $N/K$ is a Galois extension containing $L$ as an intermediate field.
    
    The next question is: Is $N/K$ still a radical extension?
    
    \begin{lemma}{}{1.32}
        With the above notation $N/K$ is a radical Galois extension.
    \end{lemma}
    
    \begin{proof}
        It suffices to show that $N/K$ is radical. We keep notations in the above discussion. We set $m_i:=\deg\left(p_i(x)\right)$ and let $u_{i,1} := u_i, u_{i,2},\dots, u_{i,m_i}$ be the roots of $p_i(x)$.
        
        We know from the assumption that $K(u_{1,1})/K$ is radical. So assume that $u_{1,1}^{n_1} \in K$ for some $n_1 \in \N$.
        
        \begin{claim*}{}{}
            $K(u_{1,1} ,u_{1,2})/K(u_{1,1})$ is simple radical.
        \end{claim*}
        \begin{proof}[Proof of Claim]
            First, since $u_{1,1}$ and $u_{1,2}$ have the same minimal polynomial, there exists a $K$-isomorphism $K(u_{1,1}) \overset{\cong}{\rightarrow} K(u_{1,2})$. By post-composing $K(u_{1,2}) \hookrightarrow N$ we obtain a diagram:
            
            \begin{equation*}
                \xymatrix{
                    N \ar@{.>}[r]^{\sigma} & N \\
                    K(u_{1,1})\ar@{^{(}->}^{j}[u] \ar^{\cong}[r]\ar^{i}[ur] & K(u_{1,2})\ar@{^{(}->}[u]
                }
            \end{equation*}
            
            From Proposition \textit{1.3.32} in \cite{Cobra}, there exists $\sigma: N \rightarrow N$ such that $\sigma$ is a field automorphism satisfying $\sigma j = i$. Therefore $\sigma \in \Gal{N/K}$ exists satisfying $\sigma(u_{1,1}) = u_{1,2}$.
            
            Now $u_{1,2}^{n_1} = \sigma(u_{1,1})^{n_1} = \sigma(u_{1,1}^{n_1}) \in K$, therefore $K(u_{1,1},u_{1,2})/K(u_{1,1})$ is simple radical.
        \end{proof}
        In general, we add to $K$ step by step the roots $u_{1,1},u_{1,2},\dots,u_{1,m_1},u_{2,1},\dots,u_{2,m_2}, \dots, u_{r,1}, \dots, u_{r,m_r}$. The argument in the above claim shows that at each step either we get the same field or we get a simple radical extension. This shows that $N/K$ is radical
    \end{proof}
    
    Now we can prove $(1) \Rightarrow (2)$. We recall the notation
    \begin{equation*}
        K = K_1 \subseteq K_2 \subseteq \dots \subseteq K_{r+1} = L
    \end{equation*}
    such that $K_f \subseteq L$, $K_{i+1} = K_i(u_i)$ with $u_i^{n_i} \in K_i$. According to the Lemma, we assume that $L/K$ is Galois.
    
    Although $L/K$ is Galois, each step $K_{i+1}/K_i$ is not necessarily Galois. But this time it is not hard to repair it since we can fix it by adjoining roots of unity:
    Let $\ell = n_1 \dots n_r$ and $\zeta$ be a primitive $\ell$-th root of unity. Note that if we set $a_i := u_i^{n_i} \in K_i$, then $K_{i+1}(\zeta) = K_i(u_i,\zeta)$ is a splitting field of the equation $x^{n_i} - a_i \in K_i(\zeta)[x]$ (any $n_i$-th root of unity is a power of $\zeta$) so it is Galois.
    According to Exercise \textit{7}. $\Gal{K_{i+1}(\zeta)/K_i(\zeta)}$ is a cyclic group.
    
    Now we consider another chain of field extensions
    
    \begin{equation*}
        \xymatrix{
            K_0 := K \subseteq K_1(\zeta) \subseteq K_2(\zeta) \subseteq \dots \subseteq K_{r+1}(\zeta) = L(\zeta)
        }
    \end{equation*}
    
    and set $K'_i:= K_i(\zeta)$. By our construction $L$ is a splitting field of $p(x)$ over $K$, hence $L(\zeta)$ is a splitting field of $p(x)(x^\ell - 1)$ over $K$. Hence $K_{r+1}'/K_0'$ is Galois.
    
    \begin{claim*}{}{}
        The group $\Gal{L(\zeta)/K}$ is solvable.
    \end{claim*}
    
    Assume now that the claim is proven. We consider the extensions $L(\zeta)/K_f /K$: they are all Galois, so by Corollary \ref{cor:1.14} we have an isomorphism of groups
    
    \begin{equation*}
        \faktor{\Gal{L(\zeta)/K}}{\Gal{L(\zeta)/K_f}} \cong \Gal{K_f / K}
    \end{equation*}
    
    Hence $\Gal{K_f/K}$ is a quotient of the solvable group $\Gal{L(\zeta)/K}$. By Exercise \ref{exe:8} it is solvable.
    
    \begin{proof}[Proof of Claim]
        We apply $\Gal{L(\zeta)/\_}$ to the above sequence and get:
        $\{\id\} = \Gal{L(\zeta)/K'_{r+1}} \subseteq \Gal{L(\zeta)/K'_r} \subseteq \dots \subseteq \Gal{L(\zeta)/K'_0}$.
        
        We have normal subgroups $\Gal{L(\zeta)/K'_{i+1}} \trianglelefteq \Gal{L(\zeta)/K'_i}$.
        
        According to Galois correspondence, applied to $L(\zeta)/K'_{i+1}/K'_i$, this property follows from the fact that $K'_{i+1}/K'_i$ is Galois.
        
        The subquotients $\Gal{L(\zeta)/K'_i}/\Gal{L(\zeta)/K'_{i+1}}$ are cyclic.
        
        Applying Corollary \ref{cor:1.14} again this group is isomorphic to $\Gal{K'_{i+1}/K'_i}$, which is cyclic.
        
    \end{proof}
    
    The proof of the direction $(1) \Rightarrow (2)$ is then complete.
    
    \subsection*{Proof of $(2)\Rightarrow (1)$} 
    Assume that $\Gal{f}$ is solvable, we construct a radical extension $L/K$ with $K_f \subseteq L$.
    
    The key point of this construction is given a cyclic group $\Z/m\Z$, how to construct a radical extension?
    
    \begin{lemma}{}{1.33}
        Let $K$ be a field of characteristic $0$ containing a primitive $m$-th root of unity $\zeta$. If $L/K$ is Galois with $\Gal{L/K} \cong \Z/m\Z$, then $L/K$ is a simple radical extension.
    \end{lemma}
    
    \begin{proof}
    {\color{blue} Let $\sigma$ be a generator of $\Gal{L/K}$. It follows from $\sigma^m=\mathrm{id}_L$ that the minimal polynomial of $\sigma$ over $K$ divides $x^m-1$, which splits into linear factors over $K$. Hence $\sigma$ is diagonalizable over $K$. We let $\Lambda$ denote the set of its eigenvalues. We claim that $\Lambda$ is a subgroup of $(\mu_m,\cdot)$. Indeed, if $\lambda_1,\lambda_2\in\Lambda$ then $\lambda_1^{-1},\lambda_1\lambda_2\in\Lambda$ since if $\ell_1,\ell_2$ are eigenvectors associated to eigenvalues $\lambda_1$ and $\lambda_2$, then $\ell_1^{-1}$ and $\ell_1\ell_2$ are eigenvectors having eigenvalues $\lambda_1^{-1}$ and $\lambda_1\lambda_2$ respectively.} {\color{green} Moreover $1 \in \Lambda$.}
    
    {\color{blue} Let $k:=\#\Lambda$. Then any element $\lambda\in\Lambda$ satisfies $\lambda^k=1$. If $k<m$, then $\sigma$ would have order strictly less than $m$. This contradiction forces $k=m$ and $\Lambda=\mu_m$. It follows that there exists $0\neq \ell\in L$ such that $\sigma(\ell)=\zeta^{-1}\ell$. Since $\sigma$ is a $K$-automorphism, applying repeatedly $\sigma$ shows that $\ell$, $\ell^2$, $\ldots$, $\ell^m$ are eigenvectors of $\sigma$ with eigenvalues $\zeta^{-1},\ldots,\zeta^{-(m-1)},\zeta^{-m}=1$.}
        %There are some inaccuracies in the proof. For a more detailed proof see "Non-vanishing of resolvent" in Moodle.
%First, since $L/K$ is finite and separable, from the primitive element theorem, we can assume that $L = K(\alpha)$ for some $\alpha \in L$.
        
        %The crucial construction is the following Lagrange resolvent: For $\sigma \in \Gal{L/K}$ a generator:
        %\begin{equation*}
         %   \ell := \alpha + \zeta \sigma(\alpha) + \zeta^2 \sigma^2(\alpha) + \dots + \zeta^{m-1}\sigma^{m-1}(\alpha)
       % \end{equation*}
        From $\zeta \in K$ and $\sigma(\ell) = \zeta^{-1} \ell$ it follows $\sigma^k(\ell) = \zeta^{-k} \ell$. Now we consider the intermediate field $L/K(\ell)/K$ and look at $\Gal{L/K(\ell)}$ as a subgroup of $\Gal{L/K}$.
        
        Notice that $\sigma,\sigma^2,\dots,\sigma^{m-1}$ do not fix $\ell$, hence they are not in $\Gal{L/K(\ell)}$. It follows from $\Gal{L/K} \cong \Z/m\Z$ that $\Gal{L/K(\ell)}$ is the trivial group and hence $L = K(\ell)$ (Galois correspondence).
        
        It remains to show that $\ell^m \in K$. Notice that for any $1 \leq k \leq m-1$, $\sigma^k(\ell^m) = \ell^m$. Therefore again by Galois correspondence, $\ell^m \in L^{\Gal{L/K}} = K$.
    \end{proof}
    
    
   {\color{blue}The generator $\ell$ in the proof of the lemma can be written in an explicit form, see Corollary \ref{cor:1.35}. This explicit form, called Lagrange resolvent, will be very helpful in finding solutions to cubic equations by radicals.}
    
    
    We go back to the proof of $(2) \Rightarrow (1)$.
    
    \begin{proof}
    
    Assume that $\Gal{K_f/K}$ is solvable. Like in the part $(1) \Rightarrow (2)$ we set $r:=[K_f:K]$ and $\zeta$ to be a primitive $r$-th root unity. Then consider the extension $K_f(\zeta)/K(\zeta)$ which is also Galois. By restricting $\sigma \in \Gal{K_f(\zeta)(K(\zeta)}$ to $K_f$, since $\sigma$ fixes $\zeta$, we realised $\Gal{K_f(\zeta)/K(\zeta)}$ as a subgroup of $\Gal{K_f/K}$. As a consequence of Exercise \ref{exe:8} $\Gal{K_f(\alpha)/K(\alpha)}$ is solvable.
    
    We choose a composition series of it with cyclic subquotients:
    \begin{equation*}
        \Gal{K_f(\zeta)/K(\zeta)} = G_2 \supseteq G_3 \supseteq \dots \supseteq G_{r+1} = \{\id\}
    \end{equation*}
    Applying the Galois correspondence gives us fixed fields
    \begin{equation*}
        K(\zeta) = K_2 \subseteq K_3 \subseteq \dots \subseteq K_{r+1} = K_f(\zeta)
    \end{equation*}
    where $K_i = K_f(\zeta)^{G_i}$.
    
    We set $K_1 = K$. It remains to show that $K = K_1 \subseteq K_2 \subseteq \dots \subseteq K_{r+1} = K_f(\zeta)$ is a chain of simple radical extensions.
    
    The extension $K_2 / K_1$ is cyclotomic and hence is simple radical. We look at $K_{i+1}/K_i$ for $i \geq 2$.
    Since the primitive $r$-th root of unity $\zeta$ is in $K_i$ for $i \geq 2$, we want to apply the above Lemma.
    The last thing to show for this is that $K_{i+1}/K_i$ is Galois with cyclic Galois group.
    
    For this, we apply Galois correspondence to $K_f(\zeta)/K_{i+1}/K_i$. 
    Since $\Gal{K_f(\zeta)/K_{i+1}} = G_{i+1}$ is a normal subgroup in $\Gal{K_f(\zeta)/K_i}$ the extension $K_{i+1}/K_i$ is Galois with Galois group $G_i/G_{i+1}$, which is cyclic by assumption.
    The proof of the Galois theorem is then complete.    
    \end{proof}
    
    \subsection{Quadratic and cubic equations}

    {\color{blue}We start from Lagrange resolvent. We keep notations as in Lemma \ref{lem:1.33} with $G:=\Gal{L/K}$.
    
    Let $L^\times:=L\setminus\{0\}$ be the multiplicative group of the field $L$. A \emph{character} of $G$ (with value in $L^\times$) is a group homomorphism $\chi:G\to L^\times$. We denote $\mathrm{Ch}(G,L^\times)$ the set of such characters. Note that these characters are different to those we will see in group representation theory.
    
    \begin{lemma}{Dedekind}{1.34}
        Different characters of $G$ are linearly independent over $L$.
    \end{lemma}

    \begin{proof}
    Assume the statement is false, that is to say, there exist different characters $\chi_1,\ldots,\chi_m$ of $G$ which are linearly dependent over $L$. We assume that $m$ is minimal, that is to say, for any $1\leq k<m$, any $k$ characters in $G$ are independent over $L$ (such an $m$ exists since any single character is linearly independent). Then there exists a linear relation $a_1\chi_1+\ldots+a_m\chi_m=0$ with $a_1,\ldots,a_m\in L^\times$. 

    The linear relation means: for any $g\in G$, 
    \begin{equation}\label{Eq}
    a_1\chi_1(g)+\ldots+a_m\chi_m(g)=0.
    \end{equation} 
    Since $\chi_1\neq \chi_2$, we choose $h$ such that $\chi_1(h)\neq\chi_2(h)$. Replacing $g$ by $gh$ in \eqref{Eq} and noticing that $\chi_i$ are characters, we obtain:
    $$a_1\chi_1(g)\chi_1(h)+a_2\chi_2(g)\chi_2(h)+\ldots+a_m\chi_m(g)\chi_m(h)=0.$$
    Multiplying \eqref{Eq} by $\chi_1(h)$ gives
    $$a_1\chi_1(g)\chi_1(h)+a_2\chi_2(g)\chi_1(h)+\ldots+a_m\chi_m(g)\chi_1(h)=0.$$
    Subtracting the above two identities gives
    $$a_2(\chi_2(h)-\chi_1(h))\chi_2(g)+\ldots+a_m(\chi_m(h)-\chi_1(h))\chi_m(g)=0.$$
    Since $\chi_2(h)-\chi_1(h)\neq 0$ and $g$ is an arbitrary element in $G$, we obtain a linear dependency relation of $\chi_2,\ldots,\chi_m$ over $L$, a contradiction.
    \end{proof}


    \begin{corollary}{}{1.35}
        With the same assumption as in Lemma \ref{lem:1.33}, there exists $a\in L$ such that $$\ell(a):=a+\zeta\sigma(a)+\ldots+\zeta^{m-1}\sigma^{m-1}(a)\neq 0$$
        where $\sigma\in\Gal{L/K}$ is a generator. Moreover, $\ell(a)$ is an eigenvector of $\sigma$ of eigenvalue $\zeta^{-1}$.
    \end{corollary}

    This element $\ell(a)$ in the Lemma is called a \emph{Lagrange resolvent}.

    \begin{proof}
        In the Dedekind Lemma we choose $G=L^\times$ to be the multiplicative group. Any element in the Galois group $\mathrm{Gal}(L/K)$, when restricted to $L^\times$, gives a character in $\mathrm{Ch}(L^\times,L^\times)$ because it is invertible. Therefore there is no linear relations between elements in the Galois group $\mathrm{Gal}(L/K)$ over $L$. This proves that $$\mathrm{id}_L+\zeta\sigma+\ldots+\zeta^{m-1}\sigma^{m-1}\neq 0\in \mathrm{End}_K(L),$$
        and hence one can choose $a\in L$ such that $\ell(a)\neq 0$. The second statement is clear since both $\zeta$ and $\sigma$ have order $m$.
    \end{proof}}

        \subsubsection{(1) Quadratic equations} 
            Let $f(x) = x^2 + ax + b \in \Q[x]$. Without loss of generality, we assume that it has two distinct roots $x_1$ and $x_2$. Then $x_1 + x_2 = -a$ and $x_1x_2 = b$. We let $\zeta$ be a primitive $2$nd root of unity $(\zeta = -1)$ and define the Lagrange resolvent
            \begin{equation*}
                \ell := x_1 + \zeta x_2
            \end{equation*}
            {\color{blue} Note that it follows from $x_1\neq x_2$ that $\ell\neq 0$.}
            %Since $x_2 = -a - x_1 \implies \Q(x_1,x_2) = \Q(x_1)$. 
            The Galois group $\Gal{f} \cong S_2 = \{\id, (12)=: \tau \}$ where $\tau$ permutes the two roots. Under the action of $\tau$, $\tau(\ell) = \zeta x_1 + x_2 = \zeta \ell$.
            
            Now notice that $u:= \ell^2$ is fixed by $\Gal{f}$ so $u \in \Q$. Then we have
            \begin{equation*}
                \left\{ \begin{array}{c}
                    x_1 + \zeta x_2 = l = \sqrt{u}  \\
                    x_1 + x_2 = -a
                \end{array}\right.
            \end{equation*}
            
            Solving it gives: $x_1 = \frac{a + \sqrt{u}}{\zeta - 1}, \quad x_2 = \frac{a +\zeta \sqrt{u}}{\zeta - 1}$.
            
            Replacing $\zeta$ by $-1$ and noticing
            \begin{equation*}
                u = \ell^2 = (x_1 + \zeta x_2)^2 = x_1^2 - 2x_1 x_2 + x_2^2 = (x_1 + x_2)^2 - 4x_1x_2 = a^2 -4b
            \end{equation*}
            
            We obtain:
            \begin{equation*}
                x_1 = -\frac{1}{2}\left(a + \sqrt{a^2 - 4b}\right), \quad x_2 = -\frac{1}{2}\left(a - \sqrt{a^2-4b}\right)
            \end{equation*}
        
            Which is the $pq$-formula.
            
            This method works for cubic equations aswell:
        
        \subsubsection{(2) cubic equations}
            Let $f=x^3+ax^2+bx+c \in\Q[x]$. We will work with the assumption that $\Gal f\cong S_3$ since almost all cubic equations fulfil this assumption. The other cases are easier. As always we identify the two groups where we let $S_3$ operate on the roots via permutation.
            
            From this assumption, $f$ has three roots $x_1,x_2,x_3$. again we have 
            \begin{align*}
                &x^3+ax^2+bx+c=(x-x_1)(x-x_2)(x-x_3)\\
                &\begin{cases}
                    x_1+x_2+x_3 &=-a\\
                    x_1x_2+x_1x_3+x_2x_3 &=b\\
                    x_1x_2x_3 &=-c
                \end{cases}
            \end{align*}
            Let $\zeta = e^{\frac{2\pi i}3}$ be a primitive 3rd root of unity. We consider the Lagrange resolvent
            $$\ell:=x_1+\zeta x_2+ \zeta^2 x_3 \qquad (\sigma = (123) \in A_3).$$
            (Note that a composition series of $S_3$ is $S_3\supset A_3 \supset \curly{e}$ where $S_3/A_3 \cong \Z/2\Z$ and $A_3\cong \Z/3\Z$. Here we study the intermediate extension $\Q(\zeta,x_1)/\Q(\zeta,x_1)^{A_3}$ with galois group isomorphic to $\Z/2\Z$).
            {\color{blue}We show that $\ell\neq 0$. Indeed, let $\tau_3=(123)\in\Gal{f}$ and consider $\mathrm{id}+\zeta\tau_3+\zeta^2\tau_3^2$. If its value at $x_1$ were zero, then it would be zero on $\mathbb{Q}(\zeta)(x_1)$, contradicts to the linear independency of characters.}
            
            
            We name the elements of $S_3$:
            \begin{center}
            \begin{tabular}{cccccc}
            $\tau_1=\id$,& $\tau_2=(132)$, &$\tau_3=(123)$, &$\tau_4=(23)$, &$\tau_5=(12)$, &$\tau_6=(13)$.
            \end{tabular}
            \end{center}
            %\begin{align*}
             %   \tau_1=\id,& \tau_2=(132), &\tau_3=(123), &\tau_4=(23), &&&&\tau_5=(12), &&&&&\tau_6=(13)
            %\end{align*}
            Applying them to $\ell$ gives 
            \begin{center}
            \begin{tabular}{ccc}
            $\tau_1(\ell)=\ell$,& $\tau_2(\ell)=\zeta\ell$, &$\tau_3(\ell)=\zeta^2\ell$, \\
            $\tau_4(\ell)=\omega$, &$\tau_5(\ell)=\zeta\omega$, &$\tau_6(\ell)=\zeta^2\omega$.
            \end{tabular}
            \end{center}
            %\begin{align*}
            %    &\tau_1(\ell)=\ell, &&\tau_2(\ell)=\zeta\ell, &&&\tau_3(\ell)=\zeta^2\ell\\
            %    &\tau_4(\ell)=\omega, &&\tau_5(\ell)=\zeta\omega, &&&\tau_6(\ell)=\zeta^2\omega
            %\end{align*}
            where $\omega=x_1+\zeta^2x_2+\zeta x_3$. Now notice that both $u_1:=\ell^3$ and $u_2:=\omega^3$ are fixed by $A_3$-permutations.Hence they are in the fixed field $\Q(x_1,x_2,x_3)^{A_3}$, which is of degree 2 over $\Q$. Therefore they have to satisfy a degree 2 equation:
            $$y^2+py+q=0 \qquad \text{ with } p=-(u_1+u_2),\quad q=u_1u_2$$
            After some computation one finds
            $$p=2a^2-9ab+27c, \quad q=(a^2-3b)^3$$
            Applying the result obtained for quadratic equations, we can solve $u_1$ and $u_2$ as functions in $a,b$ and $c$.
            
            Now we have obtained three linear equations
            $$
            \begin{cases}
            x_1+\zeta x_2+\zeta^2x_3=\ell=\sqrt[3]{u_1}\\
            x_1+\zeta^2x_2+\zeta x_3=w=\omega=\sqrt[3]{u_2}\\
            x_1+x_2+x_3=-a
            \end{cases}
            $$
            Solving them gives 
            $$
            \begin{cases}
            x_1=\frac13\round{-a+\sqrt[3]{u_1}+\sqrt[3]{u_2}}\\
            x_2=\frac13\round{-a+\zeta^2\sqrt[3]{u_1}+\zeta\sqrt[3]{u_2}}\\
            x_3=\frac13\round{-a+\zeta\sqrt[3]{u_1}+\zeta^2\sqrt[3]{u_2}}
            \end{cases}
            $$
            \begin{remark}{}{1.36}
                If you notice the change of variable $z:=x-\frac a3$ we get $f(x)=z^3+\alpha z+\beta$ with $\alpha,\beta \in\Q$. In this case the formulae for $p$ and $q$ above are easier to compute: $p'=27\beta, q'=-27\alpha^3$ (verify it for yourself).
            \end{remark}
            For a general degree 4 equation, we can use the Lagrange resolvent to reduce it to a degree 3 equation and find the solution by radicals. But Lagrange found that the same method applied to a degree 5 equation will only give us an equation of a higher degree. This made Lagrange conjecture that a general solution might not exist and served as an inspiration for the work of Galois.
            
            
\chapter{Linear representations of finite groups}
    We are going to use \cite{Serre.1977} and \cite{FultonHarris.1991} as references. You can find the PDFs on Moodle.
    
    In this chapter, we study linear representations of finite groups. The algebraic side of representations theory studies representations of:
    
    \begin{enumerate}
        \item[a)] finite groups, a*) compact and algebraic groups
        \item[b)] Lie algebras and their quantization
        \item[c)] Finite dimensional algebras
        \item[d)] ad hoc examples
    \end{enumerate}
    
    The specific methods used in the study of representations of these algebraic structures may be quite different (algebraic, geometric, combinatorial). But the connection between these different branches of representation theory is a big aspect of modern representation theory.
    
    In this lecture we will study the basic theory in parts a) and c). Next semester in the Lecture "Lie algebra" we will learn part b).
    
    The idea of representation theory is linearization. We use vector spaces as films to take a picture of the group (let the group hit the film and examine the \underline{trace}). If we take enough pictures of the group we can recover the group to some extent (in fact we can recover the entire group which is called Tannakian construction, this is an advanced topic).
    
    We briefly recall tensor products of vector spaces. Details can be found in the lecture notes of Lineare Algebra II \cite{LA} (which you can also find on the Moodle page).
    
    \setcounter{section}{-1}
    
\section{Revision: tensor products}
    
    Let $K$ be a field and $V, W$ be two $K$-vector spaces. We denote their Cartesian product as $V \times W$. The tensor product $V \tens W$ is defined as the $K$-vector space $K(V \times W)/R$, where $K(V \times W)$ is the $K$-vector space with basis $V \times W$ and
    \begin{equation*}
        \begin{split}
            R = \spitz{ \left. \begin{array}{c}
                (v_1+v_2,w_1) - (v_1,w_1) - (v_2,w_1), \\ (v_1,w_1 + w_2) - (v_1,w_1) - (v_1,w_2),  \\
                (\lambda v_1, \mu w_1) - \lambda \mu (v_1,w_1) 
            \end{array}  \ \right| \ v_1,v_2 \in V, \ w_1,w_2 \in W \text{ and } \lambda,\mu \in K }_K        
        \end{split}
    \end{equation*}
    
    There is a canonical map
    \begin{equation*}
    \begin{split}
        \alpha: V \times W &\to V \tens W \\ (v,w) &\mapsto [(v,w)]        
    \end{split}
    \end{equation*}
    We will denote $v \tens w := [(v,w)]$.
    
    \begin{proposition}{Universal property of the tensor product}{2.1}
        Let $T$ be a $K$-vector space and $f: V \times W \to T$ be a bilinear map. Then there exists a unique $K$-linear map $\overline{f}: V\times W \to T$ such that $\tilde{f} \circ \alpha = f$. This can be described via the following diagram
        \begin{equation*}
            \xymatrix@R=14pt {
                V \times W \ar[dr]_(.4){\alpha} \ar[rr]^(.55){\forall f} & & T \\
                & V \tens W\ar@{.>}[ur]_{\exists! \bar{f}} \ar@{}[u]|(.55){\circlearrowleft}}
        \end{equation*}
    \end{proposition}
    The idea behind this universal property is: bilinear maps are less familiar to us than linear maps. By looking at tensor products, we can transform bilinear maps into linear maps. For linear maps, we have the big machinery from Linear Algebra to deal with them. We can also summarize the above universal property in the following isomorphism of $K$-vector spaces
    \begin{equation*}
        \begin{split}
            \Bil(V,W;T) &\overset{\cong}{\to} \Hom_K(V \tens W, T) \\
            f &\mapsto \bar{f}            
        \end{split}
      \end{equation*}
    
    \begin{remark*}{}
        The universal property defines the tensor product up to isomorphism, see \cite{LA}.
    \end{remark*}
    
    To define a linear map $V \tens W \to T$, it suffices to define it on the elementary tensors $v \tens w$ for $v \in V,w \in W$.
    
    Now we assume that both $V$ and $W$ are finite-dimensional. Let $(v_1,\dots,v_m)$ be a basis for $V$ and $(w_1,\dots,w_n)$ be a basis for $W$. We have seen in LA II \cite{LA} that $\round{(v_i \tens w_j) \ | \ 1 \leq i \leq m, 1 \leq j \leq n}$ is a basis of $V \tens W$. Therefore $\dim(V \tens W) = \dim(V) \cdot \dim(W)$.
    
    What will be of use to us in the subsequent chapters is the fact that the Hom-space can be written as a tensor product of vector spaces.
    
    \begin{proposition}{}{2.2}
        Let $V, W$ be two finite-dimensional $K$-vector spaces. Then
        \begin{equation*}
        \begin{split}
            \varphi: W \tens V^* &\to \Hom_K(V,W) \\
            w \tens f &\mapsto (v \mapsto f(v)w )    
        \end{split}
        \end{equation*}
        is an isomorphism of $K$-vector spaces.
    \end{proposition}
    
    \begin{proof}
        We construct the inverse of $\varphi$:
        \begin{equation*}
        \begin{split}
            \psi: \Hom_K(V,W) &\to W \tens V^* \\
            \alpha &\mapsto \sum_{i=1}^n \alpha(v_i) \tens v_i^*
        \end{split}
        \end{equation*}
        where $(v_1,\dots,v_n)$ is a basis of $V$ and $(v_1^*,\dots,v_n^*)$ its dual basis, gives the inverse map of $\varphi$.
        
        First, we need to check that $\psi \circ \varphi = \id_{W \tens V^*}$ on a basis of $W \tens V^*$. For this we fix a basis $(w_1,\dots,w_m)$ of $W$. This yields a basis $(w_l \tens v_j^*\ | \ 1 \leq l \leq m, \ 1 \leq j \leq n)$ of $W \tens V^*$. We have:
        \begin{equation*}
        \begin{split}
            (\psi \circ \varphi)(w_l \tens v_j^*) = \sum_{i=1}^{n} \varphi(w_l \tens v_j^*)(v_i) \tens v_i^* = \sum_{i=1}^{n} v_j^*(v_i)w_l \tens v_i^* = \sum_{i=1}^n \delta_{i,j} w_l \tens v_i^* = w_l \tens v_j^*
        \end{split}
        \end{equation*}
        
        Conversely take $\alpha \in \Hom_K(V,W)$, then we again only need to check it for the $v_j$, we have:
        \begin{equation*}
        \begin{split}
            (\varphi \circ \psi)(\alpha)(v_j) = \varphi\round{\sum_{i=1}^n \alpha(v_i) \tens v_i^*}(v_j) = \sum_{i=1}^n \varphi(\alpha(v_i) \tens v_i^*)(v_j) = \sum_{i=1}^n v_i^*(v_j)  \alpha(v_i) = \alpha(v_j)
        \end{split}
        \end{equation*}
    \end{proof}
    
\section{Linear representations}
    In the rest of this chapter, we assume that $G$ is a finite group and fix $\C$.
    
    \subsection{Definition and examples}
        Recall that for a vector space $V$, $\GL(V)$ is the group of invertible endomorphisms of $V$.
        \begin{definition}{}{2.3}
            A (linear) \textbf{representation} of $G$ (also called $G$-representation) is a pair $(V,\rho_V)$ where
            \begin{itemize}
                \item $V$ is a $\C$-vector space
                \item $\rho_V: G \to \GL(V)$ is a group homomorphism
            \end{itemize}
            
            The dimension of $V$ is called the \textbf{dimension} or \textbf{degree} of the representation. We let $\Rep(G)$ denote the category of all representations of $G$ and $\rep(G)$ denote the subcategory of all finite dimensional representations.
        \end{definition}
        
        \begin{example}{}{2.4}
            Let $G \circlearrowleft X$, i.e. $X$ is a $G$-set. By definition, there exists a homomorphism of groups
            \begin{equation*}
            \begin{split}
                \rho_X: G &\to S_X:=\Bij(X,X) \\
                        g &\mapsto (x \mapsto g.x)
            \end{split}
            \end{equation*}
            
            We consider the vector space $\C(X)$ whose elements are $\C$-linear combinations of elements in $X$. In notation
            \begin{equation*}
                \C(X) = \left\{\sum_{i=1}^{k} \lambda_i x_i \ | \ \lambda_i \in \C, x_i \in X \right\}
            \end{equation*}
            The vector space $V:= \C(X)$ admits a $G$-rep structure via the map
            \begin{equation*}
            \begin{split}
                \rho_V: G &\to \GL(V) \\
                    g &\mapsto \left(x_i \mapsto \rho_X(g)(x_i)\right)
            \end{split}
            \end{equation*}
            Since $\rho_X(g) \in S_X$ it permutes the basis elements of $V$. Hence the linear continuation of $\rho_X(g)$ is in $\GL(V)$.
        \end{example}
        
        We look at some special cases:
        
        \underline{(1): $G = S_n, X = [n]$:}
        
            We identify $\C(X)$ with $\C^n$ together with a basis $e_1,\dots,e_n$. The above construction gives a representation of $S_n$:
            \begin{equation*}
            \begin{split}
                \rho_{\C^n}: S_n &\to \GL_n(\C) \\
                            \sigma &\mapsto \perm(\sigma)
            \end{split}
            \end{equation*}
            
            where $\perm(\sigma)$ is the permutation matrix of $\sigma$. Clearly $\perm(\sigma) \in \GL_n(\C)$, since $\det(\perm(\sigma)) = \sgn(\sigma)$. Furthermore $\perm(\sigma \circ \tau) = \perm(\sigma) \circ \perm(\tau)$.
            
        \underline{(2): $X=G$:}
        
            Then $G$ acts on $X$ by left multiplication. The above construction gives a representation of $G$
            \begin{equation*}
            \begin{split}
                \rho^{\text{reg}} \text{ or } \rho_{\C(G)}: G &\to \GL(\C(G)) \\
                g &\mapsto \round{\sum_{h \in G} \lambda_h h } \mapsto \round{\sum_{h \in G} \lambda_h gh }
            \end{split}
            \end{equation*}
            Such a representation is called the left regular representation of $G$.
            
        \underline{(3): $X = \{x\}$:}
        
        Then $G$ acts on $X$ by $\rho_X(g)(x) = x$. The associated $G$-representation on $\C(X) \cong \C$ is:
        \begin{equation*}
        \begin{split}
            \rho: G &\to \GL(\C) \overset{\cong}{\to} \C\setminus\{0\} \\
            g &\mapsto \id_\C \mapsto 1
        \end{split}
        \end{equation*}
        such a representation is called the trivial representation of $G$. It is a one-dimensional representation.
        
\separline{Week 5}

        We discuss how to construct new representations from the old ones next.
        
        \begin{definition*}{Subrepresentation (a)}
            Let $(V,\rho_V) \in \Rep(G)$. An element $(W,\rho_W)$ is called a \textbf{subrepresentation} of $G$, if $W \leq V$ (subspace) and $\rho_W(g) = \rho_V(g)|_W$ for all $g \in G$.
        \end{definition*}
        
        We consider the example \ref{exa:2.4} (1) above with $G=S_n$ and $V=\C^n$. Let $W = \spitz{e_1 + \dots + e_n}$ then for any $g \in S_n$, $\rho_V(g)(e_1 + \dots + e_n) = e_1 + \dots + e_n$, hence $\rho_V(g)|_W \in \GL(W)$ and $(W,\rho_V(g)|_W)$ is a subrepresentation of $(V,\rho_V)$.
        
        Note that for a representation $(V,\rho_V)$, there always two subrepresentations $(\{0\},\rho_V|_{\{0\}})$ and $(V,\rho_V)$.
        
        
        \begin{definition}{Irreducible representation}{2.5}
            A representation $(V,\rho_V) \in \Rep(G)$ is called \textbf{irreducible} if $V$ has \underline{exactly} two subrepresentations.
        \end{definition}
        
        \textbf{Examples}:
        \begin{enumerate}
            \item The trivial representation $\rho^{\text{triv}}: G \to \GL(\C), \quad \rho(g) = \id $ is irreducible
            \item The representation in Example \ref{exa:2.4} (1) is not irreducible.
            \item The $0$ representation $\rho_{0}: G \to \GL\round{\{0\}}$ is \textbf{not} irreducible
        \end{enumerate}
    
    \underline{Question:} Is the left regular representation of $G$ irreducible?
    \begin{definition*}
    {Quotient representation (b)}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$ such that $W \subseteq V$ is a subrepresentation. We define a $G$-representation structure on the vector space $V/W:$
        \begin{equation*}
        \begin{split}
            \rho_{V/W}: G &\to \GL(V/W) \\
                        \rho_{V/W}(g)(v+W) &= \rho_V(g)(v) + W \in V/W
        \end{split}
        \end{equation*}
    \end{definition*}
    
    Fix $g \in G$ and say $v - v' \in W$. Then $\rho_V(g)(v-v') \in W$, hence $\rho_V(g)(v) \in \rho_V(g)(v') + W$.
    
    \begin{exercise}{}{10}
        We consider the above representation of $S_3$ on $\C^3$ and its subrep. $W = \spitz{e_1 + e_2 + e_3}$. Show that: $\C^3/W$ is an irreducible representation of $S_3$. How about replacing $3$ by any $n \in \N$?
    \end{exercise}

    \begin{proof}
        For $n=3$ we would need to have a $1$-dimensional subrepresentation $U/W$ that is $\rho_{V/W}(g)$-invariant. Say $U/W = \spitz{u + W}$, then $u + W = \rho_{V/W}(g)(u+W) = \rho_{V}(g)(u) + W$, hence $\rho_V(g)(u) - u \in W$ for all $g \in S_3$. If we take $u = (a,b,c)^t, g = (12)$, then:
        \begin{equation*}
            \rho_V(12)(u) - u = \round{\begin{array}{c}
                 b-a  \\
                 a-b \\
                 c
            \end{array}} \implies a-b = b-a = c \implies a = b \land c = 0
        \end{equation*}
        If we take $g = (13)$ we get $c = a$ and hence $u = (0,0,0)^t$ which yields a contradiction.
        
        It is actually irreducible for any $n \in \N$. We define the Hyperplane $H = \{x \in \C^n \ | \ \sum_{i=1}^n x_i = 0\}$, which is a vector space. Now we have $G$-structure on $H$ via:
        \begin{equation*}
        \begin{split}
            \rho_H(g): G \to \GL(H) \\
                        g \mapsto \rho_V(g)|_H
        \end{split}
        \end{equation*}
        
        which is well defined, since $\sum_{i=1}^n\rho_V(g)(x)_i = \sum_{i=1}^n x_{g(i)} = 0$. Now we show that $V/W \cong H$ as $G$-reps. For this, we consider the map
        \begin{equation*}
        \begin{split}
            \varphi: V/W &\to H \\
                    v + W &\mapsto w \in v + W \text{ with } \sum_{i=1}^n w_i = 0
        \end{split}
        \end{equation*}
        
        This $w$ exists since for any $v$, we set $w_i = v_i - \frac{1}{n}\sum_{i=1}^n v_i$ and it is unique since if there where two such $w,w' \in v +W$, then $w-w' \in W$, hence $w = w' + \lambda (e_1 + \dots + e_n)$, but
        \begin{equation*}
        \begin{split}
            0 = \sum_{i=1}^n w_i = \sum_{i=1}^n w_i' + \lambda = \lambda \cdot n \implies \lambda = 0 \implies w = w'
        \end{split}
        \end{equation*}
        
        One can easily check that there is an isomorphism of $G$-reps:
        
        \begin{equation*}
        \xymatrix@C=40pt @R=40pt {
            V/W \ar^{\rho_{V/W}(g)}[r] \ar[d]_{\varphi} & V/W\ar[d]^{\varphi} \\
            H \ar[r]_{\rho_H(g)} & H\ar@{}|{\circlearrowleft}[ul]
        }
        \end{equation*}
        
        Clearly $\mathbf{1} \oplus \rho_H = \rho_V$ and since $S_n$ acts doubly transitive on $[n]$, by Exercise 3 c) of sheet 5, $(\rho_H,H)$ is irreducible and so is $(\rho_{V/W},V/W)$.
    \end{proof}
     
    \begin{definition*}
    {Direct sum (c)}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$. The \textbf{direct sum} of vector spaces $V \oplus W$ is a $G$-rep. via
        \begin{equation*}
        \begin{split}
            \rho_{V \oplus W}: G &\to \GL(V \oplus W) \\
                            g &\mapsto \round{(v,w) \mapsto (\rho_V(g)(v),\rho_W(g)(w))}
        \end{split}
        \end{equation*}
    \end{definition*}
    This operation will be essential for decomposing representations.
    
    \begin{definition*}
    {Tensor product (d)}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$. The \textbf{tensor product} $V \tens W$ is a $G$-representation via:
        \begin{equation*}
        \begin{split}
            \rho_{V \tens W}: G &\to \GL(V \tens W) \\
                    \rho_{V \tens W}(g)(v \tens w) &= \rho_V(g)(v) \tens \rho_W(g)(w)
        \end{split}
        \end{equation*}
        and then use the universal property \ref{prop:2.1} to extend it to a linear map.
    \end{definition*}

    \begin{definition*}
    {Dual representation (e)}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$. The dual space $V^*$ admits a $G$-representation structure via:
        \begin{equation*}
        \begin{split}
            \rho_{V^*}: G &\to\GL(V^*) \\
                \spitz{\rho_{V^*}(g)(f),v} &= \spitz{f,\rho_V(g^{-1})(v)}
        \end{split}
        \end{equation*}
        where $\spitz{\bullet,\bullet}$ is the evaluation pairing $\Hom_\C(V,W) \times V \to W, \quad (f,v) \mapsto f(v) = \spitz{f,v}$
    \end{definition*}
    
    \begin{definition*}
    {Hom-space (f)}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$. The $\C$-vector space $\Hom_\C(V,W)$ is a $G$-representation via:
        \begin{equation*}
        \begin{split}
            \spitz{\rho_{\Hom}(g)(f),v} = \rho_W(g)\round{\spitz{f,\rho_V(g^{-1})(v)}}
        \end{split}
        \end{equation*}
        i.e. $\rho_\Hom(g)(f) = \rho_W(g) f \rho_V(g^{-1})$.
    \end{definition*}
    \begin{remark*}{}
        Using \ref{prop:2.2} we get this as a special case from the previous two cases.
    \end{remark*}
    
    Now that we defined objects, we need to look at morphisms between them.
    
    \begin{definition}{}{2.6}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$.
        
        A linear map $f: V \to W$ is called a \textbf{(homo)morphism of $G$-representations}, if $\forall g \in G$ the following diagram commutes:
        \begin{equation*}
        \xymatrix @C=30pt @R=30pt{
            V \ar^{\rho_V(g)}[r] \ar_{f}[d] & V\ar^{f}[d] \\
            W \ar_{\rho_W(g)}[r] & W\ar@{}|{\circlearrowleft}[ul]
        }
        \end{equation*}
        We let $\Hom_G(V,W)$ denote the set of homomorphisms of $G$-representations from $V$ to $W$. It is a $\C$-vector space.
        
        The representations $(V,\rho_V)$ and $(W,\rho_W)$ are called \textbf{isomorphic}, if there exists an $f \in \Hom_G(V,W)$ which is an isomorphism of vector spaces.
    \end{definition}
    
    \begin{exercise}{}{11}
        Let $(V,\rho_V), (W,\rho_W) \in \Rep(G)$ and $f \in \Hom_G(V,W)$.
        \begin{enumerate}
            \item Verify that $\Ker(f)$ is a subrep of $V$ and $\Img(f)$ is a subrep of $W$.
            \item Show that $f$ induces an isomorphism of $G$-reps: $\bar{f}: V/\Ker(f) \cong \Img(f)$.
            \item Let $V,W$ be finite dimensional vector spaces.
            
            Show that the linear map $\varphi: W \tens V^* \to \Hom_\C(V,W)$ from Proposition \ref{prop:2.2} is an isomorphism of $G$-reps. This explains why we define the $G$-structure on $\Hom_\C(V,W)$ as in (f). It follows from (d) and (e).  
        \end{enumerate}
    \end{exercise}
%    \begin{proof}
%        
%    \end{proof}
    The vector space $\Hom_G(V,W)$ of $G$-homs. is in fact an invariant space.
    \begin{definition*}{invariant space}
        For $V \in \Rep(G)$, we denote its \textbf{invariant space} $V^G := \{v \in V \ | \ \forall g \in G: \rho_V(g)(v) = v \}$.
    \end{definition*}

    \begin{lemma}{}{2.7}
        Let $V,W \in \Rep(G)$. The the $\C$-vector space $\Hom_G(V,W)$ and $\Hom_\C(V,W)^G$ are equal.
    \end{lemma}
    \begin{proof}
        \begin{equation*}
        \begin{split}
            f \in \Hom_\C(V,W)^G &\Leftrightarrow \forall g \in G: \rho_{\Hom}(g)(f) = f \\
            &\Leftrightarrow \forall g \in G: \rho_W(g) \circ f \circ \rho_V(g^{-1}) = f \\
            &\Leftrightarrow \forall g \in G: \rho_W(g) \circ f = f \circ \rho_V(g) \\
            &\Leftrightarrow f \in \Hom_G(V,W)
        \end{split}
        \end{equation*}
    \end{proof}

    \subsection{Group algebras}
        We can endow the vector space $\C(G)$ with an associative multiplication (extending the one of $G$ by linearity)
        \begin{equation*}
        \begin{split}
            \round{\sum_{g \in G} \lambda_g g } \cdot \round{\sum_{h \in G} \mu_h h} &= \sum_{g \in G} \sum_{h \in H} \lambda_g \mu_h gh \\
            &\overset{gh = k}{=} \sum_{g \in G} \sum_{k \in G} \lambda_g \mu_{g^{-1}k} k
        \end{split}
        \end{equation*}
        The vector space $\C(G)$, becomes a $\C$-algebra with this multiplication. We call it the group algebra and denote it by $\C[G]$.
        
        Another point of view on the group algebra is to look at the elements as functions on $G$.

        Let $\cF(G) = \{f:G \to \C \}$ be the set of functions on $G$ taking values in $\C$. It is straightforward to verify that $\cF(G)$ is a $\C$-vector space with a basis given by the Dirac functions $\delta_g \in \cF(G)$ where:
        \begin{equation*}
            \delta_g(h):=\left\{ \begin{array}{cc}
                1, & h=g \\
                0, & h \neq g
            \end{array}\right.
        \end{equation*}
        On $\cF(G)$ we study the \textbf{convolution product}:
        For $\varphi,\psi \in \cF(G)$ we set
        \begin{equation*}
        \begin{split}
            (\varphi \star \psi)(g) = \sum_{g_1g_2 = g} \varphi(g_1)\psi(g_2) = \sum_{h \in G} \varphi(h)\psi(h^{-1}g)
        \end{split}
        \end{equation*}
        Clearly $\varphi \star \psi \in \cF(G)$.
        It is also straightforward to verify that $(\cF(G),\star)$ is an associative $\C$-Algebra of $\C$-dimension $\#G$.

        \begin{proposition}{}{2.8}
            The $\C$-linear map
            \begin{equation*}
            \begin{split}
                {}^\circ : \cF(G) &\to \C[G] \\
                \varphi &\mapsto \varphi^\circ:= \sum_{g \in G}\varphi(g)g
            \end{split}
            \end{equation*}
            is an isomorphism of $\C$-algebras.
        \end{proposition}
        \begin{proof}
            The map is clearly injective. Since both sides have dimension $\#G$, it suffices to show that the map is an algebra homomorphism.
            Take $\varphi,\psi \in \cF(G)$ and $\lambda \in \C$, then
            \begin{equation*}
            \begin{split}
                \round{\varphi + \lambda \psi}^\circ = \sum_{g \in G} (\varphi + \lambda \psi)(g)g = \sum_{g \in G} (\varphi(g) + \lambda \psi(g))g = \sum_{g \in G} \varphi(g)g + \lambda \sum_{g \in G} \psi(g) =\varphi^\circ + \lambda \psi^\circ
            \end{split}
            \end{equation*}
            Furthermore
            \begin{equation*}
            \begin{split}
                \round{\varphi \star \psi}^\circ &= \sum_{g \in G}(\varphi \star \psi)(g)g = \sum_{g \in G} \sum_{h \in G} \varphi(h) \psi(h^{-1}g)g \overset{k = h^{-1}g}{=} \sum_{k \in G} \sum_{h \in G} \varphi(h) \psi(k) hk \\
                &= \sum_{h \in G}\sum_{k \in G} \varphi(h) \psi(k) hk = \round{\sum_{h \in G} \varphi(h)h} \cdot \round{\sum_{k \in G} \psi(k)k} = \varphi^\circ \cdot \psi^\circ 
            \end{split}
            \end{equation*}
        \end{proof}
        Although we will study the modules over an algebra in the next chapter in a detailed fashion, we give some hints on the connections between representations and modules in the following. Modules constitute a convenient way of studying representations.

        Recall that for a $\C$-algebra $A$, an $A$-module $M$ is a $\C$-vector space $M$ with a $\C$-algebra homomorphism $\lambda_M: A \to \End_\C(M)$.
        \begin{enumerate}
            \item Let $(V,\rho_V)$ be a representation of $G$. We define a $\C[G]$-module structure
            \begin{equation*}
            \begin{split}
                \tilde{\rho}_V: \C[G] &\to \End_\C(V) \\
                                \tilde{\rho}_V\round{\sum_{g \in G} \lambda_g g}(v) &= \sum_{g \in G} \lambda_g \rho_V(g)(v) \in V
            \end{split}
            \end{equation*}
            \item On the other hand, if $\lambda: \C[G] \to \End_\C(V)$ we obtain a $\C[G]$-module structure on $V$. We define a representation of $G$ on $V$ by
            \begin{equation*}
            \begin{split}
                \rho_V: \G &\to \GL(V) \\  
                        \rho_V(g)(v) &= \lambda(g)(v)
            \end{split}
            \end{equation*}
        \end{enumerate}
        One easily verifies that these two operations are mutually inverse. As a summary, we obtained a bijection:
        \begin{equation*}
            \Hom_{\mathrm{Grp}}(G,\GL(V)) \cong \Hom_{\C\mathrm{-alg}}(\C[G],\End(V))
        \end{equation*}
        \begin{exercise}{}{12}
            Verify that under this correspondence, homomorphisms of $G$-representations are exactly the $\C[G]$-module homomorphisms and vice versa.
        \end{exercise}
        \begin{proof}
            Let $M,N$ be $\C[G]$ modules (and in turn $G$-reps). Since the action of $\C[G]$ on $M, N$ is uniquely determined by $g.m = \rho(g)(m)$ (linear extension), a module homomorphism $\varphi: M \to N$ is $\C[G]$-linear, so it satisfies the diagram
            \begin{equation*}
            \xymatrix {
                M\ar[d]_{\varphi} \ar[r]^{g} & M \ar[d]^{\varphi} \\
                N \ar[r]^{g}& N\ar@{}|{\circlearrowleft}[ul]
            }
            \end{equation*}
            but this is exactly the diagram for a morphism of $G$-reps.
        \end{proof}
\separline{Week 6}

\section{Character theory}
    The first question to answer in representation theory is: Given two representations, are they isomorphic? To always construct isomorphisms between them seems complicated, proving that none exist even more so.
    
    The goal of character theory is to give a numerical criterion to determine whether two given representations are isomorphic or not.
    
    \subsection{Character of a representation}
        \begin{definition}{Character}{2.9}
            Let $V\in\rep(G)$. The \textbf{character} of $V$ is the function $\chi_V : G\to \C$ defined by:
            \begin{equation*}
                \chi_V(g)=\Tr(\rho_V(g))
            \end{equation*}
        \end{definition}
        Note that if $V\cong W$ as $G$-representations, then $\chi_V = \chi_W$.
        
        We start with some properties of characters.
        
        \begin{lemma}{}{2.10}
            Let $V,W\in\rep(G)$. The following hold:
            \begin{enumerate}
                \item For any $g,h\in G,\ \chi_V(ghg^{-1})=\chi_V(g)$, or equivalently $\chi_V(gh)=\chi_V(hg)$.
                \item $\chi_V(e)=\dim V$
                \item $\chi_{V\oplus W}=\chi_V+\chi_W$
                \item $\chi_{V\otimes W} = \chi_V \cdot \chi_W$
                \item for any $g\in G,\ \chi_V(g^{-1}) = \overline{\chi_V(g)}$
                \item $\chi_{V^*}=\overline{\chi_V}$
            \end{enumerate}
        \end{lemma}
        \begin{proof} Take $V,W \in \rep(G)$ and by $\chi_V,\chi_W$ denote their characters.
        
            \begin{enumerate}
                \item [(1)]
                    Take $g,h \in G$, then we have
                    \begin{align*}
                        \chi_V\round{ghg^{-1}}&=\Tr\round{\rho_V\round{ghg^{-1}}}\\
                        &=\Tr\round{\rho_V(g)\rho_V(h)\rho_V\round{g^{-1}}}\\
                        &=\Tr\round{\rho_V(g)\rho_V\round{g^{-1}}\rho_V(h)}\\
                        &= \Tr(\rho_V(h))\\
                        &= \chi_V(h)
                    \end{align*}

                \item [(2)]
                    We have $\rho_V(e)=\id_V$, hence $\chi_V(e) = \Tr(\rho_V(e)) = \dim V$
                
                \item [(3)]
                    We fix a basis $B_V = \round{v_1,\dots, v_n}$ of $V$ and a basis $B_W = \round{w_1\dots,w_m}$ of $W$. This yields a basis of $V \oplus W$, namely $B_{V \oplus W} := B_V \times \{0\} \cup \{0\} \times B_W$. %Let $M(\rho_V(g))$ and $M(\rho_W(g))$ be the matrices of $\rho_V(g)$ and $\rho_W(g)$ with respect to these bases.
                    From $\rho_{V\oplus W}(g)(v,w):=(\rho_V(g)(v),\rho_W(g)(w))$ it follows that with respect to this basis, the transformation matrix is of the form
                    \begin{align*}
                        \tensor[^{B_{V \oplus W}}]{\rho_{V\oplus W}(g)}{^{B_{V \oplus W}}} = \begin{pmatrix}
                        \tensor[^{B_V}]{\rho_V(g)}{^{B_V}} & 0 \\
                        0 & \tensor[^{B_W}]{\rho_W(g)}{^{B_W}}
                        \end{pmatrix}
                    \end{align*}
                    and therefore
                    \begin{equation*}
                    \begin{split}
                        \chi_{V\oplus W}(g) &= \Tr \round{ \tensor[ ^{B_{V \oplus W}} ] {(\rho_{V\oplus W}(g)} {^{B_{V \oplus W}}}} \\
                        &= \Tr \round{\tensor[^{B_V}]{\rho_V(g)}{^{B_V}}} + \Tr \round{\tensor[^{B_W}]{\rho_W(g)}{^{B_W}}} \\
                        &= \chi_V(g) + \chi_W(g)
                    \end{split}
                    \end{equation*}
                \item [(4)]
                    As above, we fix bases $\round{v_1,\dots, v_n}$ of $V$ and $\round{w_1\dots,w_m}$ of $W$. Then $B_{V \tens W} = \round{v_i\otimes w_j \ | \ i \in [n], j \in [m]}$ is a basis of $V \otimes W$. We fix the order of $B_{V\otimes W}$ as
                    \begin{equation*}
                        v_1\otimes w_1, \dots, v_1\otimes w_n, v_2\otimes w_1, \dots, v_n\otimes w_1, \dots, v_n\otimes w_m
                    \end{equation*}
                    and set $A := \tensor[^{B_V}]{\rho_V(g)}{^{B_V}} = (a_{i,j}), B: = \tensor[^{B_W}]{\rho_W(g)}{^{B_W}}$.
                    The transformation matrix of $\rho_{V\otimes W}(g)$ under $B_{V \tens W}$ is:
                    \begin{equation*}
                        \tensor[^{B_{V \tens W}}]{\rho_{V \tens W}(g)}{^{B_{V \tens W}}} = \begin{pmatrix}
                            a_{1,1}B & a_{1,2}B & \dots & a_{1,n}B \\
                            \vdots & \vdots & & \vdots \\
                            a_{n,1}B & a_{n,2}B & \dots & a_{n,n}B
                        \end{pmatrix} = A \tens B
                    \end{equation*}
                    the Kronecker product of $A$ and $B$ (see \cite{LA} for further reference).
                    Indeed one calculates (I think there is a mistake in the notes, I wrote my version)
                    \begin{equation*}
                    \begin{split}
                        \rho_{V \tens W}(g)(v_i \tens w_j) &= \rho_V(v_i) \tens \rho_W(w_j) = \round{\sum_{k=1}^n a_{ki}v_k }\tens \round{\sum_{l=1}^m b_{lj}w_l } \\
                        &= \sum_{k=1}^n a_{ki} \round{v_k \tens \sum_{l=1}^m b_{lj} w_l} = \sum_{k=1}^n a_{ki} \sum_{l=1}^m b_{lj} \round{v_k \tens w_l } 
                    \end{split}
                    \end{equation*}
    
                    So with the ordering we choose, we get:
                    \begin{equation*}
                        \rho_{V \tens W}(g)(v_i \tens w_j)_{B_{V \tens W}} =
                        \begin{pmatrix}
                            a_{1,i} b_{1j} \\
                            a_{1,i} b_{2j} \\
                            \vdots \\
                            a_{1,i} b_{mj} \\
                            a_{2,i} b_{1,j} \\
                            \vdots \\
                            a_{n,i} b_{m,j}
                        \end{pmatrix}
                    \end{equation*}
    
                    Which is precisely the $j$-th row of the $i$-th Block of $A \tens B$. Therefore
                    
                    \begin{equation*}
                    \begin{split}
                        \chi_{V\otimes W}(g) &= \Tr \round{\tensor[^{B_{V \tens W}}]{\rho_{V \tens W}(g)}{^{B_{V \tens W}}}} = \Tr(A\otimes B) = \Tr(a_{11}B) + \dots + \Tr(a_{nn}B) \\
                        &= \Tr(A)\Tr(B) = \chi_V(g) \cdot \chi_W(g) = (\chi_V \cdot \chi_W)(g)
                    \end{split}
                    \end{equation*}
                \item[(5)]
                    We first show that the eigenvalues of $\rho_V(g)$ are roots of unity. Since $G$ is a finite group for any $g \in G$, there exists an $m \in \N$ such that $g^m = e$ (every element in $G$ has finite order). This implies $\rho_V(g)^m  \id_V$, hence the minimal polynomial of $\rho_V(g) \in \End(V)$ divides $t^m - 1$. It then follows that the eigenvalues of $\rho_V(g)$ are roots of $t^m - 1$ hence they are roots of unity.

                    We let $\lambda_1,\dots,\lambda_m$ denote the eigenvalues of $\rho_V(g)$ (with multiplicity), then $\lambda_i^{-1} = \lambda_i$ and
                    \begin{equation*}
                        \overline{\chi_V(g)} = \overline{\lambda_1 + \dots + \lambda_m} = \overline{\lambda_1} + \dots + \overline{\lambda_m} = \lambda_1^{-1} + \dots + \lambda_m^{-1} = \chi_V(g^{-1})
                    \end{equation*}
                    Where we used that $\rho_V(g^{-1}) = \rho_V(g)^{-1}$.
                \item[(6)]
                    Form $(5)$ it suffices to show that for any $g\in G$, we have $\chi_{V^*}(g) = \chi_V(g^{-1})$. We choose a basis $(v_1,\dots,v_n)$ of $V$ and the corresponding dual basis $(v_1^*,\dots,v_n^*)$ of $V^*$. We can write $\rho_{V^*}(g)(v_i^*) = \sum_{j=1}^n b_{ij} v_j^*$, for some $b_{ij} \in \C$. Therefore
                    \begin{equation*}
                        \spitz{\rho_{V^*}(g)(v_i^*),v_j} = b_{ij}
                    \end{equation*}
                    According to the $G$-representation structure on $V^*$, we have
                    \begin{equation*}
                        \spitz{v_i^*,\rho_V(g^{-1})(v_j)} = b_{ij}
                    \end{equation*}
                    That is to say, $\rho_V(g^{-1})(v_j) = \sum \limits_{i=1}^n b_{ij} v_i$, hence
                    \begin{equation*}
                        \chi_{V^*}(g) = \Tr(\rho_{V^*}(g)) = \sum_{i=1}^n b_{ii} = \Tr(\rho_V(g^{-1})) = \chi_V(g^{-1})
                    \end{equation*}
            \end{enumerate}
        \end{proof}
        \begin{corollary}{}{2.11}
            For $V,W \in \rep(G)$, we have $\chi_{\Hom(V,W)} = \chi_W \cdot \overline{\chi_V}$
        \end{corollary}
        \begin{proof}
            By Exercise \ref{exe:11} (3), $\Hom(V,W) \cong W \tens V^*$, as $G$-reps, therefore using Lemma \ref{lem:2.10}:
            \begin{equation*}
                \chi_{\Hom(V,W)} = \chi_{W \tens V^*} = \chi_W \cdot \chi_{V^*} = \chi_W \cdot \overline{\chi_V}
            \end{equation*}
        \end{proof}
\subsection{Central functions}
    We put characters in a larger family by studying functions on $G$ satisfying the condition in Lemma \ref{lem:2.10} (1). They are the key to opening the door to character theory.

    \begin{definition}{central/class function}{2.12}
        A function $f: G \to \C$ is called \textbf{central}, if it is constant on each conjugacy class of $G$, i.e. for all $g,h \in G: f(ghg^{-1}) = f(h)$ or equivalently $f(gh) = f(hg)$. We let $\CF(G)$ denote the set of central functions on $G$. It is a $\C$-vector space.
    \end{definition}
    In the literature, the central functions are also called class functions. We will soon explain why we use the name "central".

    We let $\cC(G)$ denote the set of conjugacy classes in $G$. Then a class function $f:G \to \C$ induces a function $f: \cC(G) \to \C$.
    
    Before going further, we look at some examples.
    \begin{example}{}{2.13}
        \begin{enumerate}
            \item
                For $V \in \rep(G)$, $\chi_V \in \CF(G)$.
            \item
                Let $C \in \cC(G)$, we define
                \begin{equation*}
                \begin{split}
                    \gamma_C: G &\to \C \\
                            g &\mapsto \left\{ \begin{array}{cl}
                                1, & \text{if } g \in C  \\
                                0, & \text{else}
                            \end{array} \right.
                \end{split}
                \end{equation*}
                Then $\gamma_C \in \CF(G)$.
        \end{enumerate}
    \end{example}
    
    We recall the isomorphism
    \begin{equation*}
    \begin{split}
        \cF(G) &\to \C[G] \\
            f &\mapsto f^\circ = \sum_{g \in G}f(g)g
    \end{split}
    \end{equation*}
    Since $\CF(G) \subseteq \cF(G)$, we can examine its image in $\C[G]$.
    \begin{definition*}{center}
        For a monoid $M$, we denote its center $Z(M) = \{m \in M \ | \ \forall n \in M: mn = nm\}$.
    \end{definition*}
    Since $\C[G]$ is a monoid with $\cdot$, $Z(\C[G]) = \{x \in \C[G] \ | \ \forall y \in \C[G]: xy = yx \}$.

    \begin{corollary}{}{2.14}
        When restricted to $\CF(G)$, the above isomorphism gives on isomorphism of $\C$-algebras
        \begin{equation*}
            \CF(G) \overset{\cong}{\to} Z(\C[G])
        \end{equation*}
    \end{corollary}
    \begin{proof}
        Given $\alpha \in CF(G)$, we first verify that its image lands in $Z(\C[G])$, that is to say, $\forall h \in G$:
        \begin{equation*}
            h \round{\sum_{g \in G} \alpha(g) g} = \round{\sum_{g \in G} \alpha(g)g}h
        \end{equation*}
        It suffices to show that $\sum_{g \in G} \alpha(g) h g h^{-1} = \sum_{g \in G} \alpha(g)$.
        
        Since $\inn_h: G \to G, g \mapsto hgh^{-1}$ is a bijection, the left hand side reads:
        \begin{equation*}
            \sum_{g \in G} \alpha(g) h g h^{-1} = \sum_{g \in G} \alpha(hgh^{-1}) h g h^{-1} = \sum_{k \in G} \alpha(k) k = \text{RHS}
        \end{equation*}
        Where RHS means right-hand side, i.e. we have shown equality.

        Thus we obtained a $\C$-algebra homomorphism $\CF(G) \to Z(\C[G])$ which is injective. We show that it is also surjective:

        Take $x = \sum_{\lambda_g} g \in Z(\C[G])$ and $h \in G$, since $hx^h{-1} = x$ it follows that
        \begin{equation*}
            \sum_{g \in g} \lambda_g h g h^ {-1} = \sum_{g \in G} \lambda_g g
        \end{equation*}
        which implies that $\lambda_g = \lambda_{hgh^{-1}}$ by comparing coefficients in $\C[G]$. So the coefficients of $x$ are constant on each conjugacy class which means
        \begin{equation*}
            \lambda: G \to \C, g \mapsto \lambda_g 
        \end{equation*}
        is the central function with image $x$.
    \end{proof}

    Our goals are:
    \begin{enumerate}
        \item Study different vector space bases of $\CF(G)$.
        \item Define a hermitian form on $\CF(G)$ to get an orthonormal basis.
    \end{enumerate}

    \begin{lemma}{}{2.15}
        The tuple $(\gamma_C \ | \ C \in \cC(G))$ form a basis of $\CF(G)$. In particular, $\dim_\C(\CF(G)) = \# \CF(G)$.
    \end{lemma}
    \begin{proof}
        \underline{Generating:}
            Let $f \in \CF(G)$. Since $f$ is constant on each conjugacy class $C \in \cC(G)$, we let $f(C)$ denote this value. Then we can write
            \begin{equation*}
                f = \sum_{C \in \cC(G)} f(C) \gamma_C
            \end{equation*}
        \underline{Independence:}
            Assume $\sum_{C \in \cC(G)} \lambda_C \gamma_C = 0$ for $\lambda_C \in \C$. We fix $C_0 \in \cC(G)$ and choose $g \in C_0$, then
            \begin{equation*}
                0 = \sum_{C \in \cC(G)} \lambda_C \gamma_C(g) = \lambda_{C_0} \gamma_{C_0}(g) = \lambda_{C_0}
            \end{equation*}
    \end{proof}
    \begin{corollary}{}{2.16}
        The $\C$-vector space $Z(\C[G])$ has a basis $\round{\sum\limits_{g \in C} g \ | \ C \in \cC(G)}$. In particular $\dim_\C(Z(\C[G])) = \# \cC(G)$.
    \end{corollary}
    \begin{proof}
        Combining Corollary \ref{cor:2.14} and Lemma \ref{lem:2.15}, $\round{\gamma_C^\circ \ | \ C \in \cC(G)}$ forms a basis of $\Z(\C[G])$. It suffices to notice that $\gamma_C^\circ = \sum\limits_{g \in G} \gamma_C(g)g = \sum\limits_{g \in G} g$
    \end{proof}

    We introduce a Hermitian form on the $\C$-vector space $\CF(G)$.
    \begin{definition}{}{2.17}
        Let $\varphi,\psi \in \CF(G)$, we define:
        \begin{equation*}
            \spitz{\varphi,\psi} := \frac{1}{\#G} \sum_{g \in G} \varphi(g) \overline{\psi(g)} \in \C
        \end{equation*}
    \end{definition}
    It is straightforward to verify that $\spitz{\_,\_}$ is a Hermitian form (i.e. linear on the first component and $\spitz{\varphi,\psi} = \overline{\spitz{\psi,\varphi}}$).
    In view of Corollary \ref{cor:2.11}, we have:
    \begin{equation*}
        \spitz{\chi_W,\chi_V} = \frac{1}{\#G} \sum_{g \in G} \chi_{\Hom(V,W)}(g)
    \end{equation*}
    If we extend $\chi_{\Hom(V,W)}$ by linearity to $\C(G)$, i.e.
    \begin{equation*}
        \chi_{\Hom(V,W)}\round{\sum_{g \in G} \lambda_g g} = \sum_{g \in G} \lambda_g \chi_{\Hom(V,W)}(g)
    \end{equation*}
    Then we can rewrite the above formula as:
    \begin{equation*}
        \spitz{\chi_W,\chi_V} = \chi_{\Hom(V,W)} \round{\frac{1}{\#G} \sum_{g \in G}g}
    \end{equation*}
    this special element $\frac{1}{\#G} \sum\limits_{g \in G}g$ will be crucial in the character theory. One should keep the above formula in mind.
    \begin{example}{}{2.18}
        We compute $\spitz{\gamma_C,\gamma_{C'}}$ for $C,C' \in \cC(G)$.
        Recall the formula
        \begin{equation*}
            \spitz{\gamma_C,\gamma_{C'}} = \frac{1}{\#G} \sum_{g \in G}\gamma_C \overline{\gamma_{C'}(g)}
        \end{equation*}
        If $C \neq C'$, then for all $g \in G$, either $\gamma_C(g) = 0$ or $\gamma_{C'}(g) = 0$ hence $\spitz{\gamma_C,\gamma_{C'}}$.
        
        If $C = C'$, then
        \begin{equation*}
            \spitz{\gamma_C,\gamma_{C'}} = \frac{1}{\#G}\sum_{g \in G} |\gamma_C(g)|^2 = \frac{1}{\#G} \sum_{g \in C} |\gamma_C(g)|^2 = \frac{\#C}{\#G}
        \end{equation*}
        As a summary: $\spitz{\gamma_C,\gamma_{C'}} = \frac{\#C}{\#G} \delta_{C,C'}$. Hence the basis $(\gamma_C \ | \ C \in \cC(G)) $ is orthogonal, but not orthonormal.
    \end{example}

    We introduce another basis of $\CF(G)$.

    Let $\Irr(G)$ be the set of finite dimensional irreducible representations of $G$ up to isomorphism. That is to say, isomorphic representations are identified as the same object in $\Irr(G)$.
    \begin{theorem}{}{2.19}
        The tuple $(\chi_V \ | \ V \in \Irr(G))$ forms an orthonormal basis of $\CF(G)$.
    \end{theorem}
    This is the first main result of this chapter. We delay its proof to chapter \textit{2.4}.

    \begin{corollary}{}{2.20}
        \begin{enumerate}
            \item
                The number of finite dimensional irreducible representations of $G$ (up to isomorphism) coincides with $\dim_\C(\CF(G)), \cC(G)$ and $\dim_\C(Z(\C[G]))$.
            \item
                Let $\Irr(G) = \{S_1,\dots,S_t\}$ and $V = \bigoplus_{i=1}^t S_i^{\oplus n_i}$. Then $n_i = \spitz{\chi_V,\chi_{S_i}}$.
        \end{enumerate}
    \end{corollary}
    \begin{proof}
        We start with the proof of the Corollary \ref{cor:2.20}.
        \begin{enumerate}
            \item[(1)] It follows from $\round{\chi_V \ | \ V \in \Irr(G)}$ is a basis of $\CF(G)$.
            \item[(2)] By Lemma \ref{lem:2.10} (3) and Theorem \ref{thm:2.19}
            \begin{equation*}
                \spitz{\chi_V,\chi_{S_i}} \overset{\ref{lem:2.10}}{=} \spitz{\sum_{j=1}^t n_j \chi_{S_j},\chi_{S_i}} = \sum_{j=1}^t n_j \spitz{\chi_{S_j},\chi_{S_i}} \overset{\ref{thm:2.19}}{=} n_i 
            \end{equation*}
        \end{enumerate}
    \end{proof}

\section{Projector, Maschke and Schur}
    We have a closer look at the element $P:=\frac1{\#G}\sum_{g\in G}g$. This element is in the image of $\frac 1{\#G}\sum_{C\in\cC(G)}\gamma_C$ in $Z(\C[G])$, hence $\forall g\in G:\ gP=Pg=P$, and in particular, $P^2=P$. We call $P$ the \textbf{projector}.
    \begin{definition}{idempotent endomorphism}{2.21}
        An endomorphism $\varphi\in\End(G)(V)$ is called \textbf{idempotent}, if $\varphi^2=\varphi$.
    \end{definition}
    Idempotents can be used to decompose representations:

    \begin{lemma}{}{2.22}
        Let $(V,\rho_V) \in \rep(G)$ and $\varphi\in\Hom_G(V,V)$ be idempotent. Then $V\cong_G\Ker\varphi\oplus\Img\varphi$.
    \end{lemma}
    \begin{proof}
        From Linear Algebra \cite{LA}
        \begin{equation*}
        \begin{split}
            \alpha: V &\to \Ker\varphi\oplus\Img\varphi \\
            v &\mapsto \round{v-\varphi(v), \varphi(v)}
        \end{split}
        \end{equation*}
        is an isomorphism in the category of vector spaces. It suffices to show that $\alpha$ is a homomorphism of $G$-representations. So for $g\in G$, we need to show that
        $\rho_{\ker\varphi\oplus\Img\varphi}(g)(\alpha(v))= \alpha(\rho_V(g)(v))$. 
        Indeed
        \begin{align*}
            \alpha(\rho_V(g)(v))&=\round{\rho_V(g)(v)-\varphi(\rho_V(g)(v)), \varphi(\rho_V(g)(v))}\\
            &=\round{\rho_V(g)(v)-\rho_V(g)(\varphi(v)), \rho_V(g)(\varphi(v))}\\
            &=\round{\rho_v(G)(v-\varphi(v)),\rho_V(g)(\varphi(v))}\qquad \text{since $\ker\varphi, \Img\varphi$ are subrepresentations of $(V,\rho)$}\\
            &= \rho_{\ker\varphi\oplus\Img\varphi}(g)(\alpha(v))
        \end{align*}
    \end{proof}

    The projector can be used to produce an idempotent in $\Hom_G(V,V)$.

    \begin{corollary}{}{2.23}
        For $(V,\rho_V) \in\rep(G)$, we have $\Img\tilde\rho_V(P)=V^G$.
        In particular, for $V,W\in\rep(G)$, 
        $$\tilde\rho_{\Hom(V,W)}(P): \Hom(V,W)\to \Hom(V,W)$$ has an image in $\Hom_G(V,W)$.
    \end{corollary}
    \begin{proof}
        For "$\subseteq$" we take $v\in V$, then for any $g\in G$,
        $$\rho_V(g)(\tilde\rho_V(P)(v)) = \tilde{\rho}_V(g) \circ \tilde{\rho}_V(P)(v) = \tilde\rho_V(gP)(v) = \tilde\rho_V(P)(v)$$
        hence $\rho_V(g)\round{\tilde\rho_V(P)(v)}\in V^G$.
        
        The other inclusion "$\supseteq$" is clear: if $v\in V^G$, then $\tilde\rho_V(P)(v)=\sum\frac1{\#G}\sum_{g\in G}\rho_V(g)(v)=v$
        The statement on $\Hom(V,W)$ follows from \ref{lem:2.7}.
    \end{proof}

    As an application of the projector, we can prove the following fundamental theorem in group representation theory:

    \begin{theorem}{Maschke}{maschke}
        Any finite-dimensional representation of $G$ is isomorphic to a direct sum of irreducible representations.
    \end{theorem}
    \begin{proof}
        We can proceed by induction on the dimension of the representation. The one-dimensional case is clear.

        Let $V\in\rep(G)$. If $V$ is irreducible then we are done. If not there is a subrepresentation $W$ of $V$ of dimension $\geq 1$. $W$ is a subspace of $V$. As vector spaces, we have a direct sum composition $V=W\oplus W'$. We denote with $\pi: V \to W$ the canonical projection and $\iota: W\to V$ the canonical injection arising from the direct sum. Then $\varphi=\iota\circ\pi$ is an idempotent in $\Hom(V,V)$. Let $\tilde P:=\tilde \rho_{\End(V)}(P):\Hom(V,V)\to \Hom_G(V,V)$ be the projection in \ref{cor:2.23}. Then $\tilde P(\varphi)$ is an idempotent in $\Hom_G(V,V)$ with image $W$ ($W$ is a subrepresentation, hence $\frac1{\# G}\sum_{g\in G}\rho(g)(\varphi)$ has image $W$.)\footnote{Verify that $\Img \tilde{\rho} (\varphi)=W$, then for any $v\in V: \tilde\rho(\varphi)^2(v)=\tilde\rho(\varphi)\underbrace{\round{\tilde\rho(\varphi)(v)}}_{\in W} = \tilde\rho(\varphi)(v)$.}
        \ref{lem:2.22} gives an isomorphism $V\cong \ker\tilde P(\varphi)\oplus W$ as $G$-representations. Then we can proceed by induction.
    \end{proof}

    \begin{remark}{}{2.25}
        The projector is well-defined if we replace $\C$ by any field $K$ whose characteristic does not divide $\#G$. Hence Maschke's theorem holds over such fields.
    \end{remark}

    Another tool to prove \ref{thm:2.19} is the following lemma by Schur:

    \begin{lemma}{Schur}{schur}
        Let $V,W \in \rep(G)$ be two irreducible representations. Then
        $$\dim_\C\Hom_G(V,W)=\begin{cases}
            1 & V\cong_G W\\
            0 & \text{otherwise}
        \end{cases}$$
    \end{lemma}
    \begin{proof}
        Let $\varphi: V \to W$  be a nonzero homomorphism of $G$-representations.
        \begin{enumerate}
            \item Assume that $f:V\cong_G W$. For any $t\in\C$, $f\inv\circ\varphi-t\cdot\id_V\in\Hom_G(V,V)$ and hence $\ker(f\inv\circ\varphi-t\id_V)$ is a subrepresentation of $V$. Let $\lambda$ be an eigenvalue of $f\inv\circ\varphi:V\to V$. Then $\ker(f\inv\circ\varphi-\lambda\id_V) = V$ because $V$ is irreducible. That is to say, $\varphi=\lambda f$.
            \item Assume that $V\not\cong_G W$. Then $\ker\varphi$ is a subrepresentation of of $W$. since $\varphi\neq 0$, $\varphi$ is injective. Moreover, since $\Img\varphi$ is a subrepresentation of $W$ and $\varphi\neq 0$, $\varphi$ is surjective. This contradicts $V\not\cong W$, and hence such a map $\varphi$ does not exist. It follows $\Hom_G(V,W)=\curly 0$.
        \end{enumerate}
    \end{proof}
    \begin{corollary}{}{2.27}
        For $V\in\rep(G)$ irreducible and $z\in Z(\C[G])$, then $\tilde\rho_V(z) \in \Hom_G(V,V)$ equals to a scalar map $\lambda\id_V$
    \end{corollary}

    \begin{proof}
        Since $z \in Z(\C[G])$, for all $g \in G:$
        \begin{equation*}
            \rho_V(g) \tilde \rho_V(z) = \tilde \rho_V(z) \rho_V(g)
        \end{equation*}
        so $\rho_V(z)$ is $G$-equivariant. By Schur's Lemma \ref{lem:schur}, $\dim \Hom_G(V,V) = 1$, hence $\tilde \rho_V(z)$ is a scalar map.
    \end{proof}
    
\section{Proof of Theorem \ref{thm:2.19}}
    We first show that for $V,W\in\Irr(G)$ $$\spitz{\chi_W,\chi_V}=\delta_{V,W}$$
    We have seen that $$\spitz{\chi_W,\chi_V}=\chi_{\Hom(V,W)}(P)=\Tr\round{\tilde\rho_{\Hom(V,W)}(P)}$$

    Note that $P$ is an idempotent and $\tilde\rho_{\Hom(V,W)}(P)$ is a projection from $\Hom(V,W)$ to $\Hom_G(V,W)$. Therefore the trace of $\tilde\rho_{\Hom(V,W)}(P)$ equals to  $\dim_\C\Hom_G(V,W)=\delta_{V,W}$ by Schur's lemma (the eigenvalues of idempotents are 0 and 1). From this orthogonality, it follows, that characters of irreducible representations are linearly independent.

    It remains to show the generating property.

    \begin{claim*}{}
        If $\alpha\in\text{CF}(G)$ satisfies $\spitz{\alpha,\chi_V}=0\ \forall V\in\Irr(G)\Rightarrow \alpha=0$.
    \end{claim*}
    \begin{proof}
        Let $V\in\Irr(G)$. By \ref{cor:2.14}, $\alpha^\circ\in Z(\C[G])$ and by \ref{cor:2.27}, $\tilde\rho_V(\alpha)$ is a scalar $\lambda\id_V$. We determine this scalar $\lambda$.
        \begin{align*}
            \lambda\dim V &= \Tr(\tilde\rho_V(\alpha(\alpha^\circ))= \sum_{g\in G} \alpha(g) \chi_V(g)\\
            &\overset{\ref{lem:2.10}}= \#G\frac1{\#G}\sum_{g\in G}\alpha(g)\chi_{V^*}(g)\\
            &=\#G\cdot\spitz{\alpha, \chi_{V^*}}=0
        \end{align*}
        For the last equality, we used Maschke to write $V^*$ as the sum of irreducible representations. For each of their characters, the scalar product vanishes.

        From this computation, $\lambda=0$. We have shown, that for any irreducible representation $V$, $\tilde\rho_V(\alpha\circ)=0$. Again Maschke tells us that for any $W\in\rep(G)$, $\tilde\rho_W(\alpha\circ)=0$.
        To complete the proof, we examine a special choice of $W$: The left regular representation. Let $W = \C(G)$. Then
        \begin{equation*}
            0 = \tilde{\rho}_W(\alpha^\circ)(e) = \sum_{g \in G} \alpha(g) \rho_W(g)(e) = \sum_{g \in G} \alpha(g)g
        \end{equation*}
        Since elements in $G$ form a basis of $\C(G)$, for any $g \in G$, we have $\alpha(g) = 0$
    \end{proof}
    We deduce several immediate consequences.
    \begin{corollary}{}{2.28}
        \begin{enumerate}
            \item
                Let $V,V' \in \rep(G)$. $V$ is isomorphic to $V'$ as a $G$-representation if and only if $\chi_V = \chi_{V'}$.
            \item
                Let $V \in \rep(G)$. $V$ is irreducible if and only if $\spitz{\chi_V,\chi_V} = 1$
        \end{enumerate}
    \end{corollary}
    \begin{proof}
        Ad \textit{(1)}:
        
        Let $\Irr(G) = \{S_1,\dots,S_t\}$. We use Maschke's theorem \ref{thm:maschke} to write $V \cong S_1^{\oplus n_1} \oplus \dots \oplus S_t^{\oplus n_t}$ and $V' \cong S_1^{\oplus m_1} \oplus \dots \oplus S_t^{\oplus m_t}$. From Corollary \ref{cor:2.20} (2), $n_i = \spitz{\chi_V,\chi_{S_i}} = \spitz{\chi_{V'},\chi_{S_i}} = m_i$ and hence $V \cong V'$. The other direction is clear.

        Ad \textit{(2)}:
        
        With the above notation and Corollary \ref{cor:2.20} (2), $\spitz{\chi_V,\chi_V }= \sum_{i=1}^t n_i^2 = 1$, which is equivalent to say that in the decomposition there is only one irreducible representation appearing.
    \end{proof}


\separline{Week 7}
    \begin{corollary}{Second orthogonal relation}{2.29}
        Let $\Irr(G) = \{S_1,\dots,S_t\}$ with characters $\chi_i := \chi_{S_i}$. For $g \in G$, let $C_g \in \cC(G)$ be such that $g \in C_g$, then
        \begin{equation*}
            \sum_{i=1}^t \chi_i(g) \overline{\chi_i(g)} = \frac{\#G}{\#C_g}
        \end{equation*}
        and if $h \not \in C_g$, then
        \begin{equation*}
            \sum_{i=1}^t \chi_i(g) \overline{\chi_i(h)} = 0
        \end{equation*}
    \end{corollary}
    \begin{proof}
        We fix $s \in G$ and $C_s \in \cC(G)$ with $s \in C_s$. We expand $\gamma_{C_s}$ in the orthonormal basis $(\chi_1,\dots,\chi_t)$. We have $\gamma_{C_S} = \sum_{i=1}^t \lambda_i \chi_i$, where $\lambda_i = \spitz{\gamma_{C_s},\chi_i} = \frac{1}{\#G} \sum_{g \in G} \gamma_{C_s}(g) \overline{\chi_i(g)} $.

        The sum is in fact over $C_s$ and $\gamma_{C_s}, \chi_i$ are central functions. Therefore $\lambda_i  = \frac{\#C_s}{\#G}\overline{\chi_i(s)}$. Hence
        \begin{equation*}
            \gamma_{C_s} = \sum_{i=1}^t \frac{\#C_s}{\#G} \overline{\chi_i(s)} \chi_i
        \end{equation*}
        If $g \in C_s$, then
        \begin{equation*}
            \sum_{i=1}^t \chi_i(s) \overline{\chi_i(g)} = \sum_{i=1}^t \chi_i(g) \overline{\chi_i(g)} = \frac{\#G}{\#C_s}
        \end{equation*}
        otherwise
        \begin{equation*}
            \sum_{i=1}^t \chi_i(g) \overline{\chi_i(s)} = 0
        \end{equation*}
    \end{proof}

\section{Character tables}
    It is usually convenient to put the values of characters into a table.

    The columns of such a table are indexed by conjugacy classes and the rows are indexed $\Irr(G)$. For a conjugacy class $C$ and an irreducible representation $S$, the corresponding element in the table reads $\chi_S(C)$.

    For example, we compute such a table for $S_3$. There are three conjugacy classes in $S_3$:
    \begin{equation*}
        C_1 = \{e\}, \quad C_2 = \{(12),(13),(23)\}, \quad C_3 = \{(123),(132)\}
    \end{equation*}
    Then there are three elements in $\Irr(S_3)$:
    We denote them by $S_1, S_2, S_3$, where $S_1$ is the trivial representation.
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|}
        \hline
             & $C_1$ & $C_2$ & $C_3$  \\ \hline
            $\rho^{\text{triv}}$ & $1$ & $1$ & $1$  \\ \hline
            $S_2$ & $a$ & $b$ & $c$ \\ \hline 
            $S_3$ & $d$ & $e$ & $f$ \\ \hline
        \end{tabular}
    \end{table}

    Theorem \ref{thm:2.19} implies that the rows of the table are "weighted" orthogonal. For example,
    \begin{equation*}
        0 = \spitz{\chi_1,\chi_2} = \#C_1 \chi_1(C_1) \overline{\chi_2 (C_1)} + \#C_2 \chi_1(C_2) + \overline{\chi_2(C_2)} + \#C_3 \chi_1(C_3) + \overline{\chi_2(C_3)} = \overline{a} + 3 \overline{b} + 2 \overline{c} 
    \end{equation*}
    Therefore $a + 3b + 2c = 0$.
    We choose $C_1 = \{e\}, C_2 = \{(12),(13),(23)\}$ and $C_3 = \{(123),(132)\}$
    Corollary \ref{cor:2.29} implies that the columns of the table also have a certain orthogonality. If we look at $C_1$ and itself, the first part of Corollary \ref{cor:2.29} gives:
    \begin{equation*}
        1 + a^2 + d^2 = \#G = 6
    \end{equation*}
    We notice that $\chi_{S_i}(e) = \dim_\C(S_i) \in \N_0$ and choose a solution $a=1$, $d=2$ (the only other solution is $a=2$, $d=1$).
    We get
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|}
        \hline
             & $\{e\}$ & $\{(12),(23),(23)\}$ & $\{(123),(132)\}$  \\ \hline
            $\rho^{\text{triv}}$ & $1$ & $1$ & $1$  \\ \hline
            $S_2$ & $1$ & $b$ & $c$ \\ \hline 
            $S_3$ & $2$ & $e$ & $f$ \\ \hline
        \end{tabular}
    \end{table}

    
    If we look at the first and second columns, the corollary gives:
    \begin{equation*}
        1 + a\overline{b} + d \overline{e} = 0 \implies 1 + b + 2e = 0
    \end{equation*}
    The irreducible representation $S_2$ is one-dimensional.
    Elements in $C_i$ have order $i$. Since any $\sigma \in C_2$ satisfies $\rho_{S_2}(\sigma)^2 = \id_{S_2}$, $\rho_{S_2}(\sigma)$ has eigenvalue either $1$ or $-1$. A similar argument shows that for $\tau \in C_3$, $\rho_{S_2}(\tau)$ has third roots of unity as eigenvalues.

    For the sake of contradiction let us assume that $b = \chi_{S_2}(\sigma) = 1$, then it follows from $1 + 3b + 2c = 0$, that $c = \chi_{S_2}(\tau) = -\frac{1}{2} (1 + 3b) = -\frac{1}{2} (4) = -2$, which contradicts the fact that $c$ is a (third) root of unity. So we get $b = -1$ and $c = 1$.
    We obtained two equations of $e$ and $f$:
    \begin{equation*}
        \left\{ \begin{array}{cc}
             2 + 3e + 2f = 0  \\
             2 - 3e + 2f = 0 
        \end{array}\right. \implies \left\{ \begin{array}{cc}
             e = 0  \\
             f = -1 
        \end{array}\right.
    \end{equation*}
    The complete character table of $S_3$ is then:
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|}
        \hline
             & $C_1$ & $C_2$ & $C_3$  \\ \hline
            $\chi_{\triv}$ & $1$ & $1$ & $1$  \\ \hline
            $\chi_{S_2}$ & $1$ & $-1$ & $1$ \\ \hline 
            $\chi_{S_3}$ & $2$ & $0$ & $-1$ \\ \hline
        \end{tabular}
    \end{table}

    If you look at this character table, you only see integers and it is tempting to guess that: for any $g \in S_n, \chi_V(g) \in \Z$ for any $V \in \Irr(S_n)$.

    This is in fact true, we will prove it in the next chapter when we talk about the representation theory of $S_n$.

    We did not use the structure of $S_3$, but we have already computed all characters of $S_3$.

    \begin{exercise}{}{13}
        Let $V = \C^3$ be the representation of $S_3$ arising from the action of $S_3$ on $[3]$. Decompose $V^{\tens n}$ into a direct sum of irreducible representations of $S_3$ (start from computing $\chi_V$ using the definition).
    \end{exercise}
    \begin{proof}
        We calculate $\chi_V(1) = 3, \chi_V(12) = 1, \chi_V(123) = 0$, hence
        \begin{equation*}
            \chi_V = \chi_{\std} + \chi_{\triv}
        \end{equation*}
        where $\chi_{\std}$ is the representation from exercise \ref{exe:10}. Using the relations
        \begin{equation*}
            \begin{split}
                \chi_triv \cdot \chi_i = \chi_i, \quad \chi_\sgn \chi_\std = \chi_\std, \quad \chi_\std^2 = \chi_\triv + \chi_\sgn + \chi_\std
            \end{split}
        \end{equation*}
        one can show via induction that
        \begin{equation*}
            \chi_{V^{\tens n}} = (3^{n-1}+1)/2 \chi_\triv + (3^{n-1}-1)/2 \chi_\sgn + 3^{n-1} \chi_\std
        \end{equation*}
    \end{proof}
\section{Left regular representation}
    We proceed to study the left regular representation in detail with the help of character theory.

    Let $\chi^{\text{reg}}$ be the character of the left regular representation $\rho^{\text{reg}}$. If we denote $G = \{g_1,\dots,g_n\}$, then for $g_k \neq e$ we have $g_k g_l \neq g_l$ for any $1 \leq l \leq n$, this shows
    \begin{equation*}
        \chi^{\mathrm{reg}}(g) = \left\{ \begin{array}{cc}
            0, & g \neq e \\
            \#G & g = e
        \end{array}\right.
    \end{equation*}
    From Maschke, we write $\C(G) \cong S_1^{\oplus n_1} \oplus \dots \oplus S_t^{\oplus n_t}$, where $\{S_1,\dots,S_t\} = \Irr(G)$. By Corollary \ref{cor:2.20}
    \begin{equation*}
        n_i = \spitz{\chi^{\text{reg}},\chi_{S_i}} = \frac{1}{\#G}\sum_{g \in G} \chi^{\text{reg}}(g) \overline{\chi_{S_i}(g)} = \chi_{S_i}(e) = \dim S_i
    \end{equation*}
    We obtained: $\C(G) \cong S_1 ^{\oplus \dim S_1} \oplus \dots \oplus S_t^{\dim S_t}$. If we denote $d_i := \dim S_i$, then $\#G = d_1^2 + \dots + d_t^2$.
    \begin{corollary}{}{2.30}
        All finite dimensional irreducible representations of an abelian group $G$ are of dimension $1$.
    \end{corollary}
    \begin{proof}
        $G$ is abelian $\Leftrightarrow \#\cC(G) = \# G$ which forces $d_1  = \dots = d_t = 1$ in the above equality (notice that $t = \#\cC(G)$).
    \end{proof}
    The formula $\#G = d_1^2  +\dots + d_t^2$ suggests that $\C[G]$ is a direct sum of matrix algebras. This is actually true. We start from the following
    \begin{corollary}{}{2.31}
        Let $x \in \C[G]$ be such that for any $1 \leq i \leq t$, $\tilde{\rho}_{S_i}: \C[G] \to \End(S_i)$ satisfies $\tilde{\rho_{S_i}}(x) = 0$, then $x = 0$.
    \end{corollary}
    \begin{proof}
        Since $\C(G) \cong S_1 ^{\oplus d_1} \oplus \dots \oplus S_t^{d_t}$ with $d_i > 0$, then it follows from $\tilde{\rho}_{S_i}(x) = 0$ for all $1 \leq i \leq t$ that $\tilde \rho_{\C(G)}(x) = 0$. But $x = \tilde{\rho}_{\C(G)}(x)(e) = 0$
    \end{proof}
    (the same trick used in the proof of theorem \ref{thm:2.19} in the claim)
    Let $A,B$ be two $\C$-algebras. The product $A \times B$ is the $\C$-algebra defined by:
    \begin{itemize}
        \item as $\C$-vector space it is $A \oplus B$ (as vector space)
        \item for $a_1,a_2 \in A$, $b_1,b_2 \in B$:
            $(a_1,b_1) + (a_2,b_2) = (a_1 + a_2, b_1 + b_2)$ and $(a_1,b_1) \cdot (a_2,b_2) = (a_1a_2, b_1 b_2)$
    \end{itemize}
    \begin{corollary}{}{2.32}
        The map
        \begin{equation*}
        \begin{split}
            \tilde \rho: \C[G] &\to \prod_{i=1}^t \End(S_i) \\
                            x &\mapsto \round{\tilde{\rho}_{S_1}(x),\dots,\tilde{\rho}_{S_t}(x)}
        \end{split}
        \end{equation*}
        is an isomorphism of $\C$-alg.
    \end{corollary}
    \begin{proof}
        Since each $\tilde \rho_{S_i}$ is a $\C$-algebra homomorphism it follows that $\tilde{\rho}$ is a $\C$-algebra homomorphism. From Corollary \ref{cor:2.31}, $\tilde{\rho}$ is injective. Since $\dim \C[G] = \# G$, the bijectivity follows from $\#G = \round{\dim S_1}^2 + \dots + \round{\dim S_t}^2$.
    \end{proof}
    The inverse map is called the Fourier inversion formula (see for example Proposition \textit{11} in the book of Serre \cite{Serre.1977}).

    Under this isomorphism $Z(\C[G])$ is sent to $\prod\limits_{i=1}^t \C \id_{S_i}$ (the center of $\End(S_i)$ is $\C \id_{S_i}$):

    We define maps
    \begin{equation*}
        \omega_{S_i}: Z(\C[G]) \overset{\cong}{\to} \prod\limits_{i=1}^t \C \id_{S_i} \overset{\mathrm{pr}_i}{\to} \C \id_{S_i} = \C 
    \end{equation*}
    where $\mathrm{pr}_i$ is the projection onto the $i$-th component. In other words for $z \in Z(\C[G])$.
    \begin{equation*}
        \omega_{S_i}(z) = \frac{\Tr\round{\tilde \rho_{S_i}(z)}}{\chi_{S_i}(e)} = \frac{\chi_{S_i}(z)}{\chi_{S_i}(e)}
    \end{equation*}
    where $\chi_{S_i}(z)$ is the extension of $\chi_{S_i}$ is the extension to $\C[G]$ by linearity.
    (i.e. $z$ acts on $S_i$ as this scalar). Then $(\omega_{S_1},\dots ,\omega_{S_t}): Z(\C[G]) \overset{\cong}{\to} \C \id_{S_1} \times\dots \times \C \id_{S_t}$ is the $\C$-algebra isomorphism above.
    
    \separline{Week 8}
    
\section{Integrality and divisibility}
    In this part we will prove the following stronger divisibility result:
    \begin{proposition}{}{2.33}
        Let $S\in\Irr(G)$. Then $\dim S|\# G$.
    \end{proposition}
    For this, we will need several basic results on algebraic integers.
    \begin{definition}{Integral of $\Z$}{2.34}
        \begin{enumerate}
            \item Let $R$ be a commutative ring. An element is called \textbf{integral over $\Z$}, if it is a root of a monic polynomial in $\Z[x]$
            \item Let $R=\C$. Complex numbers that are integral over $\Z$ are called \textbf{algebraic integers}. We let $\overline\Z$ denote the set of all algebraic integers.
        \end{enumerate}
    \end{definition}
    For example, roots of unity are algebraic integers.
    \begin{exercise}{}{14}
        Show that $\Q\cap\overline\Z = \Z$.
    \end{exercise}
    \begin{proof}
        This is a weaker version of the rational root theorem.
        
        Take $x \in \Q \cap \overline{\Z}$. Then there exists a monic polynomial $f(x) \in \Z[x]$ such that $f(x) = 0$. Furthermore let $x = \frac{a}{b}$ for $a \in \Z, b \in \N$ such that $\gcd(a,b) = 1$. Since $f(x)$ is monic, we have:
        \begin{equation*}
            0 = \frac{a^n}{b^n} + \alpha_{n-1} \frac{a^{n-1}}{b^{n-1}}\dots + \alpha_0 \implies -a^n = \alpha_{n-1} a^{n-1}b + \dots + \alpha_0 b^n = b(\alpha_{n-1} a^{n-1} + \dots + \alpha_0 b^{n-1})
        \end{equation*}
        But this means $b | a^n$. But since $\gcd(a,b) = 1$, this is only possible if $b = 1$ (note that we chose $b \in \N$), hence $x \in \Z$.
    \end{proof}
    \begin{lemma}{}{2.35}
        The following statements on $r\in R$ are equivalent:
        \begin{enumerate}
            \item $r$ is integral over $\Z$
            \item $\Z[r]$ is a finitely generated $\Z$-module
            \item there exists a finitely generated $\Z$-submodule $M\subseteq R$ containing $\Z[r]$
        \end{enumerate}
    \end{lemma}
    \begin{proof} We prove the statement using ring closure:
        \begin{enumerate}
            \item [$(1)\Rightarrow (2)$] We assume that $r$ is a root of $x^m+a_{m-1}x^{m-1}+\dots+a_1x+a_0$ where $a_0,\dots,a_{m-1}\in\Z$. Then $\Z[r]$ is a $\Z$-module generated by $1,r,\dots,r^{m-1}$
            \item[$(2)\Rightarrow (3)$] Take $M=\Z[r]$.
            \item[$(3)\Rightarrow (1)$] Since $\Z$ is Noetherian, submodules of a finitely generated module are finitely generated.
            Therefore $\Z[r]$ is finitely generated.
            We consider the submodule $M_k$ of $\Z[r]$ generated by $1,r,\dots r^{k-1}$. Since $\Z[r]$ is finitely generated, there exists $m$ such that $\Z[r]=M_{m-1}$, which means that we can write $r^m$ as a $\Z$-linear combination of $1,r,\dots,r^{m-1}$. Then $r$ is integral over $\Z$.
        \end{enumerate}
    \end{proof}
    
    \begin{corollary}{}{2.36}
        \begin{enumerate}
            \item $\bar\Z$ is a ring
            \item For any $V\in\rep(G), \chi_V:G \to\C$ has image in $\bar\Z$
        \end{enumerate}
    \end{corollary}

    \begin{proof}
        \begin{enumerate}
            \item It suffices to show: for $\alpha,\beta\in\bar\Z$, $\alpha+\beta, \alpha\beta$ are in $\bar\Z$. Assume that $\Z[\beta]$ is generated by $1,\alpha,\dots,\alpha^m$ and $\Z[\alpha]$ is generated by $1,\beta,\dots,\beta^n$. Then $\Z[\alpha,\beta]$ is contained in the $\Z$-module $M$ generated by $\curly{\alpha^i\beta^j|0\leq i\leq m, 0\leq j\leq n}$. Applying \ref{lem:2.35} $(3)\Rightarrow (1)$ to $\Z[\alpha+\beta], \Z[\alpha\beta]\subseteq\Z[\alpha,\beta]\subseteq M$ prooves what we needed to show.
            \item For $g\in G,\ \chi_V(g)$ is a sum of roots of unity, which are algebraic integers, hence by (1), $\chi_V$ has an image in $\bar\Z$.
        \end{enumerate}
    \end{proof}
    
    \begin{corollary}{}{2.37}
        For any $C\in\cC(G)$, the element $\gamma_C^\circ\in Z(\C[G])$ is integral over $\Z$.
    \end{corollary}

    \begin{proof}
        Let $\cC(G)=\curly{C_1,\dots,C_N}$. we consider the $\Z$-submodule $R:=\Z\gamma_{C_1}^\circ\oplus\dots\oplus\Z\gamma_{C_l}^\circ\subseteq Z(\C[G])$. From the multiplication formula $\gamma_{C_i}^\circ\cdot\gamma_{C_j}^\circ=\sum_{k=1}^lm_{ij}^k\gamma^\circ_{c_k}$ where $M_{ij}^k:=\#\curly{(g,h)\in C_i\times C_j| gh\in \C_k}$ (verify it!).

        It follows that $R$ is a finitely generated $\Z$-submodule of $Z(\C[g])$ which contains $\Z[\gamma_{C_i}^\circ]$ for any $i\in [l]$.
    \end{proof}
    
    \begin{lemma}{}{2.38}
        Let $V\in\Irr(G)$ and $C\in\cC(G)$. For any $g\in C$, 
        $$\frac{\chi_V(g)\cdot\#C}{\dim V}=\frac{\chi_V(g)\cdot\#C}{\chi_V(e)}\in\bar\Z$$
        
    \end{lemma}
    
    \begin{proof}
        Since $V$ is irreducible and $\gamma_C^\circ\in Z(\C[G])$, it follows from Schurs lemma (\ref{cor:2.27} to be precise) that $\tilde\rho_V(\gamma^\circ_C)=\sum_{h\in C}\rho_V(h)$ \textit{is} a scalar $\lambda\cdot \id_V$ for $\lambda\in\C$. To determine $\lambda$, we take the trace and obtain:
        $$\lambda\dim V=\sum_{h\in C}\chi_V(h)=\#C\chi_V(g)$$ for any $g\in C$. It remains to show: $\lambda \in\bar \Z$.
        According to \ref{cor:2.37}, $\gamma_C^\circ$ fulfills an equation
        $$(\gamma_c^\circ)^m+a_{m-1}(\gamma_c^\circ)^{m-1}+\dots+ a_1\gamma_C^\circ+a_0=0$$
        with $a_0,\dots,a_{m-1}\in\Z$. Applying the ring homomorphism $\tilde\rho_V$ gives an equation:
        $$\lambda^m+a_{m-1}\lambda^{m-1}+\dots+a_1\lambda+a_0=0$$
        Therefore $\lambda\in\bar\Z$
    \end{proof}

    \begin{proof}[Proof of \ref{prop:2.33}]
        Let $\cC(G)=\curly{C_1,\dots,C_l}$ and choose $g_i\in C_i$. Since $V\in\Irr(G), \spitz{\X_V,\chi_V}=1$ gives us $$\#G=\sum_{i=1}^l\#C_i\cdot \chi_V(g_i)\bar{\chi_V(g_i)}$$
        Dividing both sides by $\dim V$, we obtain $$\frac{\#G}{\dim V}=\sum_{i=1}^r\frac{\# C_i\cdot\chi_V(g_i)}{\dim V}\bar{\chi_V(g_i)}$$
        since $\bar{\chi_V(g_i)}=\chi_V(g_i\inv$, it is in $\bar \Z$ by \ref{cor:2.36}.
        Applying \ref{cor:2.36} agin, \ref{lem:2.38} implies that the right-hand side is in $\bar \Z$.So the left-hand side is in $\bar\Z$. So the left-hand side is in $\Q\cap\bar\Z=\Z$ (\ref{exe:14}).
    \end{proof}
    
\section{Applications}

    We use representation theory to study the properties of finite groups.

    We fix some notations: $\Irr(G)=\curly{S_1,\dots, S_t}$ where $S_1$ is the trivial representation. We write $\rho_i:=\rho_{S_i}$ and $\chi_i:= \chi_{S_i}$ for short.
    \subsection{Normal subgroups}

        First notice that the kernel $\ker(\rho)$ is a normal subgroup of $G$ since $\rho_i$ is a group homomorphism.
        
        \begin{proposition}{}{2.39}
            Let $N\trianglelefteq G$. There exist $1\leq i_1,\dots, i_r\leq t$ such that $$N=\bigcap_{s=1}^r\ker\rho_{i_s}$$ 
        \end{proposition}
    
        \begin{proof}
            Let $\rho$ be the left regular representation of $G/N$ on $\C(G/N)$: $\rho: G/N\to\GL(\C(G/N))$.
    
            By composing $\pi:G\to G/N$ we obtain a representation of $G$: $\rho\circ \pi: G\to\GL(\C(G/N))$.
    
            By \ref{thm:maschke}, we decompose it into $$\C(G/N)\cong S_1^{\oplus n_1} \oplus \dots \oplus S_t^{\oplus n_t}$$
            Let $i_1,\dots,i_r$ be the indices with $n_{i_1},\dots,n_{i_r}\neq 0$. It remains to show two things:
            \begin{enumerate}
                \item $\ker(\rho\circ \pi)=N$
                \item $\ker(\rho\circ\pi)=\bigcap_{s=1}^r\ker\rho_{i_s}$
            \end{enumerate} 
            
            The first point holds since $\ker\rho=\curly{e}$. For the second point, it suffices to notice that the above decomposition gives:
            $$\rho\circ\pi=(\underbrace{\rho_{i_1},\dots, \rho_{i_1}}_{n_{i_1}\text{-times}}, \dots, \underbrace{\rho_{i_r},\dots, \rho_{i_r}}_{n_{i_r}\text{-times}})$$
        \end{proof}
    
        Now the question is: how to compute $\ker\rho_{i_s}$? Character!
        
        \begin{lemma}{}{2.40}
            We have: $$\ker\rho_i = \curly{ g\in G \ | \ \chi_i(g)=\chi_i(e)}$$     
        \end{lemma}
        \begin{proof}
            We show that: $\rho_i(g)=\id:{s_i}\Leftrightarrow \chi_i(g)=\chi_i(e)$.
            Let $\lambda_1,\dots, \lambda_r$ denote the eigenvalues of $\rho_i(g)$ with multiplicities. Then $\chi_i(g)=\chi_i(e) \Leftrightarrow = r$. Since the $l_i$ are roots of unity
            $$r=\vert{\lambda_1+\dots+\lambda_r}\leq\vert{\lambda_1}+\dots+\vert{\lambda_r}=r$$
            which implies that $\lambda_1,\dots,\lambda_r$ are colinear. Together with $\lambda_1+\dots +\lambda_r=r$, it follows $\lambda_1=\dots =\lambda_r=1$. It is equivalent to $\rho_i(g)=\id_{s_i}$.
        \end{proof}
    
        Recall that a group $G$ is called simple if it has exactly two subgroups $\curly{e}$ and itself.
        
        \begin{corollary}{}{2.41}
            The group $G$ is simple if and only if for any $2\leq i \leq t$ and $g\neq e$, $\chi_i(g)\neq \chi_i(e)$.     
        \end{corollary}
        
    \subsection{Centre}

        We propose two methods to compute the centre of the group $G$ using the character table.

        The first one relies on the identity from \ref{cor:2.29}: For $C\in\cC(G)$ and $g\in C$:
        
        $$\sum_{i=1}^t \chi_i(g)\bar{\chi_i(g)}=\frac{\#G}{\#C}$$

        Note that $g\in Z(G)\Leftrightarrow \curly g \in \cC(G)$. Therefore:
        
        \begin{corollary}{}{2.42}
            $g\in Z(G)$ if and only if $$\#G=\sum_{i=1}^t\vert{\chi_i(g)}^2$$
        \end{corollary}

        The second method uses the notion of the centre of a character. 
        \begin{definition*}{centre of a class function}
            Let $\chi\in\CF(G)$. We define its centre $$ Z(\chi):=\curly{g\in G \ | \ \vert{\chi(g)}=\chi(e)}$$
        \end{definition*}
    
        \begin{proposition}{}{2.43}
            We have:
            $$Z(G)=\bigcap_{i=1}^t Z(\chi_i)$$
        \end{proposition}
        \begin{proof}
            Is left as an exercise (See also exercise sheet 7, exercise 2b)).
        \end{proof}
    
\section{Bursides' theorem}
    As an application of character theory, we prove Burnsides' theorem. The following proof is mostly due to Burnside. A proof without representation theory is only known since the 1970s.
    
    \begin{theorem}{Burnside 1904}{2.44}
        Let $p,q$ be two prime numbers and $a,b \in \N$. Any group of order $p^a \cdot q^b$ is solvable.
    \end{theorem}

    To prove the theorem, we need two preliminary results.
    
    \begin{lemma}{}{2.45}
        Let $\e_1,\dots,\e_n$ be roots of unity. If $\frac{1}{m}\round{\e_1 + \dots + \e_m }$ is an algebraic integer,
        then either $\e_1 + \dots + \e_m = 0$ or $\e_1 = \dots = \e_m$.
    \end{lemma}

    \begin{proof}
        We need a bit of Galois theory.

        Let $\alpha:= \frac{1}{m}\round{\e_1 + \dots + \e_m }$ and assume that not all $\e_i$ are equal.
        It 
        follows that $\vert{\alpha} < 1$. If $\alpha \in \Q$, then $\alpha \in \Q \cap \overline{\Z} = \Z$, which implies that $\alpha = 0$.
        But this is not always the case, so we have to use some Galois theory.

        Let $L$ be a field containing $\e_1,\dots,\e_m$ such that $L/\Q$ is a Galois extension. Any $\sigma \in \Gal{L/\Q}$ will send $\e_i$ to a root of unity and
        $\sigma(\e_1),\dots,\sigma(\e_m)$ are not all equal. It follows then $\sigma(\alpha) \in \overline{\Z}$ and $\vert{\sigma(\alpha)} < 1$.

        We consider the element
        \begin{equation*}
            \beta := \prod_{\sigma \in \Gal{L/\Q}} \sigma(\alpha) \in L
        \end{equation*}
        Then $\beta \in L^{\Gal{L/\Q}} = \Q $ by Artins Lemma~\ref{lem:artin}. As a prodcut of algebraic integers, $\beta \in \overline{\Z}$ and moreover $\vert{\beta} < 1$.
        Therefore $\beta \in \Q \cap \overline{\Z} = \Z$, hence $\beta = 0$.

        This means $\sigma(\alpha) = 0$ for some $\sigma \in \Gal{L/\Q}$. It follows that $\alpha = 0$.
    \end{proof}

    \begin{proposition}{}{2.46}
        Let $C \in \cC(G)$ with $\#C = p^t$, where $p$ is prime and $t>0$. Then $G$ is not simple.
    \end{proposition}

    \begin{proof}
        First, $G$ can not be abelian since otherwise, each conjugacy class will only have one element.
        Let $\Irr(G) = \{S_1,\dots,S_n\}$ where $S_1$ is the trivial representation. Let $\chi_i := \chi_{S_i}$ and take $g \in C$.
        From Corollary~\ref{cor:2.29}, since $g \neq e$:
        \begin{equation*}
            \sum_{i=2}^t \chi_i(e) \overline{\chi_i(g)} = -1
        \end{equation*}
        We claim that $\exists \ 2 \leq j \leq t$ such that $\overline{\chi_j(g)} \neq 0$ and $p \nmid \chi_j(e)$. Otherwise, we will obtain
        \begin{equation*}
            \sum_{i=2}^t \frac{\chi_i(e)}{p} \overline{\chi_i(g)} = -\frac{1}{p}
        \end{equation*}
        but the left-hand side is in $\overline{\Z} \cap \Q = \Z$, so must be the right-hand side, which is a contradiction.

        Therefore $p^t$ and $\chi_j(e)$ are coprime. Bzout tells us that there exist $a,b \in \Z$:
        \begin{equation*}
            a p^t + b \chi_j(e) = 1
        \end{equation*}
        Multiplying both sides by $\frac{\chi_j(g)}{\chi_j(e)}$ gives:
        \begin{equation*}
            \frac{\chi_j(g)}{\chi_j(e)} = a \frac{\#C \cdot \chi_j(g)}{\chi_j(e)} + b \chi_j(g)
        \end{equation*}
        By Lemma~\ref{lem:2.38}, $\frac{\#C \cdot \chi_j(g)}{\chi_j(e)} \in \overline{\Z}$, by Corollary~\ref{cor:2.36} $\chi_j(g) \in \overline{\Z}$,
        hence the right hand side is in $\overline{\Z}$ and then $\frac{\chi_j(g)}{\chi_j(e)} \in \overline{\Z}$.

        Notice that $\chi_j(g)$ is a sum of $\chi_j(e)$-many roots of unity.
        Since $\chi_j(g) \neq 0$, the above lemma implies that there exists a root of unity $\omega$ such that $\chi_j(g) = \omega \chi_j(e)$.
        
        Moreover, $\omega$ is the only eigenvalue of $\rho_j(g)$, hence $\rho_j(g) = \omega \id_{S_j}$.

        Now we consider the subgroup
        \begin{equation*}
            H:= \curly{h \in G \ | \ \rho_j(h) = \omega_h \id_{S_j}}
        \end{equation*}
        where $\omega_h$ is a certain root of unity depending on $h$. Since $\rho_j$ is a group homomorphism,
        $H$ is a normal subgroup containing $g$. If $H \neq G$, then we are done.

        So let us assume that $H=G$. Then for any $g \in G$, $\rho_{S_i}(g)$ is a scalar, since $S_j$ is irreducible, it must be of dimension $1$.
        This implies that $\chi_j: G \to \C \setminus \{0\}$ is a group homomorphism and $\ker \chi_j$ is a normal subgroup of $G$. Since $S_j$ is not the trivial representation, $\ker \chi_j \neq G$.
        Since $G$ is not abelian, $\ker \chi_j \neq \{e\}$ (if $\ker \chi_j = \{e\}$, then $G \hookrightarrow \C \setminus\{0\}$).

        As a summary, if $H=G$, we have found another non-trivial normal subgroup $\ker \chi_j$ of $G$, hence $G$ is not a simple group.
    \end{proof}

    \begin{proof}[Proof of Theorem~\ref{thm:2.44}]

        We proceed by induction over $\#G$.

        For the base case just recall that $p$-groups are solvable.

        If $G$ is a non-trivial normal subgroup $N$, then both $N$ and $G/N$ have an order of form $p^c q^d$. The induction hypothesis implies that both of them are solvable.
        By Exercise~\ref{exe:8}(4), $G$ is solvable.

        Now we can assume that $G$ is simple of order $p^a\cdot q^b$. Without loss of generality, we assume that $a,b \neq 0$ (otherwise $G$ is not simple, a $p$-group or a $q$-group).
        Let $P$ be a $p$-Sylow subgroup of $G$. Then the center of $P$ is non-trivial ($P$ is a $p$-group) and we choose $g \in Z(P) \setminus\{e\}$.

        Let $C_G(g) = \{h \in G \ | \ hg = gh\}$ be the \textbf{centralizer} of $g$. Since $g \in Z(P)$, $P \subseteq C_G(g)$.
        Hence $[G:C_G(g)]$ divides $[G:P] = q^b$. Therefore $[G:C_G(g)]$ is a power of $q$, say $q^r$. Notice that $[G:C_G(g)]$ is the cardinality of the conjugacy class containing $g$ (by the orbit-stabilizer theorem).
        If $r \geq 1$, from the above propositon~\ref{prop:2.46}, $G$ is not simple, a contradiciton ($r=0 \Rightarrow G$ not simple since $e \neq g \in Z(G)$).
    \end{proof}

    \begin{exercise}{}{15}
        \begin{enumerate}
            \item Show that for three distinct prime numbers $p,q,r$ a group of order $pqr$ is solvable.
            \item Show that any group of order $<60$ is solvable.
            \item Show that any group of order $>60$ and $<120$ is solvable. 
        \end{enumerate}
    \end{exercise}
    
%    \begin{proof}
%        
%    \end{proof}
    
\chapter{Semi-simple algebras}
    In the last chapter we have seen that studying the representation theory of groups is equivalent to studying modules over the group algebra $\C[G]$, see Exercise \ref{exe:12}.

    From this chapter on we will study the representation theory of finite dimensional algebras. In this chapter we study the simplest such algebras from a representation-theoretical point of view, they are essentially matrices.
\setcounter{section}{-1}
\section{Non-commutative tensor product}\label{non-commTP}
    For the sake of completeness, I (Felix) added this chapter. This is not actually part of the notes but I think this is actually necessary for understanding the following sections in particular, why we take $R$-structure on only one component of the tensor product.

    Let us set the stage. Take a ring $R$ (unital and associative of course). Now if $R$ is commutative, then we define the tensor product of two $R$-modules $M,N$ exactly as before:
    
    \begin{definition*}{Universal property of the commutative tensor product}{}
        Let $R$ be a commutative ring and $M,N$ be $R$-modules. The tensor product is a map $\alpha: M \times N \to T$ such that for all $R$-modules $Z$ and all $f \in \Bil_R(M;N,Z)$ there exists a unique $\overline{f}: M \tens N \to Z$ satisfying the following diagram
        \begin{equation*}
        \xymatrix {
            M \times N \ar[rr]^(.55){\forall f} \ar[dr]_(.45){\alpha}& & Z \\
            & T \ar@{}[u]|{\circlearrowleft}\ar[ur]_(.6){\exists ! \overline{f}}
        }
        \end{equation*}
    \end{definition*}
    The object $M \tens N :=(M \times N)/S$, where
        \begin{equation*}
            \begin{split}
                S = \spitz{ \left. \begin{array}{c}
                    (m_1+m_2,n_1) - (m_1,n_1) - (m_2,n_1), \\ (m_1,n_1 + m_2) - (m_1,n_1) - (m_1,n_2),  \\
                    (\lambda m_1, \mu n_1) - (\mu\lambda)( m_1, n_1) 
                \end{array}  \ \right| \ m_1,m_2 \in M, \ n_1,n_2 \in N \text{ and } \lambda,\mu \in R }_R        
            \end{split}
        \end{equation*}
    satisfies this universal property. We denote $\overline{(m,n)}:= m \tens n$.
    There is $R$-module structure on $M \tens N$ via:
    \begin{equation*}
        r.(m \tens n) = (rm) \tens n = m \tens (rn)
    \end{equation*}
    Note that this definition of the $R$-module structure already uses the universal property of the tensor product, as we only defined the action of $R$ on elementary tensors $m \tens n$.

    Sadly, if $R$ is not commutative, life is not that easy. Say we take two $R$-leftmodules $M,N$, then if we were to define $r.(m \tens n) := (rm) \tens n = m \tens (rn)$, then we would run into the following problem:
    \begin{equation*}
        (rs).(m \tens n) = (rsm) \tens n = (r(sm)) \tens n = (sm) \tens (rn) = m \tens (s(rn)) = m \tens (srn) = (sr).(m \tens n) 
    \end{equation*}
    To fix this problem, we take a $R$-rightmodule $M$ and a $R$-leftmodule $N$. We set $r.(m \tens n) = (mr) \tens n = m \tens (rn)$, i.e. $R$ and the $\tens$-sign commute. But there is still no $R$-module structure with this definition, because
    \begin{equation*}
        m \tens ((rs)n) = (rs).(m \tens n) \overset{\star}{=} r.(s.(m \tens n)) = r.((ms) \tens n) = (ms) \tens (rn) = m \tens ((sr)n)
    \end{equation*}
    where $\star$ denotes the point, where we assumed the $R$-module structure to be associative. Thus we only get an abelian group $M \tens N$. To use the language of \ref{cat}, the tensor functor for non-commutative rings is a mapping $R\Mod \to \Ab$.

    \begin{definition*}{Universal property of the noncommutative tensor product}{}
        Let $R$ be a ring and $M$ a right $R$-module and $N$ a left $R$-module. The tensor product is a map $\alpha: M \times N \to T$ such that for all abelian groups $Z$ and all $f \in \Bil_{\Z,R}(M;N,Z)$ there exists a unique $\overline{f}: M \tens N \to Z$ satisfying the following diagram
        \begin{equation*}
        \xymatrix {
            M \times N \ar[rr]^(.55){\forall f} \ar[dr]_(.45){\alpha}& & Z \\
            & T \ar@{}[u]|{\circlearrowleft}\ar[ur]_(.6){\exists ! \overline{f}}
        }
        \end{equation*}
    \end{definition*}
     The object $M \tens N :=(M \times N)/S$, where
        \begin{equation*}
            \begin{split}
                S = \spitz{ \left. \begin{array}{c}
                    (m_1+m_2,n_1) - (m_1,n_1) - (m_2,n_1), \\ (m_1,n_1 + m_2) - (m_1,n_1) - (m_1,n_2),  \\
                    (m_1 \lambda , \mu n_1) -   (\mu \lambda)(m_1, n_1) 
                \end{array}  \ \right| \ m_1,m_2 \in M, \ n_1,n_2 \in N \text{ and } \lambda,\mu \in R }_R        
            \end{split}
        \end{equation*}
    satisfies this universal property. We denote $\overline{(m,n)}:= m \tens n$.
    
    But we want to talk about modules, hence we have to make even more concessions. The fix is to let $R$ only operate on one component, i.e. $r.(m \tens n) = (rm) \tens n$ if $M$ is a left $R$-module. Now all the contradictions that we got above do not work any more, since we only have $R$-structure on one component of the tensor product.

    Now we return to the lecture notes.
\section{Modules over algebras}
    \subsection{Modules}
        In Computeralgebra \cite{Cobra} we have already studied modules over (commutative) rings. In the following, we mainly analyse the non-commutative setting.

        Let $K$ be a field. In this lecture, all algebras and rings are associative and unital. Unital means, there is a multiplicative identity.

        \begin{definition}{}{3.1}
            \begin{enumerate}
                \item
                    An $\mathbf{K}$\textbf{-algebra} $(A,+,\cdot)$ is a ring and a $K$-vector space satisfying:
                    \begin{equation*}
                        \forall a,b \in A, \lambda \in K: \lambda(ab) = (\lambda a)b = a (\lambda b)
                    \end{equation*}
                \item
                    The \textbf{dimension} of $A$ is its dimension as $K$-vector space.
                \item 
                    A \textbf{subalgebra} of $A$ is a $K$-vector subspace and a subring (not an ideal!) of $A$
                \item
                    Let $A,B$ be two $K$-algebras. A $\mathbf{K}$\textbf{-algebra homomorphism} between $A$ and $B$ is a $K$-linear map and a ring homomorphism. Similarly, we can define iso-, endo-, automorphisms and so on.
            \end{enumerate}
        \end{definition}
        
        \textbf{Examples:}
            \begin{enumerate}
                \item
                    Let $K$ be a field, then $K$ is a $K$-algebra.
                \item
                    Let $G$ be a group, then $\C[G]$ is a $\C$-algebra.
                \item
                    Let $V$ be a $K$-vector space, then $\End_K(V)$ is a $K$-algebra.
            \end{enumerate}

        Here is a small intermezzo, which may or may not be of interest to you:
        
        \begin{remark*}{abstract nonsense - F}
            One can formulate the above relations by means of a commutative diagram. For example, let $\mu: A \times A \to A, (a,b) \mapsto a \cdot b$ be the multiplication of $A$, then we can formulate associativity of $A$ via
            \begin{equation*}
                \xymatrix {
                & A \times A \times A \ar[dl]_{(\mu,1)} \ar[dr]^{(1,\mu)}& \\
                A \times A \ar[dr]_{\mu} & \circlearrowleft & A \times A\ar[dl]^{\mu} \\
                & A & 
                }
            \end{equation*}
            \textbf{Why should we care?}
            
            The above diagram describes associativity not only for algebras but for groups, monoids, rings and so on. Similarly, we can realize all the other requirements for a group say.
            
            Now the maps given above are maps between sets, i.e. the diagram above is realized in the category $\mathrm{Set}$. Now we collect all the diagrams for the definition of a group, but we switch the category, say to $\Top$, the category of topological space, which in the case above means that $\mu$ needs to be a continuous map, collecting all the diagrams in the "new" setting, we gain the definition of a \textbf{topological group} (the same works for algebraic groups etc.).
            
            In contrast - if we were to just write out the definition of a topological group the "choice" of definition would be way less natural.

            To summarize: Mathematics likes to reward generalization, and we like rewards, so we should strive for abstraction. In this case, we got rewarded with a deeper understanding of the definition of a (topological)group and structure in general.
        \end{remark*}

        Returning to the lecture notes, we define modules over $K$-algebras just as in the commutative case, we just have to make a difference between the left and right modules.

        \begin{definition}{Modules}{3.2}
            Let $A$ be a $K$-algebra. A \textbf{left} $A$\textbf{-module} $M$ is a $K$-vector space $M$ together with a $K$-algebra homomorphism $\lambda_M: A \to \End_K(M)$.

            For $a \in A, m \in M$ we will usually denote $a \cdot m := \lambda_M(a)(m)$.

            Let $A^{\op}$ be the $K$-algebra whose $K$-vector space structure is the same as $A$, and the multiplication is defined by $a \cdot_{\op} b:= b \cdot a$ for all $a,b \in A^{\op}$

            We call the $K$-vector space $M$ a right $A$-module, if there exists a $K$-algebra homomorphism $\rho_M: A^\op \to \End_K(M)$.
        \end{definition}
        
        \begin{remark}{}{3.3}
            In general, $A$ is not isomorphic to $A^\op$ as a non-commutative ring could be quite different (for example, one can be noetherian but the other not). For an example of such an algebra see Section 2.8 in \cite{Jacobson.2009}.
        \end{remark}
        Sometimes we will simply write $\tensor[_A]{M}{}$ or $M_A$ to emphasize that it is a left or right $A$-module. When we just say $A$-module we mean left module by default.

        \begin{example}{}{3.4}
            \begin{enumerate}
                \item
                    Let $\tensor[_A]{M}{}, \tensor[_A]{N}{}$ be two $A$-modules. Their direct sum $M \oplus N$ is the left $A$-module defined by $M \oplus N$ as a vector space and the action
                    \begin{equation*}
                        a\cdot(m,n):=(a \cdot m, a \cdot n)
                    \end{equation*}
                    for all $a \in A, m \in M, n\in N$.
                \item
                    Let $\tensor[_A]{M}{}$ be an $A$-module. The dual vector space $M^*$ is a right $A$-module with the following multiplication:
                    \begin{equation*}
                        \spitz{f\cdot a, m} = \spitz{f,a \cdot m}
                    \end{equation*}
                    for all $f \in M^*, m \in M$ and $a \in A$.
                \item
                    $A$ is both a left and right $A$-module:
                    \begin{equation*}
                    \begin{split}
                        A &\to \End_K(A) \\
                        a &\mapsto (b \mapsto ab) \\
                        A^\op &\to \End_K(A) \\
                            a &\mapsto (b \mapsto ba)
                    \end{split}
                    \end{equation*}
            \end{enumerate}
        \end{example}
        But one should keep in mind that there is no \underline{natural} $A$-module structure on the tensor product of two $A$-modules. The group algebra (and the universal enveloping algebra you will see in the Liealgebren lecture) are very special because it admits a bialgebra structure.

\separline{Week 9}

        \begin{definition}{Submodules}{3.5}
            \begin{enumerate}
                \item
                    Let $\tensor[_A]{M}{}$ be an $A$-module. A $K$-vector subspace $N$ of $M$ is called an $\mathbf{A}$\textbf{-submodule} if for all $a \in A$ and $n \in N: \lambda_M(a)(n) \in N$.
                \item
                    Submodules of the left/right $A$-module $A$ are called \textbf{left/right ideals} of $A$.
                \item
                    A subspace $I \subseteq A$ is called an ideal (or a two-sided ideal) if it is both a left and a right ideal in $A$. We will write $I \trianglelefteq A$ for a two-sided ideal.
            \end{enumerate}
        \end{definition}

        \textbf{Examples:}
        \begin{enumerate}
            \item Take $A$ to be a field, then $A$ is an $A$-algebra the $A$-modules are precisely the $A$ vector spaces.
            \item Let $\varphi: A \to B$ be a homomorphism of $K$-algebras, then $\ker \varphi$ is an ideal.
        \end{enumerate}

        If we expand the definition (3), a subspace $I \leq A$ is a left (respective right) ideal if $\forall a \in A: aI \subseteq I$ (respective $Ia \subseteq I$), where $aI = \curly{ax \ | \ x \in I}$.

        \begin{definition}{}{3.6}
            A homomorphism between two left $A$-modules $\tensor[_A]{M}{},\tensor[_A]{N}{}$ is a $K$-linear map $\varphi: M \to N$ such that
            \begin{equation*}
                \forall a \in A, m \in M: \varphi(a.m) = a \cdot \varphi(m)
            \end{equation*}
            We denote the vector space of homomorphisms between $M$ and $N$ as $\Hom_A(M,N)$. We set $\End_A(M):= \Hom_A(M,M)$.
        \end{definition}

        Because we are in an abelian category, the isomorphism theorem holds. Let $\varphi \in \Hom_A(M,N)$, then $\faktor{M}{\ker \varphi} \cong \Img \varphi$ as $A$-modules.

        \begin{example}{}{3.7}
            We claim that $\End_A\round{\tensor[_A]{A}{}} \cong A^\op$. Take $\varphi \in \End_A \round{\tensor[_A]{A}{}}$, then
            \begin{equation*}
                \varphi(a) = \varphi(a \cdot 1_A) = a \cdot \varphi(1_A)
            \end{equation*}
            This means applying $\varphi$ to an element of $A$ just means right multiplication by $\varphi(1_A)$. Thus we define a $K$-linear map:
            \begin{equation*}
            \begin{split}
                \Phi: \End_A \round{\tensor[_A]{A}{}} &\to A^\op \\
                \varphi &\mapsto \varphi(1_A)
            \end{split}
            \end{equation*}
            take $\varphi,\psi \in \End_A \round{\tensor[_A]{A}{}}$ and $\lambda \in K$, then we have
            \begin{equation*}
            \begin{split}
                \Phi(\lambda \varphi + \psi) &= (\lambda \varphi + \psi)(1_A) = \lambda \varphi(1_A) + \psi(1_A) = \lambda \Phi(\varphi) + \Phi(\psi) \\
                \Phi(\varphi \cdot \psi) &= (\varphi \cdot \psi)(1_A) = \varphi(1_A) \cdot \psi(1_A) = \Phi(\varphi) \cdot \Phi(\psi)
            \end{split}
            \end{equation*}
            The inverse of $\Phi$ is given by
            \begin{equation*}
                \begin{split}
                    \Psi: A^\op &\to \End_A \round{\tensor[_A]{A}{}} \\
                            \alpha &\mapsto \mu_{\alpha}: a \mapsto a \cdot \alpha
                \end{split}
            \end{equation*}
            because for all $a \in A$
            \begin{equation*}
            \begin{split}
                \Psi \circ \Phi (\varphi)(a) &= \Psi(1_A)(a) = a \cdot 1_A = a \\
                \Phi \circ \Psi (a) &= \Phi(\mu_a) = \mu_a(1) = a
            \end{split}
            \end{equation*}
        \end{example}
\subsection{Idempotents and decomposition}
    Decomposing an algebra or a module simplifies the object we study: for example, using Maschke's theorem \ref{thm:maschke} we decomposed a module into a direct sum of irreducible ones. We have used idempotents (projector) in that proof. Here we study more decompositions from idempotents. This is quite important in the theory of finite dimensional algebras.

    Let $A$ be a $K$-algebra.

    \begin{definition}{}{3.8}
        An element $e \in A \setminus \{0\}$ is called an \textbf{idempotent}, if $e^2 = e$.
    \end{definition}

    Let $M$ be an $A$-module and $\varphi \in \End_A(M)$ be an idempotent. The same proof as in Lemma \ref{lem:2.22} gives a decomposition of $M$ into a direct sum of $A$-modules:
    \begin{equation*}
        M \cong \Ker \varphi \oplus \Img \varphi
    \end{equation*}

    To deal with a decomposition into more than two components, we need the following definition:

    \begin{definition}{}{3.9}
        Two idempotents $e_1,e_2 \in A$ are called \textbf{orthogonal}, if $e_1e_2 = e_2 e_1 = 0$. More generally a family $(e_i)_{i \in I}$ of idempotents is called orthogonal, if for all $i \neq j$ from the index set $I$, $e_i$ and $e_j$ are orthogonal. 
    \end{definition}

    If $e_1$ and $e_2$ are orthogonal, then $e_1 + e_2$ is an idempotent.

    \begin{proposition}{}{3.10}
        Let $M$ be an $A$-module. There exists a bijection between
        \begin{enumerate}
            \item a decomposition of $M$ into a finite direct sum of its submodules
            \item orthogonal idempotents in $\End_A(M)$ with sum $\id_M$
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        \underline{$1) \Rightarrow 2)$:}
        Let $M = \bigoplus_{i=1}^n M_i$ be a decomposition in \textit{1)}. Then for any $1 \leq i \leq n$ there exist two $A$-module homomorphisms:
        \begin{equation*}
        \begin{split}
            \pi_i: M &\to M_i \\
            (m_1,\dots,m_n) &\mapsto m_i \\
            \iota_i: M_i &\to M \\
            m_i &\mapsto (0,\dots,0,\underbrace{m_i}_{i\text{-th component}},0,\dots,0)=:m_i e_i
        \end{split}
        \end{equation*}
        We set $\varphi_i := \iota_i \circ \pi_i \in \End_A(M)$. Now we have:
        \begin{enumerate}
            \item $\varphi_i^2 = \varphi_i$, because $\pi_i \circ \iota_i = \id M_i$
            \item since $M_i \cap M_j = \{0\}$: $\varphi_i \circ \varphi_j = 0$ for all $i \neq j$ 
            \item $\sum\limits_{i=1}^n \varphi_i = \id_M$, due to the fact that $\varphi_i(m_1,\dots,m_n) = m_i e_i$.
        \end{enumerate}

        \underline{$2) \Rightarrow 1)$:} Let $\varphi_1,\dots,\varphi_n \in \End_A(M)$ be orthogonal idempotents with $\sum_{i=1}^n \varphi_i = \id_M$. We denote $M_i := \Img \varphi_i$, which is an $A$-submodule of $M$. It follows from the last condition that $M \subseteq \sum_{i=1}^n M_i$. Hence $M = \sum_{i=1}^n M_i$. It remains to show that the sum is direct.

        From the orthogonality, we have for a fixed $j$ and all $i \neq j: M_i \subseteq \ker \varphi_j$. Hence
        \begin{equation*}
            \sum_{i=1, i \neq j}^n M_i \subseteq \ker \varphi_j
        \end{equation*}
        Since $\varphi_i$ is idempotent, $\ker \varphi_i \cap \Img \varphi_i = \{0\}$ and hence
        \begin{equation*}
            M_i \cap \round{\sum_{i=1, i \neq j}^n M_i} = \{0\}
        \end{equation*}
    \end{proof}
    In particular, if $A = K$, we obtain a decomposition of vector spaces. Having definition \ref{def:3.5} and example \ref{exa:3.7} in mind, taking $M = A$ in the above proposition gives a bijection between:
    \begin{enumerate}
        \item a decomposition of $A$ into a direct sum of its left ideals
        \item orthogonal idempotents in $A^\op$ with sum $1_A$.
    \end{enumerate}

    To decompose $A$ into two-sided ideals, we need the notion of central idempotents.

    \begin{definition}{}{3.11}
        An idempotent $e \in A$ is called central, if for any $a \in A$, $ae = ea$.
    \end{definition}

    \begin{corollary}{}{3.12}
        There exists a bijection between:
        \begin{enumerate}
            \item a decomposition of $A = \bigtimes\limits_{i=1}^n A_i$ into $K$-algebras
            \item a decomposition of $1_A$ into a sum of orthogonal central idempotents.
        \end{enumerate}
    \end{corollary}

    \begin{proof}
        The map from $(1)$ to $(2)$ is easy, we set $e_i := (0,\dots,0,1_{A_i},0 ,\dots,0)$, then they are orthogonal central idempotents with $\sum_{i=1}^n e_i = 1_A$.

        On the other hand, if $e_1,\dots,e_n$ are orthogonal central idempotents with sum $1_A$, we define a $K$-linear map
        \begin{equation*}
        \begin{split}
            \varphi: A &\to \bigtimes_{i=1}^n A e_i \\
                    a &\mapsto (ae_1, \dots, ae_n)
        \end{split}
        \end{equation*}
        Since the $e_i$ are central idempotents, $(Ae_i) \cdot (Ae_i) = Ae_i$ and $Ae_i$ is a $K$-algebra with unit $e_i$ and $\varphi$ is a homomorphism of $K$-algebras. It suffices to verify that $\varphi$ is bijective.

        Say $a \in \ker \varphi$, then $(ae_1,\dots,ae_n) = (0,\dots,0)$, hence
        \begin{equation*}
            a = a \cdot 1_A = a \cdot \round{\sum_{i=1}^n e_i} = \sum_{i=1}^n ae_i = 0
        \end{equation*}
        hence $\ker \varphi = \{0\}$.

        The pre-image of $(a_1e_1,\dots,a_ne_n)$ is $\sum_{i=1}^n a_ie_i$, because
        \begin{equation*}
            \varphi \round{\sum_{i=1}^n a_ie_i} = \round{\sum_{i=1}^n a_ie_i e_1,\dots,\sum_{i=1}^n a_ie_i e_j,\dots, \sum_{i=1}^n a_ie_i e_n} = \round{a_1,\dots,a_j,\dots,a_n}
        \end{equation*}
    \end{proof}

    Note that the $K$-algebras $A_1,\dots, A_n$ in \textit{(1)} are \underline{NOT} subalgebras of $A$ since they do not share the same unit.

\section{Simple modules and algebras}
    We first remove "semi" from the title of this chapter and study finite-dimensional simple algebras over $\C$. In the rest of this chapter, we fix $\C$ as the base field, although any algebraically closed field of characteristic $0$ would make do.
    
\subsection{Simple modules}
    The simplest modules are the simple modules.

    \begin{definition}{}{3.13}
        \begin{enumerate}
            \item An $A$-module $M$ is called \textbf{simple}, if there are \underline{exactly} two submodules $\{0\}$ and $M$.
            \item A $\C$-algebra $A$ is called \textbf{simple}, if there is no non-trivial left $A$-module which is also a right $A$-module. That is to say, it has \underline{exactly} two two-sided ideals, $\{0\}$ and $A$.
        \end{enumerate}
    \end{definition}

    \textbf{Examples:}
    \begin{enumerate}
        \item Let $A = \C$, then the simple $A$ modules are exactly the $1$-dimensional $A$-vector spaces.
        \item Let $G$ be a finite group. The irreducible representations of $G$ are exactly the simple $\C[G]$-modules.
        \item Simple modules are cyclic but the converse is not true (why?).
        \item The zero module $\{0\}$ is \textbf{not} simple.
    \end{enumerate}

    \begin{example}{}{3.14}%pi :)
        The matrix algebra $A = M_n(\C)$ is a simple algebra. The $M_n(\C)$-module $\C^n$ is simple. We prove the first statement and leave the second statement as an exercise.

        Let $E_{i,j} \in M_n(\C)$ be the elementary matrix, whose only non-zero entry is a $1$ at entry $i,j$. To prove that $M_n(\C)$ is simple, we choose a non-zero matrix $M \in M_n(\C)$ with $M = (m_{i,j})_{i,j}$ and show that $AMA = M_n(\C)$ (here $AMA = \curly{XMY \ | \ X, Y \in M_n(\C)}$), this set is contained in the smallest two-sided ideal containing $M$. %is the smallest two-sided ideal containing $M$.

        Since $M \neq 0$, we assume that a certain $m_{p,q} \neq 0$, then
        \begin{equation*}
            m_{p,q}^{-1} E_{p,p} M E_{q,q} = E_{p,q} \in AMA
        \end{equation*}
        Now for any $1 \leq i,j \leq n$, $E_{i,j} = E_{ip} E_{p,q} E_{q,j} \in AMA$, hence $A \subseteq AMA$.
    \end{example}

    For simple modules, we again have a version of Schurs Lemma \ref{lem:schur}. The proof is the same as in the last chapter.

    \begin{lemma}{}{schurMOD}
        Let $A$ be a $\C$-algebra, $M,N$ two finite-dimensional simple $A$-modules. Then
        \begin{equation*}
            \dim_{\C} \Hom_A(M,N) = \left\{ \begin{array}{rc}
                1, &  \text{if } M \cong N \text{ as $A$-modules}\\
                0, & \text{otherwise}
            \end{array}\right.
        \end{equation*}
        
    \end{lemma}

\subsection{Wedderburn's theorem}
    The goal of this section is to prove the following theorem due to Wedderburn. The proof is a generalization of example \ref{exa:3.14}, because $\End(V) \cong M_n(\C)$, where $\dim V = n$.

    \begin{theorem}{Wedderburn}{wedderburn}
        Any finite-dimensional simple $\C$-algebra $A$ is isomorphic to $\End(V)$ for a $\C$-vector space $V$. In particular, the dimension of $A$ is a square.
    \end{theorem}

    The proof of the theorem relies on the following proposition:

    \begin{proposition}{Burnside}{burnside}
        Let $A$ be a finite-dimensional $\C$-algebra and $S$ be a finite-dimensional simple $A$-module. Then the structural map $\rho: A \to \End(S)$ is surjective.
    \end{proposition}

    We first assume the proposition and prove Wedderburn's theorem.

    \begin{proof}[Proof of Theorem \ref{thm:wedderburn}]

        We first assume that $A$ has a finite-dimensional simple module $S$. Later we will argue why such a module exists.
        
        Since $A$ is simple, $\rho$ is injetive. By the proposition above, $\rho: A \to \End(S)$ is surjective, hence an isomorphism.

        Now we show that such an $S$ exists. For this, we look at the regular $A$-module $A$. We take a submodule $\{0\} \neq S \subseteq A$ and $\dim_\C(S)$ minimal among all $A$-submodules in $A$. Since $A \neq \{0\}$ is a finite-dimensional submodule of $A$, such an $S$ exists. If $S$ is not a simple $A$-module, it will have a non-trivial submodule $S' \subseteq S$, which is an ideal of $A$ with $\dim_\C(S') < \dim_\C(S)$, which is a contradiction. Then $S$ is simple.
    \end{proof}

    Now to Proposition \ref{prop:burnside}

    \begin{lemma}{}{3.18}
        Using the notation from \ref{prop:burnside}, we have:
        \begin{enumerate}
            \item (cyclicity) Fix $0 \neq s \in S$, then for any $s' \in S$, there exists an $a \in A$ such that $\rho(a)(s) = s'$
            \item (cocyclicity) Fix $0 \neq \alpha \in S^*$, for any $\alpha' \in S^*$, there exists an $a \in A$ such that $\alpha' = \alpha \cdot \rho(a)$ (i.e. $\forall s \in S, \alpha'(s) = \alpha(\rho(a)(s))$
            \item If $u \in \rho(A)$ is of rank $\geq 2$, then there exists $u' \in \rho(A)$ such that $0 < \rank(u') < \rank(u)$.
        \end{enumerate}
    \end{lemma}

    \begin{proof}
    \begin{enumerate}
        \item Since $S$ is simple and $\rho(A)(s)$ is a submodule of $S$, hence $\rho(A)(s)=S$.
        \item We consider $S^o_\alpha = \curly{ s\in S | \forall a \in A, \spitz{\alpha, \rho(a)(s)} = 0}$, that is to say, the set of elements in S that are annihilated by the cyclic module $\alpha \rho(A)$. If $S_\alpha^o\ni s \neq 0$, it follows from $\rho(A)(s)=S$ that $\alpha(S)=\curly 0$ hence $\alpha = 0$, which is not possible. Therefore $S_\alpha^o = \curly 0$ and hence $\alpha \rho(A) = S^*$.
        \item Since $u$ has rank $\geq 2$, there exist $s_1, s_2 \in S$ such that $u(s_1), u(s_2)$ are linearly independent. From (1), there exists $a\in A$ such that $\rho(a)(u(s_1))=s_2$. Therefore $u \circ \rho(a) \circ u(s_1)$ and $u(s_1)$ are linearly independent, hence $\forall \lambda \in \C$, $u\circ\rho(a)\circ u - \lambda u\neq 0$. We look at $u \circ \rho(a): \Img(u) \to \Img (u)$. Since we are over $\C$, such an endomorphism must have an eigenvalue $\lambda$, i.e. $u':= u\circ \rho(a)\circ u -\lambda u$ is not invertible.
        
        We show that $0<\rk(u') < \rk(u)$ and $u'\in \rho(A)$.
        Since $u, \rho(A)\in \rho(a)$, it follows $u'\in \rho(A)$. Moreover, we have shown that $u'\neq 0$. Hence $\rk(u')> 0$.
        
        Since $u'=(u\circ \rho(a)-\lambda\id)u$ and $u\circ \rho(a) - \lambda \id$ is not invertible, it follows that $\rk(u')< \rk(u)$.
    \end{enumerate}
    \end{proof}
    
\separline{Week 10}

\separline{VL 15}
    \begin{proof}[Proof of \ref{prop:burnside}]
        First notice, that any element in $\End(S)$ is a linear combination of rank 1 endomorphisms. Indeed, fix a basis of $S$ to identify it with $M_{\dim S}(\C)$, then the elementary matrices $E_{i,j}$ are of rank 1.

        To show that $\rho$ is surjective, it suffices to show that all rank 1 endomorphisms are in $\rho(A)$.
        Since $\rho(1_A)=\id_S$, $\rho(A)\neq\curly{0}$ and we choose a nonzero endomorphism $\varphi\in\rho(A)$. Without loss of generality, we assume $\varphi$ to be of rank 1. Otherwise, apply the rank reduction \ref{lem:3.18}(3) until we get a rank 1 endomorphism.

        Let $\phi'$ be rank 1 endomorphism in $\End(S)$. Since $\varphi'$ has rank 1, we choose $S'$ a genarator of $\Img \varphi'$ and $\alpha'\in S^*$ such that $\varphi'(v) = \alpha'(v)s' \forall v\in S$. That is to say, the image of $\phi'\in\End(S)$ in $S\otimes S^*$ is $s\otimes a$. Similarly we denote by $s\otimes a$ the image of $\phi$ in $S\otimes S^*$. By \ref{lem:3.18}(1),(2), there exist $a,b \in A$ such that $s'=\rho(a)(s), \ \alpha'=\alpha \cdot \rho(b)$.

        We claim that $\phi'=\rho(a)\circ \phi \circ \rho(b)$.

        Indeed. $\forall v \in S$, $$\rho(a)\circ \phi \circ \rho(b)(v) = \rho(a)(\spitz{\alpha, \rho(b)(v)}s) = \alpha'(v)\rho(a)s = \alpha'(v)s' = \phi'(s)$$

        Now if $\phi=\rho(c)$ for some $c\in A$, then $\phi'=\rho(acb)\in \rho(A)$. Therefore $\rho(A)$ contains all rank 1 endomorphisms.
    \end{proof}

    \subsection{Finite dimensional modules}

    In this section, we classify finite dimensional modules over $\End(V)$ for a finite-dimensional vector space $V$. We have shown that $V$, as an $\End(V)$-module, is simple (\ref{exa:3.14}).
    By fixing a basis of $V$, it suffices to look at matrices.
    We study in this section finite dimensional $M_n(\C)$-modules.

    \begin{notation*}{}
        $M_n := M_n(\C)$
    \end{notation*}
    \begin{proposition}{}{3.19}
        Let $M$ be a finite-dimensional $M_n(\C)$-module with structure map $\rho: M_n(\C) \to \End(M)$. Then $M \cong \round{\C^n}^{\oplus r}$ as $M_n(\C)$-modules.
    \end{proposition}

    \begin{proof}
        \begin{enumerate}
            \item [Step 1] A first decomposition of $M$:

            In $M_n$ there are natural idempotents $E_{i,i}$. they are orthogonal with sum $1_{M_n}$. Let $\pi_i:= \rho(E_{i,i})$. Then $\pi_1, \dots, \pi_n$ are orthogonal idempotents with $\pi_1 + \dots + \pi_n = \id_M$. Let $V_i := \pi_i(M)$.Then from \ref{prop:3.10}, we have the decomposition of $\C$-vector spaces (the $\pi_i$ are not necessarily in $\End_{M_n}(M)$ so $V_i$ are not necessarily $M_n$-modules)
            $$M=V_1 \oplus \dots \oplus V_n$$

            \item[Step 2] We show that $\forall i,j: V_i\cong V_j$ as vector spaces. For this we show that the image of $\rho(E_{i,j})|{V_j}$ is contained $V_i$. This follows from the identity $E_{i,i}E_{i,j}=E_{i,j}$ and the following: 
            From definition, ${\pi_i}_{|V_i}=\id_{V_i}$ and ${\pi_i}_{|V_j}=0$ for $i\neq j$.

            Since $E_{i,j} E_{j,i} = E_{i,i}$ for all $i,j$, we get that the $\rho(E_{i,j})_{|V_j}$ are bijective.

            \item[Step 3] We show that $M\cong (\C^n)^{\oplus r}$ with $r=\frac{\dim(M)}n$.

            For this we fix a basis $e_1^{(1)}, \dots, e_r^{(1)}$ of $V_1$ and set $e_i^{(k)}:= \rho(E_{k,1})(e_i^{(1)}$ (which is of course a basis of $V_k$).

            We denote $W_j:= \spitz{e_j^{(1)}, \dots, e_j^{(n)}}$ for $1\leq j \leq r$. 

            As vector spaces $$M = W_1 \oplus \dots \oplus W_r$$

            We show that $W_j$ is an $M_n$-submodule of $M$.

            For this we compute $$\rho(E_{i,j})(e_k^{(l)}) = \rho(E_{i,j})\rho(E_{l,1})(e_k^{(1)}) = \delta_{j,l} \rho(E_{i,1})(e_k^{(1)}) = \delta_{j,l} e_k^{(i)}$$
            The action of any $E_{i,j}$ therefore will not change the lower index, so $W_j$ is an $M_n$-submodule of $M$.

            We define $\phi:M\to(\C^n)^{\oplus r}$ given by $(w_1, \dots, w_r) \mapsto (\phi_1(w_1), \dots, \phi_r(w_r))$ for $w_i\in W_i$. It remains to show that $\phi$ is an $M_n$-module homomorphism.
            We take the basis $(\underbrace{e_1^{(1)}, \dots, e_1^{(n)}}_{W_1}, \dots, \underbrace{e_r^{(1)}, \dots, e_r^{(n)}}_{W_r})$ of $M$.

            Under this basis, $\rho(E_{i,j}$ has the block diagonal form $\diag(E_{i,j}, \dots, E_{i,j})$. Therefore by linearity, for $A\in M_n$, $\rho(A)$ has the form $\diag(A,\dots, A)$. Hence $\phi$ is and $M_n$-module homomorphism.
        \end{enumerate}
    \end{proof}
    
    \begin{corollary}{}{3.20}
        Any finite dimensional simple $M_n(\C)$-module is isomorphic to $\C^n$ as $M_n(\C)$-modules.
    \end{corollary}
    
    In particular, any two finite dimensional, simple $M_n(\C)$-modules are isomorphic as $M_n(\C)$-modules.
    
    \begin{proof}
        Let $M$ be a simple $M_n$-module. If in the above decomposition $M\cong(\C^n)^{\oplus r}$ we have $r>1$, then $M$ has a nontrivial submodule $\C^n$, contradiction.
    \end{proof}
    
    Next we will explain this number $r$ in a more canonical way.

    Let $M$ be an $A$-module and $V$ be a vector space. WE define an $A$-module structure on $V\otimes V$ by $\rho: A \to \End(V\otimes M): \rho(a)(v\otimes m):= v \otimes \rho(a)(m)$ for $a\in A, v\in V$ and $m\in M$.

    This $A$-module structure is not natural because it has nothing to do with $V$.
    \begin{proposition}{Isotypic decomposition}{3.21}
        Let $M$ be a finite dimensional $M_n(\C)$-module. The evaluation map
        \begin{equation*}
        \begin{split}
            \ev: \Hom_{M_n(\C)}(\C^n,M) \tens \C^n &\to M \\
            \varphi \tens v &\mapsto \varphi(v)
        \end{split}
        \end{equation*}
        is an isomorphism of $M_n(\C)$-modules.
    \end{proposition}

    Note, that in this proposition there is no need to choose anything and the iso is God-given.

    \begin{proof}
        It is straightforward to verify, that the evaluation map is an $M_n$-module homomorphism.

    We first write down a basis of $\Hom_{M_n}(\C^n,M)$: In the proof of \ref{prop:3.19}, we have shown that $M= W_1 \oplus \dots \oplus W_r$ and there exists an $M_n$-module isomorphism $\psi_i: \C^n\to W_i$ by composing with the canonical inclusion $\iota_i: W_i \to M$ (it is an $M_n$-module homomorphism).
    \end{proof}
    
\separline{VL 16}
    Next, we will explain the number $r$ from \ref{prop:3.19} in a more canonical way.
    Let $M$ be an $A$-module and $V$ be a vector space. We define an $A$-module structure on $V \tens M$ by:
    \begin{equation*}
    \begin{split}
        \rho: A &\to \End(V \tens M)\\
            a &\mapsto \round{v \tens m \mapsto v \tens \rho(a)(m)}
    \end{split}
    \end{equation*}
    This $A$-module structure is not natural because it has nothing to do with $V$.

    Note that in proposition $\ref{prop:3.21}$ there is no need to choose anything (for example the number $r$), this means the isomorphism is natural.

    \begin{proof}
        Proposition \ref{prop:3.21}.

        Take $A \in M_n(\C)$ and $\varphi \in \Hom_{M_n(\C)}(\C^n, M), v \in \C^n$, then
        \begin{equation*}
            \ev(A.(\varphi \tens v)) = \ev(\varphi \tens Av) = \varphi(Av) = A \varphi(v) = A.\ev(\varphi \tens v)
        \end{equation*}
        we do not need to check linearity because of the universal property of the tensor product.

        We first write down a basis of $\Hom_{M_n(\C)}(\C^n,M)$. In the proof of Proposition \ref{prop:3.19}, we have shown that $M = \bigoplus_{i=1}^r W_i$ and there exist isomorphisms of $M_n(\C)$-modules $\psi_i:\C^n \to W_i$. By composing with the canonical inclusion $\iota_i: W_i \to M$ we obtain $\varphi_i:= \iota_i \circ \psi_i \in \Hom_{M_n(\C)}(\C^n,M)$. These $\varphi_i$ are linearily independent since their images are in pairwise distinct direct sum components.
        Now we show the generating property. Take $\varphi \in \Hom_{M_n(\C)}(\C^n,M)$ and consider $\pi_i \circ \varphi \in \Hom_{M_n(\C)}(\C^n,W_i)$. Since $\psi_i: \C^n \to W_i$ is an isomorphism, by Schur, $\pi_i \circ \varphi = \lambda_i \psi_i$ for some $\lambda_i \in \C$. Now we consider
        \begin{equation*}
            \varphi = (\underbrace{\iota_1\pi_1 + \dots \iota_r \pi_r}_{=\id_M})\varphi = \sum_{i=1}^r \lambda_i \varphi_i
        \end{equation*}
        As a summary, we have shown that $\dim \Hom_{M_n(\C)}(\C^n,M) = r$ and hence both sides of the evluation maps have the same dimension. It remains tho show that the map is surjective. For this we take $m \in M$ and decompose it as $m = \sum_{i=1}^r m_i$ with $m_i \in W_i$. We denote the preimage of $m_i$ under $\psi_i$ with $x_i \in \C^n$. Then
        \begin{equation*}
            \ev \round{\sum_{i=1}^r \varphi_i \tens x_i} = \sum_{i=1}^r \varphi_i(x_i) = \sum_{i=1}^r \iota_i \circ \psi_i \circ \psi_i^{-1}(m_i) = \sum_{i=1}^r \iota_i(m_i) = m
        \end{equation*}
    \end{proof}

\section{Semisimple modules and algebras}
    Let $A$ be a $\C$-algebra.
    \begin{definition}{Semisimplicity}{3.22}
        \begin{enumerate}
            \item An $A$-module $M$ is call \textbf{semisimple} if it is a direct sum of simple $A$-modules.
            \item A finite dimensional $\C$-algebra $A$ is called \textbf{semisimple}, if any non-zero finite dimensional $A$ module is semisimple.
        \end{enumerate}
    \end{definition}

    \begin{example}{}{3.23}
        \begin{enumerate}
            \item It follows from Proposition \ref{prop:3.19} and Corollary \ref{cor:3.20} that finite dimensional simple algebras are semisimple.
            \item By Maschkes theorem \ref{thm:maschke} it follows that $\C[G]$ is semisimple if $G$ is a finite group. One can actually show that if $\cha(K) \nmid \#G$, then $K[G]$ is semisimple ($K$ is a field).
        \end{enumerate}
    \end{example}

    \begin{remark*}{a comment on semisimplicity - F}
        There is another quite useful, equivalent condition for semisimplicity, namely:

        $M$ is semisimple iff any submodule $N$ has a complement $N'$ such that $M = N \oplus N'$.
        
        This even holds for infinite dimensional modules. The proof uses Zorns Lemma.
    \end{remark*}
    
    \begin{exercise}{}{16}
        Let $M$ be a finite-dimensional semisimple $A$-module. Any submodule and quotient module of $M$ is semisimple.
    \end{exercise}
    \begin{proof}
        We proof it for any $A$-module $M$, for the finite-dimensional case we don't need Zorns Lemma, but we just argue by dimension. If you need more details on the Zorns Lemma proofs just ask me (Felix).
        
        Write $M = \bigoplus_{i \in I}S_i$ for $S_i$ simple and $I$ need not be finite. Now take a submodule $N \subseteq M$. Then $M/N = \sum_{i \in I} \overline{S_i}$. Using Zorns Lemma we can refine this sum to be direct.

        For the submodule, we write $N = \sum\limits_{S_i \subseteq N \text{ simple}} S_i$. This holds because since $M$ is semisimple, any $x \in M$ is contained in a simple module (using Zorns Lemma). Again using Zorns Lemma we can refine this sum to be direct.
    \end{proof}

    \begin{proposition}{}{3.24}
        A finite-dimensional $\C$-algebra is semisimple iff the left $A$-module $A$ is semisimple.
    \end{proposition}

    \begin{proof}
        The direction $"\Rightarrow"$ follows from the definition.

        For the other direction "$\Leftarrow$", take any $A$-module $M$. Now consider $\pi: A^n \twoheadrightarrow M, e_i \mapsto m_i$, where the $m_i$ are a $\C$-basis of $M$, then $M \cong A^{n}/\ker \pi$ by the Noetherian isomorphism theorem, hence $M$ is semisimple by the exercise above.
    \end{proof}

    The main result of this section is the following:

    \begin{theorem}{Artin-Wedderburn}{ArWb}
        Let $A$ be a finite-dimensional $\C$-algebra. The following statements are equivalent:
        \begin{enumerate}
            \item $A$ is semisimple
            \item $A$ is a product of matrix algebras. That is to say there exist $n_1,\dots,n_k \geq 1$ such that $A \cong M_{n_1}(\C) \times \dots \times M_{n_k}(\C)$
        \end{enumerate}
    \end{theorem}

    We will prove the theorem in the next two sections:
    
    \subsection{Hom-spaces}
        We prove $(1) \Rightarrow (2)$ in this paragraph. By defintion, the $A$-module $A$ is semisimple. There exist simple $A$-modules $S_1,\dots,S_k$, pairwise non-isomorphic such that $A \cong \bigoplus\limits_{i=1}^k S_i^{\oplus n_i}$ as $A$-modules with $n_1,\dots,n_k \geq 1$.

        According to Example \ref{exa:3.7}, $\End_A\round{\tensor[_A]{A}{}} \cong A^\op$. So we investigate $\End_A\round{\bigoplus\limits_{i=1}^k S_i^{\oplus n_i}}$. For this, we consider the following general setup. Let $M_1,\dots,M_k$ be $A$-modules. We have an isomorphism of $\C$-vector spaces
        \begin{equation*}
            \Hom_A\round{\bigoplus_{i=1}^k M_i,\bigoplus_{i=1}^k M_i} \cong \bigoplus_{1 \leq i,j \leq k} \Hom_A(M_i,M_j)
        \end{equation*}
        We write down this map in detail. Let $\iota_i: M_i \hookrightarrow \bigoplus_{i=1}^k M_i$ and $\pi: \bigoplus_{i=1}^k M_i \twoheadrightarrow M_i$ be the canonical maps. Given $f$ in the space on the left hand side, the $\Hom_A(M_i,M_j)$-component of the image of $f$ under the above isomorphism is given by $\Phi(f)_{i,j} = \pi_j \circ f \circ \iota_i \in \Hom_A(M_i,M_j)$. Hence
        \begin{equation*}
            \Phi(f) = \bigoplus_{1 \leq i,j \leq k} \pi_j \circ f \circ \iota_i
        \end{equation*}

        How about the composition of endomorphisms?

        The big direct sum can be organized in a matrix form. Let $M$ be an $n \times n$ matrix whose $(i,j)$-th entry is from $\Hom_A(M_j,M_i)$. Moreover, we can multiply $M$ with another Matrix $M'$ of this form where the usual multiplication of scalars is replaced by the composition of maps. This gives a $\C$-algebra structure on the right-hand side.

        \begin{exercise}{}{17}
            Prove that the above linear isomorphism is indeed an isomorphism, and show that it is a homomorphism of $\C$-algebras.
        \end{exercise}
        \begin{proof}
            Its inverse is given by the following map:
            \begin{equation*}
            \begin{split}
                \Psi: \bigoplus_{1 \leq i,j \leq k} \Hom_A(M_i,M_j) &\to \Hom_A\round{\bigoplus_{i=1}^k M_i,\bigoplus_{i=1}^k M_i} \\
                \bigoplus_{1 \leq i,j \leq k} \varphi_{i,j} & \mapsto \bigoplus_{1 \leq i,j \leq k} \iota_j \circ \varphi_{i,j} \circ \pi_i
            \end{split}
            \end{equation*}
            One quickly calculates, that
            \begin{equation*}
                \Psi \circ \Phi (f) = \Psi \round{\bigoplus_{1 \leq i,j \leq k} \pi_j \circ f \circ \iota_i} = \bigoplus_{1 \leq i,j \leq k} \iota_j \circ \pi_j \circ f \circ \iota_i \circ p_i = \bigoplus_{1 \leq i,j \leq k} f_{i,j} = f
            \end{equation*}
            and
            \begin{equation*}
            \begin{split}
                \Phi \circ \Psi \round{\bigoplus_{1 \leq i,j \leq k} \varphi_{i,j}} &= \Phi \round{\bigoplus_{1 \leq i,j \leq k} \iota_j \circ \varphi_{i,j} \circ \pi_i} \\
                &= \bigoplus_{1 \leq l,m \leq k}  \bigoplus_{1 \leq i,j \leq k} \underbrace{\pi_l \circ \iota_j}_{\id_{M_j} \delta_{l,j}} \circ \varphi_{i,j} \circ \underbrace{\pi_i \circ \iota_m}_{\id_{M_i} \delta_{i,m}} 
                = \bigoplus_{1 \leq i,j \leq k} \varphi_{i,j}
            \end{split}
            \end{equation*}
            One can see, that $\Phi$ is linear. It remains to show that $\Phi$ is a homomorphism of $\C$-algebras, i.e. $\Phi(f \circ g) = \Phi(f) \cdot \Phi(g)$. This part will be delayed until the issue above is resolved.
            %We just look at $\Phi(f \circ g)_{i,j}$. Clearly, we need to have something of the form $g_{l,j}$, then for the composition to make sense and go into $i$, we need $f_{i,l}$ but we can choose $l$ freely, hence $\Phi(f \circ g)_{i,j} = \sum_{l=1}^n f_{i,l} \circ g_{l,j}$
        \end{proof}

        Applying this general setup to the above special case and noticing that by Schurs Lemma (\ref{lem:schurMOD}), $\Hom_A(S_i,S_j) = \{0\}$ for $i \neq j$. We have in fact an isomorphism of $\C$-algebras
        \begin{equation*}
            \End_A\round{S_i^{\oplus n_i}} \cong \bigoplus_{1 \leq p,q \leq n_i} \End_A(S_i) \cong \bigoplus_{1 \leq p,q \leq n_i} \C
        \end{equation*}

        Writing this isomorphism a matrix form as above, we obtain an isomorphism of $\C$-algebras $\End_A\round{S_i^{\oplus n_i}} \cong M_{n_i}(\C)$. It follows then, that there exists an isomorphism of $\C$-algebras:
        \begin{equation*}
        \begin{split}
            A^\op \cong \End\round{\tensor[_A]{A}{}} \cong \bigtimes_{i=1}^k \End_A\round{S_i^{\oplus n_i}} \cong \bigtimes_{i=1}^k M_{n_i}(\C)
        \end{split}
        \end{equation*}
        Since $M_n(\C)^\op \cong M_n(\C)$ by taking the transpose of a matrix, we proved that $A$ is isomorphic to a product of matrix algebras as $\C$-algebras.
        
    \subsection{Isotypic decomposition}
        We prove $(2)\Rightarrow(1)$ by showing that if $A$ is a product of matrices, then every $A$-module is semisimple. We start by classifying simple $A$-modules. Let $\Irr(A)$ denote the set of \underline{simple} $A$-modules up to isomorphism.

        \begin{proposition}{}{3.26}
            Let $V_1,\dots,V_k$ be finite dimensional $\C$-vector spaces and $A:=\End(V_1) \times \dots \times \End(V_k)$. Then $\Irr(A) = \{V_1,\dots,V_k\}$.
        \end{proposition}

        \begin{proof}[Proof of Proposition \ref{prop:3.26}]
            We divide the proof into steps:
            \begin{enumerate}
                \item The $V_1,\dots,V_k$ are simple $A$-modules
                \item For $i \neq j$, $V_i \not \cong V_j$ as $A$-modules
                \item Any simple $A$-module is isomorphic to some $V_l$ as $A$-modules.
            \end{enumerate}
            \underline{Step $1$:} From Corollary \ref{cor:3.20}, $V_i$ is a simple $\End(V_i)$-module, because the canoncial projection $\pi_i:A \to \End(V_i)$ is a surjective $\C$-algebra homomorphism $V_i$ is a simple $A$-module.

            \underline{Step $2$:} For this we show that $\Hom_A(V_i,V_j) = \{0\}$ for $i \neq j$. Consider the following central orthogonal idempotents
            \begin{equation*}
                p_i = (0,\dots,0,\id_{V_i},0,\dots,0) \in A
            \end{equation*}
            with sum $1_A$. Let $\varphi \in \Hom_A(V_i,V_j)$, then for any $a \in A$, we have
            \begin{equation*}
                \pi_j(a) \circ \varphi = \varphi \circ \pi_i(a)
            \end{equation*}
            because $\pi_i,\pi_j$ are the structural maps of $V_i$ and $V_j$ as $A$-modules respectively. We take $a = p_j$, then $\pi_i(a) = 0$ and $\pi_j(a) = \id_{V_j}$, hence $\varphi \equiv 0$.

            \underline{Step $3$:} Let $W$ be a simple $A$-module with structural map $\rho: A \to \End(W)$. We show that $W$ is isomorphic to some $V_l$ as $A$-modules.

            First it follows from $1_A = \sum_{i=1}^k p_i$ that there exists $1 \leq l \leq k$ such that $\rho(p_l) \neq 0$. We set $W' = \rho(p_l)(W) \subseteq W$. Since $p_l$ is central, $\rho(a)\rho(p_l)(w) = \rho(p_l) \underbrace{\rho(a)(w)}_{\in W} \in W'$ for all $a \in A, w \in W$ and hence $W'$ is a non-zero $A$-submodule of $W$. Since $W$ is simple, we have $W' = W$.

            We consider $p_i$ for $i \neq l$, then from the orthogonality of the idempotents it follows that
            \begin{equation*}
                \rho(p_i)(W) = \rho(p_i)\rho(p_l)(W) = \rho(p_ip_l)(W) = \{0\}
            \end{equation*}
            This implies that $\forall i \neq l$, $\rho|_{\End(V_i)} = 0$, therefore $\rho$ is infact $\rho_l:\End(V_l) \to \End(W)$ and this defines a simple $\End(V_l)$-module structure on $W$. It follows from Corollary \ref{cor:3.20} that there exists an $\End(V_l)$-module isomorphism $f: W \xrightarrow{\cong} V_l$, i.e. $\forall \alpha \in \End(V_l)$:
            \begin{equation*}
                \alpha \circ f = f \circ \rho_l(\alpha)
            \end{equation*}
            It remains to show that $f$ is an isomorphism of $A$-modules: Take $a \in A$. We need to show that $\pi_l(a) \circ f = f \circ \rho(a)$. Indeed
            \begin{equation*}
                f \circ \rho(a) = f \circ \rho(a p_l) = f \circ \underbrace{\rho_l(\pi_l(a))}_{\in \End(V_l)} = \pi_l(a) \circ f
            \end{equation*}
        \end{proof}

\separline{Week 11}

        \begin{proposition}{Isotypic decomposition}{3.27}
            The evaluation map
            \begin{equation*}
            \begin{split}
                \ev: \bigoplus_{i=1}^k \Hom_A(V_i,W) \tens V_i &\to W \\
                    \sum_{i=1}^k \varphi_i \tens v_i &\mapsto \sum_{i=1}^k \varphi_i(v_i)
            \end{split}
            \end{equation*}
            is an isomorphism of 
        \end{proposition}

\section{Schur-Weyl duality}
    \subsection{Commutant and double commutant}
        \begin{definition}{}{3.28}
            Let $S \subseteq \End(V)$, then the \textbf{commutant} of $S$ in $\End(V)$ is defined by:
            \begin{equation*}
                C(S):= \{u \in \End(V) \ | \ \forall s \in S: us = su \}
            \end{equation*}
        \end{definition}
        As $C(S) \subseteq \End(V)$, the \textbf{bicommutant} or \textbf{double commutant} of $S$ in $\End(V)$ is defined as $C(C(S))$.

        \begin{theorem}{Double commutant}{3.29}
            Let $A \subseteq \End(V)$ be a semisimple algebra and $B:=C(A)$. Then $B$ is semisimple and
            \begin{equation*}
                B \cong \bigoplus_{i=1}^t \End(U_i) \tens \id_{V_i}
            \end{equation*}
            In particular, $C(C(A)) = A$.
        \end{theorem}

        \begin{exercise}{}{18}
            Let $V, W$ be two $K$-vector spaces. The $K$-linear map
            \begin{equation*}
            \begin{split}
                \End(V) \tens \End(W) &\to \End(V \tens W) \\
                    \varphi \tens \psi &\mapsto (v \tens w \mapsto \varphi(V) \tens \varphi(w))
            \end{split}
            \end{equation*}
            is a $K$-algebra homomorphism. When $V,W$ are finite-dimensional, the above map is an isomorphism.
        \end{exercise}

        \begin{lemma}{}{3.30}
            Let $U,W$ be finite-dimensional $\C$-vector spaces and $V:= U \tens W$. Then in the $\C$-algebra $\End(V)$, we have
            \begin{equation*}
            \begin{split}
                C(\id_U \tens \End(W)) &= \End(U) \tens \id_W \\
                C(\End(U) \tens \id_W) &= \id_U \tens \End(W)
            \end{split}
            \end{equation*}
        \end{lemma}

\separline{VL 18}

        \begin{proposition}{}{3.31}
            Let $A,B \neq \{0\}$ be two finite-dimensional $\C$-algebras and $\varphi: A \to B$ be a surjective $\C$-algebra homomorphism. Then if $A$ is semisimple, so is $B$. 
        \end{proposition}

    \subsection{Schur-Weyl duality}

        Let $V$ be a finite-dimensional $\mathbb{C}$-vector space. 
    
        The symmetric group $\mathfrak{S}_n$ acts on $V^{\otimes n}$ by permuting the position of the tensor components. In the language of representations, we have a group homomorphism 
        $$\mathfrak{S}_n\to\mathrm{GL}(V^{\otimes n}),\ \ \sigma\mapsto(v_1\otimes\ldots\otimes v_n\mapsto v_{\sigma^{-1}(1)}\otimes\ldots\otimes v_{\sigma^{-1}(n)}).$$
        It induces an algebra homomorphism $\rho_n:\mathbb{C}[\mathfrak{S}_n]\to\mathrm{End}(V^{\otimes n})$. Let $A:=\rho_n(\mathbb{C}[\mathfrak{S}_n])$. Then $A\subseteq\mathrm{End}(V^{\otimes n})$ is a semisimple subalgebra. 
    
        The group $\mathrm{GL}(V)$ acts on $V^{\otimes n}$ in the standard way:
        $$\mathrm{GL}(V)\to\mathrm{GL}(V^{\otimes n}),\ \ g\mapsto (v_1\otimes\ldots\otimes v_n\mapsto g(v_1)\otimes\ldots\otimes g(v_n)).$$
        It induces an algebra homomorphism $\varphi_n:\mathbb{C}[\mathrm{GL}(V)]\to\mathrm{End}(V^{\otimes n})$.
        We let $B$ denote the image of $\varphi_n$ in $\mathrm{End}(V^{\otimes n})$.
    
        \begin{theorem}{Schur-Weyl duality}{SW}
        We have $B=C(A)$, hence $B$ is semisimple and $A=C(B)$.
        \end{theorem}
    
        We need the following lemma:
    
        \begin{lemma*}{}
            By identifying $M_n(\mathbb{C})$ with $\mathbb{C}^{n^2}$, the subset $\mathrm{GL}_n(\mathbb{C})$ is dense in $M_n(\mathbb{C})$.
        \end{lemma*}
    
        \begin{proof}
            For $A\in M_n(\mathbb{C})$ of rank $r$, we choose $P,Q\in\mathrm{GL}_n(\mathbb{C})$ such that $A=PI_{r,n} Q$ where $I_{r,n}=\mathrm{diag}(1,\ldots,1,0,\ldots,0)$ where there are $r$ copies of $1$. We consider a sequence of matrices $A_k:=PI_{r,n}(k)Q$ where $I_{r,n}(k)=\mathrm{diag}(1,\ldots,1,\frac{1}{k},\ldots,\frac{1}{k})$. Then $A_k\in\mathrm{GL}_n(\mathbb{C})$ and 
            $$\lim_{k\to+\infty} A_k=A$$ 
            since matrix multiplication is continuous.
        \end{proof}
    
        \begin{proof}[Proof of Theorem \ref{thm:SW}]
            It follows from the definition that $B\subseteq C(A)$ and $A\subseteq C(B)$. If we can prove that $B=C(A)$, it follows from the double commutant theorem that $B$ is semisimple and $A=C(B)$.
        
            We fix several notations. Let $e_1,\ldots,e_m$ be a basis of $V$. For a tuple $J=(j_1,\ldots,j_n)\in [m]^n$ with $1\leq j_s\leq m$, we denote $e_J:=e_{j_1}\otimes\ldots\otimes e_{j_n}\in V^{\otimes n}$. Then $\{e_J\mid J\in[m]^n\}$ forms a basis of $V^{\otimes n}$. For $\sigma\in\mathfrak{S}_n$, we denote $\sigma(J):=(j_{\sigma^{-1}(1)},\ldots,j_{\sigma^{-1}(n)})$. Then $e_{\sigma(J)}=\sigma(e_J)$.
        
            We split the proof into several parts:
            \begin{enumerate}
            \item[(a)] We first characterize elements in $C(A)$. Let $\varphi\in \mathrm{End}(V^{\otimes n})$. We write $\varphi$ into a linear combination of the basis:
            $$\varphi(e_J)=\sum_{I\in[m]^n}a_{I,J}e_I$$
            with $a_{I,J}\in\mathbb{C}$. Now $\varphi\in C(A)$ if and only if for any $\sigma\in \mathfrak{S}_n$, 
            $$\varphi(\rho_n(\sigma)(e_J))=\rho_n(\sigma)(\varphi(e_J))$$
            which is equivalent to saying that 
            $$\sum_{I\in [m]^n}a_{I,\sigma(J)}e_I=\sum_{I\in [m]^n}a_{I,J}e_{\sigma(I)}.$$
            As a summary, $\varphi\in C(A)$ if and only if $a_{I,\sigma(J)}=a_{\sigma^{-1}(I),J}$ if and only if $a_{\sigma(I),\sigma(J)}=a_{I,J}$ for any $I,J\in[m]^n$ and $\sigma\in\mathfrak{S}_n$.
        
            Let $\Gamma:=\{(I,J)\mid I,J\in[m]^n\}$. The group $\mathfrak{S}_n$ acts on $\Gamma$ via: for $\sigma\in\Gamma$, $\sigma(I,J):=(\sigma(I),\sigma(J))$. We set $\mathcal{O}:=\Gamma/\mathfrak{S}_n$ be the set of orbits of this group action. The numbers $a_{I,J}$ above can be packed into a function $a$ on the set $\Gamma$ with values in $\mathbb{C}$. What we have proved above can be rephrased: $\varphi\in C(A)$ if and only if the function $a$ is constant on each $\mathfrak{S}_n$-orbit in $\Gamma$. Therefore $\varphi$ is uniquely defined by a function $a:\mathcal{O}\to\mathbb{C}$, $\gamma\mapsto a_\gamma:=a_{I,J}$ for $(I,J)\in\gamma$.
        
            \item[(b)] We define a bilinear form on $\mathrm{End}(V^{\otimes n})$ by: for $\varphi,\psi\in\mathrm{End}(V^{\otimes n})$, 
            $$\langle\varphi,\psi\rangle:=\mathrm{tr}(\varphi\circ\psi).$$ 
            It is a non-degenerate symmetric bilinear form. We show that its restriction to the subspace $C(A)$, $\langle-,-\rangle:C(A)\times C(A)\to\mathbb{C}$ is also non-degenerate. 
        
            For this note that $C(A)=\mathrm{End}_{\mathfrak{S}_n}(V^{\otimes n})$. This suggests us to consider the projector in $\mathbb{C}[\mathfrak{S}_n]$:
            $$P:\mathrm{End}(V^{\otimes n})\to \mathrm{End}(V^{\otimes n}),\ \ \varphi\mapsto\frac{1}{\#\mathfrak{S}_n}\sum_{\sigma\in\mathfrak{S}_n}\rho_n(\sigma)\circ\varphi\circ\rho_n(\sigma^{-1}).$$
            The image of the projector is exactly $C(A)$. Moreover, if $\psi\in C(A)$ and $\varphi\in\mathrm{End}(V^{\otimes n})$ then 
            $$\langle P(\varphi),\psi\rangle=\frac{1}{\#\mathfrak{S}_n}\sum_{\sigma\in\mathfrak{S}_n}\mathrm{tr}(\rho_n(\sigma)\varphi\rho_n(\sigma^{-1})\psi)=\mathrm{tr}(\varphi\psi)=\langle\varphi,\psi\rangle$$
            since $\psi$ commutes with $\rho_n(\sigma^{-1})$.
        
            If $\psi\in C(A)$ is orthogonal to the entire $C(A)$ with respect to $\langle-,-\rangle$, then for any $\varphi\in\mathrm{End}(V^{\otimes n})$, 
            $$\langle\varphi,\psi\rangle=\langle P(\varphi),\psi\rangle=0,$$
            and hence $\psi=0$ since $\langle-,-\rangle$ is non-degenerate on $\mathrm{End}(V^{\otimes n})$.
            \item[(c)] Since $B\subseteq C(A)$, to show that they are equal, it suffices to show that if for any $\varphi\in C(A)$ and $g\in\mathrm{GL}(V)$, $\langle\varphi,\varphi_n(g)\rangle=0$ holds, then $\varphi=0$. For this, we describe $\varphi_n(g)$. 
        
            For $g\in\mathrm{GL}(V)$, 
            $$\varphi_n(g)(e_I)=g(e_{i_1})\otimes\ldots\otimes g(e_{i_n})=\sum_{J\in [m]^n}g_{j_1,i_1}\cdots g_{j_n,i_n}e_J.$$
            We set $g_{J,I}:=g_{j_1,i_1}\cdots g_{j_n,i_n}$. If $\varphi(e_J)=\sum_{K}a_{K,J}e_K$, then
            $$0=\langle\varphi,\varphi_n(g)\rangle=\mathrm{tr}(\varphi\circ\varphi_n(g))=\sum_{I}e_I^*(\varphi\circ\varphi_n(g)(e_I))=\sum_{I,J}a_{I,J}g_{J,I}.$$
            The function 
            $$\mathrm{GL}(V)\to \mathbb{C},\ \ g\mapsto \langle\varphi,\varphi_n(g)\rangle$$
            is the zero function. We identify $\mathrm{GL}(V)$ with $\mathrm{GL}_n(\mathbb{C})$ with the fixed basis of $V$, and  $g$ is then the matrix $(g_{i,j})$. So the function
            $$\mathrm{GL}_n(\mathbb{C})\to \mathbb{C},\ \ (g_{i,j})\mapsto \sum_{I,J}a_{I,J}g_{J,I}$$
            is identically zero. For $1\leq i,j\leq n$, we let $x_{i,j}:M_n(\mathbb{C})\to\mathbb{C}$ be the linear function taking the $(i,j)$-entry of a matrix. Since $\mathrm{GL}_n(\mathbb{C})$ is dense in $M_n(\mathbb{C})$, for any $(I,J)\in\Gamma$,
            
            \begin{equation}\label{Eq:AX}
                \sum_{I,J}a_{I,J}x_{J,I}=0,
            \end{equation}
            
            where $x_{J,I}:=x_{j_1,i_1}\cdots x_{j_n,i_n}$ is a polynomial function on $M_n(\mathbb{C})$.
            To show that $\varphi=0$ it remains to show that for any $(I,J)\in\Gamma$, $a_{I,J}=0$.
            \item[(d)] We complete the proof. Since $\varphi\in C(A)$, $a_{\sigma(I),\sigma(J)}=a_{I,J}$. It suffices to show that $a_\gamma=0$ for any $\gamma\in\mathcal{O}$. 
            
            Note that for $(I_1,J_1),\ldots,(I_k,J_k)\in\Gamma$, if for any $1\leq r\neq s\leq k$, $x_{I_r,J_r}\neq x_{I_s,J_s}$ then $x_{I_1,J_1},\ldots,x_{I_k,J_k}$ are linearly independent.
            
            We claim that for $(I,J),(I',J')\in\Gamma$, $x_{I,J}=x_{I',J'}$ if and only if there exists $\sigma\in\mathfrak{S}_n$ such that $I'=\sigma(I)$ and $J'=\sigma(J)$. The ``if'' part is clear from the definition of $x_{I,J}$. Assume that $x_{I,J}=x_{I',J'}$, there must be $1\leq p\leq n$ such that $x_{i_1',j_1'}=x_{i_p,j_p}$, otherwise there exists a matrix $M\in M_n(\mathbb{C})$ such that  $x_{I,J}(M)\neq x_{I',J'}(M)$. Applying this argument to $(i_2',j_2')$ and so on, one obtains a permutation $\sigma\in\mathfrak{S}_n$ such that $I'=\sigma(I)$ and $J'=\sigma(J)$. 
            
            From this claim, we can write the equation \eqref{Eq:AX} into 
            $$\sum_{\gamma\in\mathcal{O}} n_\gamma a_\gamma x_\gamma=0$$
            where $n_\gamma=\#\gamma$ and $x_\gamma=x_{I,J}$ for any $(I,J)\in\gamma$. Again from the claim, $\{x_\gamma\mid\gamma\in\mathcal{O}\}$ is linearly independent, and hence $a_\gamma=0$ for any $\gamma\in\mathcal{O}$.
            \end{enumerate}
        \end{proof}

        \begin{definition}{Partition}{3.33}
            Let $n \in \N$. A tuple of positive integers $\lambda = (\lambda_1,\dots,\lambda_k)$ is called a \textbf{partition} of $n$, if $\lambda_1 \geq \dots \geq \lambda_k$ and $\lambda_1 + \dots + \lambda_k = n$. We let $\wp(n)$ denote the set of all partitions of $n$.
        \end{definition}

        \begin{lemma}{}{3.34}
            There exists a bijection $\Irr(S_n) \leftrightarrow \wp(n)$.
        \end{lemma}

        \begin{corollary}{}{3.35}
            For any $\lambda \in \wp(n)$, $S_\lambda(V)$, is either $\{0\}$ or an irreducible representation of $\GL(V)$.
        \end{corollary}

        \begin{proposition}{}{3.36}
            Let $V \in \rep(S_n)$, then for all $g \in S_n$, $\chi_V(g) \in \Z$.
        \end{proposition}
\chapter{Toolbox: Homological algebra}
    For this chapter, we will leave the semisimple world (chapter $2$ \& $3$). To discover the new world in a safe way, we need new weapons: homological methods.
    
    Our goal is not to develop this huge domain but to see the tip of the iceberg. What we will introduce in this chapter feels like a kid's toolbox, but it will be sufficient for our use in the next chapter where we study hereditary algebras.

    Having mastered the material in this chapter, one can start reading the book of Weibel \cite{Weibel.2010} or the book of Rotman \cite{Rotman.2009}, both have the name "An Introduction to Homological Algebra".
\setcounter{section}{-1}
\section{Prelude - Categories}\label{cat}
    This section is not part of the lecture. I just added it to introduce some basic language of category theory as I thought it might be interesting to "see the real world" - Michael Schlsser.

    When studying Algebra, one can notice a certain pattern:
    We define some (algebraic) structures and then study the maps between objects that preserve these structures.
    The most common examples are groups with group homomorphisms, vector spaces with linear maps and algebras with algebra homomorphisms.
    The prelude aims to generalize this pattern to obtain the definition of a \textbf{category}.
    We will develop some basic notions of category theory.
    Being quite an abstract branch of mathematics we will accompany the definitions with a potpourri of examples
    to gain familiarity with the definitions.
    
\subsection{Defintion and basic examples}
   The definition consists of multiple parts, all of which will be built up in this section.
    First note that we somehow wish to collect realisations of a specific structure and
    the structure-preserving ''maps'' between them.
    Due to set-theoretic issues, we may not require that they will be collected in a set,
    hence we introduce the notion of a \textbf{class}. Every set is a class but not the other way around. 
    Therefore classes that do not constitute a set shall be called \text{proper classes}.
    For our purposes we need not go into detail about the difference between sets and classes,
    it suffices to know most of the operations allowed for sets are compatible with the notion of a class.
    The key point to avoid Russels' paradox is the inability of a class to contain a proper class.
    Now that we have pleased the set theorists, we shall continue with the definition of a category.
    
    Given a structure, we shall denote the class of its realisations as the \textbf{objects} of the category.
    The collection of structure-preserving ''maps'' is denoted by the class of \textbf{morphsims} of the category.

    \begin{example*}{}
        Fix a field $k$ and the $k$-vector space structure.
        The realisations of the $k$-vector space structure (objects) are the vector spaces and
        linear maps constitute the morphisms.
    \end{example*}

    Now we wish to give the morphisms some more structure that resembles the usual behaviour of maps.
    Say we have the following situation:
    \begin{equation*}
        \xymatrix {
            A \ar[r]^{f} & B \ar[r]^g & C
        }
    \end{equation*}
    Wishfully, we would like to compose $f$ and $g$ to a map $g \circ f: A \to C$.
    This composition shall be associative, hence one can write $f \circ g \circ h$ or $fgh$ whenever defined and not worry about parentheses. Furthermore, we wish to have identity maps for each object.
    
    To summarise:

    \begin{definition*}{Category}
        A category $\cC$ consists of a class of \textbf{objects} $\Obj(\cC)$ and
        a class of \textbf{morphisms} $\Mor(\cC)$ between two objects.
        
        For all $A,B,C \in \Obj(\cC)$
        \begin{equation*}
            \begin{split}
               \circ: \Mor_{\cC}(A,B) \times \Mor_{\cC}(B,C) &\rightarrow \Mor_{\cC}(A,C) \\
                (f,g) &\mapsto g \circ f
            \end{split}
        \end{equation*}
        is associative. For every object $A \in \Obj(\cC)$ there is an identity $\id_A \in \Mor_{\cC}(A,A)$:
        \begin{equation*}
           \forall f,g \text{ with } \dom(f) = A, \ \codom(g) = A:  f \circ \id_A = f, \quad \id_A \circ g = g
        \end{equation*}
    \end{definition*}

    Note that given a morphism $f$ its domain written as $\dom(f)$ and
    its codomain written as $\codom(f)$ are uniquely defined,
    hence $\Mor(A,B) \cap \Mor(C,D) = \varnothing$ if $(A,B) \neq (C,D)$.

    Usually, a category is named after its objects. Now we give examples of categories.
    To get oneself acquainted with the definition one should verify that these indeed make up categories.
    
    \textbf{Basic examples:}
        \begin{enumerate}
            \item
                $\cC = \mathrm{Set}$, where the objects are sets and the morphisms are maps.
            \item
                Fix a field $k$, then $\cC = \Vect_k$ is the category of $k$-vector spaces, where the objects are the $k$-vector spaces and the morphisms are linear maps, this is the category mentioned in opening example of this section.
            \item
                Let $R$ be a ring. Then $\tensor[_R]{\Mod}{^{}}$ is the category of left $R$-modules where the objects are left $R$-modules and the morphisms are homomorphisms of left $R$-modules.
            \item
                Let $G = (V,E)$ be a graph. With $\Obj(\cC) = V$ and $\Mor (\cC) = \{\text{paths in } G\}$ we indeed get a category. What goes wrong if $\Mor(\cC) = E$?
            \item
                $\cC = \Ab$ where objects are abelian groups and morphisms are homomorphisms of groups. This is one of the most important categories, as it is in some sense minimal with the property of being quite a tame category.
        \end{enumerate}

    \begin{remark*}{}
        The subscript $\cC$ in $\Mor_\cC(\_,\_)$ is often omitted when the category is clear from the context.
    \end{remark*}
    
    Note that morphisms need not be maps. A standard example of this is the following:

    \begin{example*}{}
        Let $(S,\leq)$ be a poset (partially ordered set). This can be turned into a category via $\Obj(\cC) = S$ and
        
        \begin{equation*}
            \Mor_{\cC}(A,B) = \left\{ \begin{array}{rl}
                (A,B), & \text{if } A \leq B  \\
                \varnothing, & \text{else} 
            \end{array} \right.
        \end{equation*}
        We define a composition
        \begin{equation*}
            (C,D) \circ (A,B) = \left\{ \begin{array}{rl}
                (A,D), & \text{if } B = C  \\
                \varnothing, & \text{else} 
            \end{array}\right.
        \end{equation*}
        Since $\leq$ is transitive, the composition is well-defined.
        We always have an identity map since $\leq$ is reflexive. One can check that the composition is associative.
    \end{example*}
    
\subsection{Functors}\label{functors}
    Realising that we just defined a structure we shall follow the pattern mentioned in the beginning and ask:
    What are the structure-preserving maps between categories?

    One firstly has to ask what kind of structure shall be preserved. Since we only required composition and
    identity to exist, it makes sense to preserve these. Naturally, we arrive at the following definition 

    \begin{definition*}{covariant functor}
        Let $\cC$ and $\cD$ be categories. A \textbf{covariant} functor $\cF: \cC \to \cD$ associates:
        \begin{itemize}
            \item $X \in \Obj(\cC)$ to an object $\cF(X) \in \Obj(\cD)$
            \item $f \in \Mor_\cC(X,Y)$ to a morphism $\cF(f) \in \Mor_\cD(\cF(X),\cF(Y))$
        \end{itemize}
        such that
        \begin{equation*}
            \cF\round{\id_X} = \id_{\cF(X)}, \quad \cF(g \circ f) = \cF(g) \circ \cF(f)
        \end{equation*}
    \end{definition*}

    \begin{definition*}{contravariant functor}
        Let $\cC$ and $\cD$ be two categories. A \textbf{contravariant} functor $\cF: \cC \to \cD$
        is a covariant functor $\cC^\op \to \cD$.
        
        This means
        \begin{equation*}
            \left(A \xrightarrow{f} B\right) \overset{\cF}{\mapsto} \left(\cF(B) \xrightarrow{\cF(f)}\cF(A)\right)
            \text{ and  } \cF(g \circ f) = \cF(f \circ_\op g) = \cF(f) \circ \cF(g)
        \end{equation*}

    \end{definition*}
    
    \begin{example*}{The contravariant Homfunctor}
        Let $R$ be a commutative ring and $\cC = \tensor[_R]{\Mod}{^{}}$ and $X \in \Obj(\cC) $. 
        Then $\Hom(\bullet,X): \cC \to \cC$, with $\Hom(\bullet,X)(Y) = \Hom(Y,X)$ for $Y \in \Obj(\cC)$ (compare to $V \mapsto V^*$).
        For $f \in \Mor(A,B)$,
        \begin{equation*}
            \begin{split}
                \Hom(\bullet,X)(f) = f^*: \Hom(B,X) &\to \Hom(A,X) \\
                \varphi &\mapsto \varphi \circ f  \\
            \end{split}
        \end{equation*}
        \begin{equation*}
            \xymatrix {
                A \ar^{f}[r]\ar_(.4){f^*(\varphi)}[dr] & B\ar^(.4){\varphi}[d] \\
                            & X
            }
        \end{equation*}
        
        $f^*$ is called the \textbf{pullback} of $f$.
    \end{example*}
    
    The dual functor is a special example of the contravariant Homfunctor, namely $\Hom(\bullet,k)$ for a field $k$.
    
    There is a covariant analogon to the contravariant Hom functor, the covariant Hom functor. Here we call $f_*$ the \textbf{pushout} of $f$.
    
    Given a sequence $\dots \xrightarrow{f_{i-1}} A_i \xrightarrow{f_i} A_{i+1} \xrightarrow{^f_{i+1}} \dots$
    in $\cC$ and a covariant functor $\cF: \cC \to \cD$ we apply $\cF$ to the sequence in $\cC$ and
    get a sequence in $\cD$:
        
        \begin{equation*}
            \xymatrix {
                \dots \ar^(.4){\cF(f_{i-1})}[r] & \cF(A_i)\ar^(.4){\cF(f_i)}[r] & \cF(A_{i+1})\ar^(.6){\cF(f_{i+1})}[r] & \dots
            }
        \end{equation*}
        The natural question is: \textit{Given an exact sequence, is the resulting sequence exact?}
        
        In general, the answer is \textbf{no}, as illustrated in the succeeding example.

        \begin{example*}{}
            Take $R=\Z$ and $\cC = \tensor[_\Z]{\Mod}{^{}}$ together with $\cF = \Hom(\Z/2\Z,\bullet)$ a covariant Hom functor, then
            \begin{equation*}
                \xymatrix @C=35pt {
                    0 \ar[r] & \Z \ar[r]^{\cdot 2} & \Z \ar[r]^{\pi} & \Z/2 \Z \ar[r] & 0
                }
            \end{equation*}
            is exact, but applying $\cF$ yields the following sequence which is not exact:
            \begin{equation*}
                \xymatrix @C=18pt {
                    0 \ar[r] & \Hom_\Z(\Z/2\Z,\Z \ar[r]^(.47){(\cdot 2)_*} & {\underbrace{\Hom_\Z(\Z/2\Z,\Z)}_{=\{0\}}} \ar[r]^{\pi_*} & {\underbrace{\Hom_\Z(\Z/2 \Z,\Z/2\Z)}_{ \neq\{0\}}} \ar[r] & 0
                }
            \end{equation*}
        \end{example*}
        One can derive conditions when the $\Hom$-functors are exact. We will do this in a later section.

        We introduce some more language. A covariant functor is called \textbf{right-exact} if it preserves the exactness of sequences of the form
        \begin{equation*}
            \xymatrix {
                A \ar[r] & B \ar[r] & C \ar[r] & 0
            }
        \end{equation*}
        and \textbf{left-exact} if it preserves the exactness of sequences of the form
        \begin{equation*}
            \xymatrix {
                 0 \ar[r] & A \ar[r] & B \ar[r] & C
            }
        \end{equation*}

         When studying the category $R\Mod$, we notice even more structure:
        
        \begin{itemize}
            \item[1.] $\Mor(M,N)$ is an abelian group for $M,N \in R\Mod$ and ''$\circ$'' is bilinear, i.e. $f \circ (g_1 + g_h) = f \circ g_1 + f \circ g_2$ and so on.
            \item[2.] There exist direct sums and a \textbf{zero object} $0$ such that $\id_0$ is the ``$0$'' of the group $ \Mor(0,0)$. 
        \end{itemize}
        
        \begin{definition*}{(pre)additive category}
            We call a category \textbf{preadditive}, if it satisfies \textit{1}.
            A category that satisfies \textit{1} and \textit{2} is called \textbf{additive}.
        \end{definition*}

        \begin{remark*}{}
            We think of the kernel and the cokernel in terms of their universal properties (see LA \cite{LA}),
            so to be precise $i: K \rightarrow M$ is the kernel and $p: N \rightarrow C$ is the cokernel.
        \end{remark*}

        Because of \textit{1.} you can write $\Hom(M,N)$ instead of $\Mor(M,N)$ if $\cC$ is $k$-linear.

        \begin{definition*}{Additive functor}
            Let $\cC,\cD$ be additive categories. A functor $\cF: \cC \to \cD$ is called additive, if $\cF(f + g) = \cF(f) + \cF(g)$ for all $f,g \in \Mor(\cC)$. That is to say
            \begin{equation*}
            \begin{split}
                \cF: \Mor_\cC(A,B) &\to \Mor_\D(\cF A, \cF B) \\
                            f &\mapsto \cF(f)
            \end{split}
            \end{equation*}
            is a homomorphism of groups for all $A,B \in \Obj(\cC)$.
        \end{definition*}

        
        
    \subsection{Adjoint functors}
    
        \begin{definition*}{adjoint functors}
            Let $\cC,\cD$ be two categories and $\cF: \cC \to \cD, \cG: \cD \to \cC$ be two functors. The pair $(\cF,\cG)$ is an \textbf{adjunction of categories} $\cC$ and $\cD$ if for all $X \in \Obj(\cC), Y \in \Obj(\cD)$ there is an isomorphisms of \underline{sets}:
            \begin{equation*}
                \Mor_\cC(X,\cG Y) \cong \Mor_\cD(\cF X,Y)
            \end{equation*}
            Furthermore, $\cF$ is called a \textbf{left adjoint} (functor) to $\cG$ and respectively $\cG$ is called a \textbf{right adjoint} (functor) to $\cF$.
        \end{definition*}
        
        The $\Hom$-functor $\Hom(A,\bullet)$ and the tensor functor $A \tens_R \bullet$ are adjoint, since
        \begin{equation*}
            \Hom(X,\Hom(Y,Z)) \cong \Hom(X \tens Y, Z)
        \end{equation*}
        in the lecture, we called this map Cartan isomorphism \ref{prop:4.5}.
        
        \begin{lemma*}{}
            If $(\cF,\cG)$ is an adjoint pair of functors, then $\cF$ is left-exact if and only if $\cG$ is right-exact.
        \end{lemma*}
        
        Since the $\Hom$-functor is left-exact \ref{prop:4.13}, the tensor functor has to be right-exact. 
\section{Cartan isomorphism and applications}

    One of the main topics of homological algebra is the study of $\Hom$ and $\tens$ (functors) in the setup of bimodules. We fix $A,B,C,D$ to be $K$-algebras.

    \begin{definition}{}{4.1}
        An $A-B$-bimodule $M$ is a left $A$-module and a right $B$-module $M$, such that for any $a \in A, b \in B, m \in M$:
        \begin{equation*}
            a.(m.b) = (a.m).b
        \end{equation*}
        We let $\tensor[_A]{\Mod}{_B}$ denote the set of all $A-B$-bimodules. An $A-A$-bimodule will be called an $A$-bimodule. Similarly, we will use the notation $\tensor[_A]{\Mod}{^{}}$ and $\Mod_B$.
    \end{definition}

    \begin{example}{}{4.2}
        \begin{enumerate}
            \item $A$ is an $A$-bimodule when we set $a.b.c = abc$ for all $a,b,c \in A$.
            \item Let $\varphi: A \to B$ be a $K$-alebra homomorphism. Then $B$ is an $A$-bimodule. For $a_1,a_2 \in A$ and $b \in B$, set $a_1 . b . a_2 = \varphi(a_1) b \varphi(a_2)$
        \end{enumerate}
    \end{example}

    \begin{definition}{}{4.3}
        Homomorphisms of $A-B$-bimodules are homomorphisms of both left $A$-modules and right $B$-modules. For $M,N \in \tensor[_A]{\Mod}{_B}$, we let $\Hom_A(M,N)_B$ denote the $K$-vector space of homomorphisms of $A-B$-bimodules.
    \end{definition}

    Similar to the tensor product of vector spaces, we can define the tensor product of modules.

    \begin{definition}{}{}
        See \ref{non-commTP}.
    \end{definition}

    The tensor product $\tens_A$ will kill the right $A$-module structure on $M$ and the left $A$-module structure on $N$. Hence $M \tens_A N$ is just a $K$-vector space. If $M \in \tensor[_A]{\Mod}{_B}$ and $N \in \tensor[_B]{\Mod}{_C}$, then $M \tens_A N \in \tensor[_A]{\Mod}{_C}$ where for $a \in A, c \in C$ and $m \tens_B n \in M \tens_B N$ (via the universal property of the tensor product,) we set:
    \begin{equation*}
        a.(m \tens n).c := (a.m) \tens (n.c)
    \end{equation*}

    \begin{exercise}{}{19}
        \begin{enumerate}
            \item Show that for $M \in \tensor[_A]{\Mod}{^{}}$ the map
            \begin{equation*}
            \begin{split}
                A \tens_A M &\to M \\
                a \tens M &\mapsto a.m
            \end{split}
            \end{equation*}
            is an isomorphism of $A$-modules.
            \item State and prove the universal property of $M \tens_A N$
            \item Show that for $M \in \tensor[_A]{\Mod}{_B}, N \in \tensor[_B]{\Mod}{_C}, P \in \tensor[_C]{\Mod}{_D}$, the map
            \begin{equation*}
            \begin{split}
                \round{M \tens_B N} \tens_C P &\to M \tens_B \round{N \tens_C P} \\
                \round{m \tens n} \tens p &\mapsto m \tens \round{n \tens p}
            \end{split}
            \end{equation*}
            is an isomorphism of $A$-$D$-bimodules.
            \item Show that $\End_A(A)_A \cong Z(A)$, where $Z(A)$ is the centre of $A$.
        \end{enumerate}
    \end{exercise}

    Next, we move to $\Hom$. Let $M \in \tensor[_A]{\Mod}{_B}$ and $N \in \tensor[_A]{\Mod}{_C}$. Then $\Hom_A(M,N)$ is a $B$-$C$-bimdoule via

    \begin{equation*}
        \spitz{b.f.c,m} := \underbrace{\spitz{f,m.b}}_{\in N}.c 
    \end{equation*}

    for all $f \in \Hom_A(M,N), b \in B$ and $c \in C$.

    One of the most important relations between $\Hom$ and $\tens$ is the following Cartan isomorphism.

    \begin{proposition}{}{4.5}
        Let $X \in \tensor[_A]{\Mod}{^{}}, Y \in \tensor[_B]{\Mod}{^{}}$ and $M \in \tensor[_A]{\Mod}{_B}$. There exists an isomorphism of $K$-vector spaces
        \begin{equation*}
        \begin{split}
            \Hom_A(M \tens_B Y, X) &\xrightarrow{\cong} \Hom_B(Y,\Hom_A(M,X)) \\
            \varphi &\mapsto (y \mapsto (m \mapsto \varphi(m \tens y)))
        \end{split}
        \end{equation*}
    \end{proposition}
    In the language of category theory: The functors $\Hom_A(M,\bullet)$ and $M \tens_B \bullet$ are adjoint.

    \begin{proof}
        The inverse map is given by
        \begin{equation*}
            \psi \mapsto (m \tens y \mapsto \psi(y)(m))
        \end{equation*}
        it is straightforward to verify that they are inverse to each other.
    \end{proof}
    In the next week, we apply this construction to study the induction and restriction of representations.
    
\separline{week 12}
    \textbf{General setup:}
        Let $A$ be a $K$-algebra and $B \subseteq A$ be a subalgebra. We look at $A \in \tensor[_A]{\Mod}{_B}$. The right $B$-module structure comes from the regular right $A$-module structure on $A$. Let $X \in \tensor[_A]{\Mod}{^{}}, Y \in \tensor[_B]{\Mod}{^{}}$. We define the induction and restriction as follows:
        \begin{equation*}
        \begin{split}
            \Ind_B^A(Y) := A \tens_B Y \quad \text{it is a $A$-module} \\
            \Res_B^A(X) := \Hom_A(A,X) \quad \text{it is a $B$-module}
        \end{split}
        \end{equation*}
    
        In this setup, the Cartan isomorphism with $M = A \in \tensor[_A]{\Mod}{_B}$ gives:
        \begin{equation*}
            \Hom_A(\Ind_B^A(Y),X) \cong \Hom_B(Y,\Res_B^A(X))
        \end{equation*}

    \textbf{Special case:}
        We consider the case where $H \subseteq G$ is a subgroup of the finite group $G$, $X \in \rep G$ and $Y \in \rep H$. In this setup $\Ind_H^G(Y) \in \rep G$ is called the induced representation of $Y$ and $\Res_H^G(X) \in \rep H$ is called the restricted representation of $X$.

        Note that the restriction is given by the inclusion $H \hookrightarrow G \to \GL(X)$. The reason for this is the isomorphism of $B$-modules
        \begin{equation*}
        \begin{split}
            \Hom_A(A,X) &\to X \\
            \varphi &\mapsto \varphi(1)
        \end{split}
        \end{equation*}
        The above isomorphism gives:
        \begin{corollary}{Frobenius reciprocity}{4.6}
            With the notation from above, we have:
            \begin{equation*}
                \Hom_G(\Ind_H^G(Y),X) \cong \Hom_H(Y,\Res_H^G(X))
            \end{equation*}
            as $H$-reps.
        \end{corollary}

        \begin{proof}[Proof of corollary \ref{cor:4.6}]
            Set $A = \C[G]$ and $B = \C[H]$ in propositon \ref{prop:4.5}. Since there is an equivalence of categories $\rep G$ and $ \C[G]-\Mod$ (each $G$ representation corresponds $1:1$ to a $\C[G]$-module and the morphism of $G$-reps are exactly the $\C[G]$-module homomorphism) we are done.
        \end{proof}

        We explain it 9in a numerical way using characters, where the name "reciprocity" is clear. We let $\spitz{-,-}_G$ and $\spitz{-,-}_H$ be the hermitian forms on $G$ and $H$ respectively.
        
        Recall that $\spitz{\chi_V,\chi_W}_G = \dim \Hom_G(V,W)$. The Forbenius reciporcity implies for $S \in \Irr(G)$ and $T \in \Irr(H)$ that:
        \begin{equation*}
            \spitz{\chi_{\Res_H^G(S)},\chi_T}_H = \spitz{\chi_S,\chi_{\Ind_H^G(T)}}_G
        \end{equation*}
        which means that the multiplicity of $S$ in $\Ind_H^G(T)$ is the same as the multiplicity of $T$ in $\Res_H^G(S)$.

        If we take $G=S_n$, $H = S_{n-1} \in S_n$, the embedding of $S_{n-1}$ in $S_n$ from the inclusion $[n-1] \hookrightarrow [n]$, then we get
        \begin{theorem}{}{4.7}
            For $V \in \Irr(S_n)$, $\Res_H^G(V) \in \rep(S_{n-1})$ is multiplicity free.

            That is to say for all $S \in \Irr(S_{n-1}): \spitz{\chi_{\Res_H^G(V)},\chi_S}_H \leq 1$.
        \end{theorem}

        \begin{proof}{Proof omitted}
        \end{proof}

        This theorem is the starting point of the Okounkov-Vershik approach to the representation theory of the symmetric group. For details, see the paper \cite{VershikOkounkov.2005}

\section{Chain complexes and projective resolutions}
    We are going to the heart of homological algebra: (co)chain complexes and their (co)homology. Let $A$ be a $K$-algebra.

    \subsection{Chain complexes and their homology}
        \begin{definition}{}{}
            \begin{enumerate}
                \item A chain complex $(C_\bullet, d^C_\bullet)$ of $A$-modules consists of a collection of $A$-modules $\{ C_n \ | \  n \in \Z \}$ and $A$-module homomorphisms $d_i^C: C_i \to C_{i-1}$ such that for any $i$, $d_{i-1}^C \circ d_i^C = 0$. We usually write it in a diagram:
                \begin{equation*}
                \xymatrix {
                    \dots \ar[r]^{d_{i+1}^C} & C_i \ar[r]^{d_{i}^C} & C_{i-1} \ar[r]^{d_{i-1}^C} & \dots
                }
                \end{equation*}
                Elements in $C_i$ are called chains of degree $i$, and the maps $d_i^C$ are called differentials, or boundary maps.
                \item A cochain complex $(C^\bullet,d^\bullet_C)$ of $A$-modules consists of a collection of $A$-mods $\{C^n \ | \ n \in \Z \}$ and $A$-module homomorphisms $d_C^i: C^i \to C^{i+1}$ such that for any $i$, $d_C^{i+1} \circ d_C^{i} = 0$. Again we can formulate this by means of a diagram
                \begin{equation*}
                \xymatrix {
                    \dots \ar[r]^{d^{i-2}_C} & C^{i-1} \ar[r]^{d^{i-1}_C} & C^i \ar[r]^{d^{i}_C} & \dots
                }
                \end{equation*}
                Elements in $C^i$ are called cochains of degree $i$ and the maps $d_C^i$ are called differentials.
            \end{enumerate}
        \end{definition}

        This notion of chain and cochain complexes comes from algebraic topology. Homological algebra was initially developed to tackle problems from topology.

        \begin{remark}{}{4.9}
            \begin{enumerate}
                \item If $(C_\bullet,d_\bullet^C)$ is a chain complex, then letting $C^n:=C_{-n}$ and $d_C^i:=d_{-i}^C$ defines a cochain complex $(C^\bullet,d^\bullet_C)$ (verify this!). So in some sense chain complexes and cochain complexes are the same thing.
                \item A chain complex with only finitely many non-zero terms is called bounded.
            \end{enumerate}
        \end{remark}

        In the following, we will mainly focus on chain complexes. You should write down the cochain versions as a long-term exercise.

        Notice that from the (only nontrivial) condition $d_{i-1}^C \circ d_i^C = 0$ it follows that $\im d_i^C \subseteq \ker d_{i-1}^C$ is a submodule.

        \begin{definition}{}{4.10}
            \begin{enumerate}
                \item We denote $Z_i(C_\bullet):= \ker d_i^C \subseteq C_i, B_i(C_\bullet):=\im d_{i+1}^C \subseteq C_i$ and $H_i(C_\bullet):= Z_i(C_\bullet)/B_i(C_\bullet)$. Elements in $Z_i(C_\bullet)$ are called $i$-\textbf{cycles} and elements in $B_i(C_\bullet)$ are called $i$-\textbf{boundaries}. The $A$-module $H_i(C_\bullet)$ is called the $i$-th \textbf{homology group} of the complex $C_\bullet$. For $z \in Z_i(C_\bullet)$ we denote its class in $H_i(C_\bullet)$ by $[z]$.
                
                \item If $H_i(C_\bullet) = \{0\}$ we say that the complex is \textbf{exact} at degree $i$.
                
                \item The complex $C_\bullet$ is called acyclic if it is exact at all degrees. In this case, $C_\bullet$ is usually called an \textbf{exact sequence}.
    
                \item An exact sequence of $A$-modules and homomorphisms $\xymatrix@C=15pt{0 \ar[r] & X \ar[r] & Y \ar[r] & Z \ar[r] & 0}$ is called a short exact sequence (s.e.s. for short).
            \end{enumerate}
        \end{definition}

        \begin{exercise}{}{20}
            Show that
            \begin{equation*}
                \xymatrix@C=15pt {
                    0 \ar[r] & X \ar[r]^f & Y \ar[r]^g & Z \ar[r] & 0
                }
            \end{equation*}
            is a s.e.s. if and only if $f$ is injective, $g$ is surjective and $\im f = \ker g$.
        \end{exercise}
        \begin{proof}
            We have seen this in Linear Algebra 1 \cite{LA}. Just note that $H_i(C_\bullet) = \{0\}$ if and only if $\ker d_i^C =: Z_i(C_\bullet) = B_i(C_\bullet) := \im d_{i+1}^C$, hence our definition of an exact sequence is equivalent to the one from Linear Algebra.
        \end{proof}

        \begin{definition}{}{4.11}
            A homomorphism $f_\bullet$ between to chain complexes $(C_\bullet,d_\bullet^C)$ and $(D_\bullet,d_\bullet^D)$ is a sequence of $A$-module homomorphisms $f_n: C_n \to D_n$ such that for any $i$, $f_{i-1} \circ d_i^C = d_i^D \circ f_i$. By means of a diagram:
            \begin{equation*}
            \xymatrix {
                \dots \ar[r]^{d_{i+1}^C} & C_i \ar[r]^{d_i^C} \ar[d]_{f_i} & C_{i-1} \ar[r]^{d_{i-1}^C} \ar[d]^{f_{i-1}} & \dots \\
                \dots \ar[r]^{d_{i+1}^D} & D_i \ar[r]^{d_{i}^D} \ar@{}[ur]|{\circlearrowleft} & D_{i-1} \ar[r]^{d_{i-1}^D}& \dots 
            }
            \end{equation*}
        \end{definition}
        Such a homomorphism is usually called a \textbf{chain map}. Homomorphisms between chain complexes induce homomorphisms between homology groups.

        \begin{lemma}{}{4.12}
            Let $f_\bullet: C_\bullet \to D_\bullet$ and $g_\bullet: D_\bullet \to E_\bullet$ be two chain maps.
            \begin{enumerate}
                \item For all $i \in \Z$, $f_\bullet$ induces a homomorphism of $A$-modules:
                    \begin{equation*}
                        H_i(f_\bullet): H_i(C_\bullet) \to H_i(D_\bullet)
                    \end{equation*}
                \item For any $i \in Z$, $H_i(g_\bullet \circ f_\bullet) = H_i(g_\bullet) \circ H_i(f_\bullet)$
            \end{enumerate}
        \end{lemma}

        \begin{remark*}{abstract nonsense -F}
            In the language of category theory, this means that $H_i(\bullet)$ is a covariant functor from the category of chain complexes (check that this is a category!) to the category of $A$-modules.
        \end{remark*}

        \begin{proof}[Proof of Lemma \ref{lem:4.12}]
            For \textit{(1)} we need to define an $A$-module homomorphism
            \begin{equation*}
                Z_i(C_\bullet)/B_i(C_\bullet) \to Z_i(D_\bullet)/B_i(D_\bullet)
            \end{equation*}
            We start by defining a map from $Z_i(C_\bullet) \to Z_i(D_\bullet)$. For this, take $z_i \in C_i$ with $d_i^C(z_i) = 0$. We show that $f_i(z_i) \in Z_i(D_\bullet)$, i.e. $d_i^D(f_i(z_i)) = 0$. Since $f_\bullet$ is a homomorphisms of chain complexes, $d_i^D(f_i(z_i)) = f_{i-1}(d_i^C(z_i)) = 0$. Or by means of a diagram:
            \begin{equation*}
            \xymatrix {
                C_i \ar[r]^{d_i^C} \ar[d]_{f_i} & C_{i-1} \ar[d]^{f_{i-1}}\\
                D_i \ar[r]_{d_{i}^D} \ar@{}[ur]|{\circlearrowleft} & D_{i-1}
            }
            \end{equation*}

            Now by composing with the canonical surjection $\pi: Z_i(D_\bullet) \to Z_i(D_\bullet)/B_i(D_\bullet)$, we obtain an $A$-module homomorphsim $Z_i(C_\bullet) \to H_i(D_\bullet)$.

            It suffices to show that the image of $B_i(C_\bullet) \subseteq Z_i(C_\bullet)$ under this map is zero. We take $b_i \in B_i(C_\bullet)$, then there exists $c_{i+1} \in C_{i+1}$ such that $d_{i+1}^C(c_{i+1}) = b_i$. Then
            \begin{equation*}
                f_i(b_i) = f_i(d_{i+1}^C(c_{i+1})) = d_{i+1}^D(f_{i+1}(c_{i+1})) \in B_i(D_\bullet)
            \end{equation*} and therefore $\eckig{f_i(b_i)} = 0$ hence we obtain an $A$-module homomorphism
            \begin{equation*}
            \begin{split}
                H_i(f_\bullet): H_i(C_\bullet) &\to H_i(D_\bullet) \\ [z_i] &\mapsto [f_i(z_i)] 
            \end{split}
            \end{equation*}
            The proof of the second statement is similar and we leave it as an exercise. 
        \end{proof}

        Similarily for a cochain complex $(C^\bullet,d_C^\bullet)$ we define:
        \begin{itemize}
            \item $Z^i(C^\bullet):= \ker d_C^i$, the \textbf{cocylces} of degree $i$ 
            \item $B^i(C^\bullet):= \im d_C^{i-1}$, the \textbf{coboundaries} of degree $i$
            \item $H^i(C^\bullet):= Z^i(C^\bullet)/B^i(C^\bullet)$, the $i$-th \textbf{cohomology group} of the complex $C^\bullet, d_C^\bullet$
        \end{itemize}

        Similar results as in Lemma \ref{lem:4.12} hold for the cohomology groups: a homomorphism $f^\bullet: C^\bullet \to D^\bullet$ of two cochain complexes (think about how to define it!) induces $H^i(f^\bullet): H^i(C^\bullet) \to H^i(D^\bullet)$.

    \subsection{Projective modules}
        The easiest $A$-modules are the free $A$-modules which are (most of the time infinite) direct sums of $A$. Free modules are rare $A$-modules. Those $A$-modules, which are not that rare but still easy are the projective modules. They are categorical notions.

        \begin{proposition}{}{4.13}
            Let $0 \to X \xrightarrow{f} Y \xrightarrow{g} Z \to 0$ be a s.e.s. of $A$-modules and $M$ an $A$-module. The following sequences of $K$-vector spaces are exact:
            \begin{enumerate}
                \item
                    \begin{equation*}
                    \xymatrix{
                        0 \ar[r] & \Hom_A(M,X) \ar[r]^{f_*} & \Hom_A(M,Y) \ar[r]^{g_*}  & \Hom_A(M,Z)
                    }
                    \end{equation*}
                \item
                    \begin{equation*}
                    \xymatrix {
                        0 \ar[r]& \Hom_A(Z,M) \ar[r]^{g^*} & \Hom_A(Y,M) \ar[r]^{f^*} & \Hom_A(X,M)
                    }
                    \end{equation*}
            \end{enumerate}
            where $f_*(\varphi) = f \circ \varphi$, analogously with $g_*$. We have seen $f^*$, $g^*$ in \ref{functors} already.
        \end{proposition}

        \begin{proof}
            See the following two propositions.
        \end{proof}

        We say that the covariant Homfunctor is \textbf{left-exact} since it preserves injective maps.

        Since the contravariant Homfucntor sends surjective maps to injective maps, it is also \textbf{left-exact}.

        \begin{proposition*}{Stronger version of proposition \ref{prop:4.13} part 1 -F}
            Let $R$ be a ring. Then the following are equivalent.
            \begin{enumerate}
                \item The sequence $0 \to L \xrightarrow{f} M \xrightarrow{g} N$ is exact.
                \item For all $X \in R\Mod:
                0 \to \Hom_R(X,L) \xrightarrow{f_*} \Hom_R(X,M) \xrightarrow{g_*} \Hom_R(X,N)$ is exact.
            \end{enumerate}
        \end{proposition*}

        \begin{proof}
            For both directions we have to show $2$ parts:
        \begin{enumerate}
            \item $f$ (or $f_*$) is injective
            \item $\Img f =\Ker g$, $\Img f_* = \Ker g_*$
        \end{enumerate}
        ''$\Rightarrow$'': Take $X \in R\Mod$.
        Let us assume that $\varphi \in \Ker f_* \subseteq \Hom(X,L)$.
        That means $f \circ \varphi = 0 \in \Hom(X,M)$. But since $f$ is injective, $\varphi = 0 \in \Hom(X,L)$.

        Take $\varphi \in \Img f_*$, then we can write $\varphi = f_* (\tau) = f \circ \tau$, hence
        $g_*(\varphi) = g(f(\tau)) = 0$, hence $\Img f_* \subseteq \Ker g_*$.
        For the other inclusion take $v \in \Ker g_* \subseteq \Hom(X,M)$, hence $g \circ v = 0$.
        First note that since the sequence $L \xrightarrow{f} M \xrightarrow{g} N$ is exact, $f$ satisfies the
        universal property of the kernel. But so does $v$, hence $v$ factors through $f$, id est there exists a unique $u \in \Hom(X,L)$, such that
        \begin{equation*}
            \xymatrix {
                X \ar[r]^{v} \ar@{.>}[dr]_{u} & M\ar[r]^{g} & N \\
                 & L \ar[u]^{f} \ar[ur]_{0}
            }
        \end{equation*}
        commutes. Hence $v = f \circ u = f_*(u) \in \Img f_*$.

        ''$\Leftarrow$'':
        For the injectivity of $f$, take $X = \Ker f$ and $i: \Ker f \hookrightarrow L$, then
        $f_*(i) = 0$, since $f_*$ is injetive, $i=0$, since $i$ is an embedding $\Ker f = \{0\}$.

        For $\Img f \subseteq \Ker g$ take $X = L$, we have
        \begin{equation*}
            0 = (g_* \circ f_*)(\id_L) = g \circ f
        \end{equation*}
        On the other hand, take $X = \Ker g$ and $i: \Ker g \hookrightarrow M$. We have:
        \begin{equation*}
            \begin{split}
                g(i) = 0 &\implies i \in \Ker g_* = \Img f_* \implies \exists \tau \in \Hom(X,L): i = f_*(\tau) = f(\tau) \\
                        &\implies \Ker g = \Img i \subseteq \Img f
            \end{split}
        \end{equation*}
        \end{proof}
        
        The analogous statements hold for the contravariant Hom functor:
        \begin{proposition*}{Stronger version of \ref{prop:4.13} part 2 -F}
            Let $R$ be a ring. The following are equivalent:
            \begin{itemize}
                \item The sequence $L \xrightarrow{f} M \xrightarrow{g} N \to 0$ is exact.
                \item For all $X \in R \Mod: 0 \to \Hom_R(N,X) \xrightarrow{g^*} \Hom_R(M,X) \xrightarrow{f^*} \Hom_R(L,X)$ is exact.
            \end{itemize}
        \end{proposition*}

        \begin{proof}
        ''$\Rightarrow$'': Take any $X \in R\Mod$. Now let us assume that $g^*(\tau) = 0$ for some $\tau \in \Hom(N,X)$,
        then $\tau \circ g = 0$, because $g$ is surjective, $\tau = 0$, hence $g^*$ is injective.

        Take $\tau \in \Hom(N,X)$, then $f^* \circ g^* (\tau) = f^*(\tau \circ g) = \tau \circ g \circ f = 0$.
        This means $\Img g^* \subseteq \Ker f^*$.

        For the other inclusion, take $\varphi \in \Ker f^* \subseteq \Hom(M,X)$. That means $\varphi \circ f = 0$,
        hence $\varphi$ satisfies the universal property of the cokernel of $f$:
        \begin{equation*}
        \xymatrix {
            X & M \ar[l]_{\varphi} \ar[d]^{g} & \ar[l]_{f} L \\
            & N \ar@{.>}[ul]^{\exists ! \psi} & 
        }
        \end{equation*}
        Hence $\varphi$ factors through $g$, which means $\varphi = \psi \circ g = g^*(\psi)$.
        This means $\ker f^* \subseteq \Img g^* $.

        ''$\Leftarrow$'': Take $X = \Coker g$, then $\pi: N \twoheadrightarrow N/\Img g = \Coker g$.
        We have $g^*(\pi) = \pi \circ g = 0$, hence $\pi \in \Ker g^* $.
        Because $g^*$ is injective, $\pi = 0 \implies \Img g = N$.

        Take $X = N$, then $f^* \circ g^* (\id_N) = f^* (g) = g \circ f = 0$,
        which means $\Img f \subseteq \Ker g$.

        For the other inclusion take $X = \Coker f = M/\Img f$. Take $\pi: M \twoheadrightarrow M/\Img f \in \Hom(M,\Coker f)$.
        Clearly $f^* \circ \pi = \pi \circ f = 0 \implies \pi \in \Ker f^* = \Img g^*$. Hence there exsits a $\tau \in \Hom(N,\Coker f): \pi = g^* \circ \tau = \tau \circ g$.
        Now $\Ker g \subseteq \Ker \pi  = \Img f $. 
    \end{proof}

    \begin{definition}{Projective Module}{4.14}
        An $A$-module $P$ is called projective, if $\Hom_A(P,-)$ sends exact sequences to exact sequences. I.e. the covariant Homfunctor $\Hom_A(P,-)$ is \textbf{exact}.
    \end{definition}

    We expand this definition. According to the above proposition \ref{prop:4.13}, a module $P$ is projective if and only if for any surjective map $g: Y \to Z$, $g_*: \Hom_A(P,Y) \to \Hom_A(P,Z)$ is surjective. This is equivalent to saying that $\varphi: P \to Z$ factors through $g$, i.e. for any $\varphi: P \to Z$ there exists $\rho:P \to Z$ such that $g \circ \rho = \varphi$.

    \begin{equation}\tag{$P$}\label{proj}
    \xymatrix {
        & P \ar[d]^{\forall \varphi} \ar[dl]_{\exists \rho}^{\circlearrowleft} \\
        Y \ar[r]_g & Z \ar[r] & 0
    }
    \end{equation}

    In the notes, there is an $\exists!$, but this is incorrect, for example, take $P$ to be free and $g$ to have a non-trivial kernel, then the image of $\rho$ in $Y$ can be chosen up to an element of the kernel of $g$, hence $\rho$ need not be unique.

    \underline{Summary}: Maps from a projective module to the target of a surjective map can be lifted (along that map?) to the source.

    \begin{example}{}{}
        Free $A$-modules are projective. Let $F$ be a free $A$-module with basis $(f_i)_{i \in I}$ and $g: Y \to Z$ a surjective $A$-module homomorphism. Given $\varphi: F \to Z$, we denote $z_i:= \varphi(f_i)$ and $y_i \in Y$ be a preimage of $z_i$ (axiom of choice!), for any $i \in I$. Then $\rho: F \to Y, f_i \mapsto y_i$ is the lifiting of $\varphi$ with $g \circ \rho = \varphi$. Hence $F$ is projective.
    \end{example}

    We will give different characterizations of projective modules. For this, we need the notion of a split exact sequence.

    \begin{definition}{Split exact sequence}{4.16}
        A s.e.s. (short exact, not split exact!) $0 \to X \xrightarrow{f} Y \xrightarrow{g} Z \to 0$ of $A$-modules and homomorphisms is called split, if there exists $r \in \Hom_R(Z,Y)$ such that $g \circ r = \id_Z$.
    \end{definition}

    Note that a right-inverse of $g$ always exists, but if there is one, that is also a homomorphism of $R$-modules (which is not always the case), only then the sequence is split. This also means that in the category $\mathrm{Set}$ every short exact sequence splits.

    \begin{lemma}{}{4.17}
        If the s.e.s. $0 \to X \xrightarrow{f} Y \xrightarrow{g} Z \to 0$ splits, then $Y \cong X \oplus Z$ as $A$-modules.
    \end{lemma}
    Actually, the converse is also true. We will see this later, though we could have also proven it right here.
    \begin{proof}[Proof of Lemma \ref{lem:4.17}]
        This means there exists $h \in \Hom_A(Z,Y)$ such that $g \circ h = \id_Z$. Now write any $y \in Y$ as $y = y - h \circ g (y) + h \circ g (y)$. We have $y \in h \circ g (y) \in \ker g$ and $g(y) \in \im h$. Hence $Y = \ker g + \im h$. Now let us show that the sum is direct. Take $y \in \ker g \cap \im h$. Then we can write $y = h(z)$ for some $z \in Z$. But since $y \in \ker g$, we have:
        \begin{equation*}
            0 = g(y) = g \circ h (z) = z
        \end{equation*}
        hence $y = h(z) = 0$. Thus $Y = \ker g \oplus \im h$. Now $\ker g = \im f \cong X$ and $\im h \cong Z$, since $h$ is injective. So we get $Y \cong X \oplus Z$ as $A$-modules.
    \end{proof}

    \begin{example}{}{4.18}
        Given two $A$-modules $X$ and $Z$, the s.e.s.
        \begin{equation*}
            \xymatrix {
                0 \ar[r] & X \ar[r]^(.4){\iota_X} & X \oplus Z \ar[r]^(.6){\pi_Z} & Z \ar[r]& 0
            }
        \end{equation*}
        is split, since $\iota_Z \in \Hom_A(Z,Z \oplus X)$ and $\pi_Z \circ \iota_Z = \id_Z$
    \end{example}

    \begin{proposition}{}{4.19}
        Let $P$ be an $A$-module, then the following are equivalent.
        \begin{enumerate}
            \item $P$ is projective
            \item Any s.e.s. $0 \to X \xrightarrow{f} Y \xrightarrow{g} P \to 0$ splits
            \item $P$ is isomorphic to a direct summand of a free $A$-module. That is to say, there exists a free $A$-module $F$ and an $A$-module $M$ such that $F \cong P \oplus M$ as $A$-modules.
        \end{enumerate}
    \end{proposition}
    Some parts of the following proof differ from the one in the notes.
    \begin{proof}[Proof of Proposition \ref{prop:4.19}]
        
        $(1) \Rightarrow (2):$ Choosing $Z = P$ in \eqref{proj}, $\id_P$ factors through $g$, hence there is a $h \in \Hom_A(P,Y)$ such that $g \circ h = \id_P$, this means the sequence $0 \to X \xrightarrow{f} Y \xrightarrow{g} P \to 0$ splits.

        $(2) \Rightarrow (3):$ Let $(p_i)_{i \in I}$ be a set of generators for $P$, then we have a short exact sequence
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & \ker \pi \ar[r] & A^{(I)} \ar[r]^{\pi} & P \ar[r] & 0
        }
        \end{equation*}
        where $A^{(I)}:=\{f: I \to A \ | \ \# \supp f < \infty \}$ and $\pi(e_i) := p_i$. By $(2)$, $P \oplus \ker \pi \cong A^{(I)}$. Since $A^{(I)}$ is a free $A$-module, we are done.

        $(3) \Rightarrow (1):$ We show the property \eqref{proj}. So let $g: Y \to Z$ be an epimorphism and $\varphi: P \to Z$ a homomorphism of $A$-modules. Since $P$ is a direct summand of a free $A$-module $F$, there are two maps $\iota_P: P \to F$ and $\pi_P: F \to P$, such that $\pi_P \circ \iota_P = \id_P$. Consider the diagram
        \begin{equation*}
        \xymatrix {
            & F \ar[d]_{\pi_P} \\
            & P \ar@/_1.0pc/@{.>}[u]_{\iota_P} \ar[d]^{\varphi} \\
            Y \ar[r]^g & Z \ar[r] & 0
        }
        \end{equation*}
        Since $F$ is projective, there is a map $\psi: F \to Y$ such that $g \circ \psi = \varphi \circ \pi_P$. Now we set $\rho := \psi \circ \iota_P$ (this is wrong in the notes). We get:
        \begin{equation*}
            g \circ \rho = g \circ \psi \circ \iota_P = \varphi \circ \pi_P \circ \iota_P = \varphi \circ \id_P = \varphi
        \end{equation*}
        or by means of a diagram:
        \begin{equation*}
        \xymatrix {
            & F \ar[d]_{\pi_P} \ar@/_0.79pc/[ddl]_{\psi} \\
            & P \ar@/_1.0pc/@{.>}[u]_{\iota_P} \ar[d]^{\varphi} \ar@{.>}[dl]_{\rho} \\
            Y \ar[r]^g & Z \ar[r] & 0
        }
        \end{equation*}
    \end{proof}

    \begin{exercise}{}{21}
        Let $A$ be a finite-dimensional semisimple $\C$-algebra and $M$ an $A$-module. Then $M$ is a projective $A$-module.
    \end{exercise}

    \begin{proof}
        Let $m_i$ for $i \in I$ be a set of generators of $M$. Consider the map $\varphi: A^{(I)} \to M, e_i \mapsto m_i$. This is clearly surjective. Hence we have an exact sequence
        \begin{equation*}
        \xymatrix {
            0 \ar[r]& \ker \varphi \ar[r]^\iota & A^{(I)} \ar[r]^\varphi& M \ar[r]& 0
        }
        \end{equation*}
        Since $A^{(I)}$ is a semisimple $A$-module, $M \subseteq A^{(I)}$ has a complement $N$. In particular, $A^{(I)} \cong M \oplus N$, since $A^I$ is free, $M$ is projective by Proposition \ref{prop:4.19}.
    \end{proof}

\subsection{Projective resolutions}
    The idea is to use projective modules and maps between them to study any arbitrary $A$-module and homomorphisms  between $A$-modules.

    \begin{definition}{}{4.20}
        Let $M$ be an $A$-module. A projective resolution of $M$ is an exact sequence
        \begin{equation*}
            \xymatrix{
                \dots \ar[r] & P_2 \ar[r]^{d_2} & P_1 \ar[r]^{d_1} & P_0 
            }
        \end{equation*}
        of projective modules and an $A$-module homomorphism $\e: P_0 \to M$ such that the following sequence is exact:
        \begin{equation*}
        \xymatrix {
            \dots \ar[r] & P_2 \ar[r]^{d_2} & P_1 \ar[r]^{d_1} & P_0 \ar[r]^{\e} & M \ar[r] & 0
        }
        \end{equation*}
        We usually denote it by $P_\bullet \xrightarrow{\e} M \to 0$ and call $\e$ the augmentation map.
    \end{definition}

    First, we deal with existence.

    \begin{lemma}{}{4.21}
        Let $M$ be an $A$-module. There exists a projective resolution of $M$.
    \end{lemma}

    \begin{proof}
        We have seen in Cobra \cite{Cobra} that free resolutions exist. Since free modules are projective, we are done.
    \end{proof}

    Next we deal with the projective resolution of a $A$-module homomorphism $\varphi: M \to N$.

    \begin{proposition}{}{4.22}
        Let $P_\bullet \xrightarrow{\e} M \to 0$ be a projective resolution of $M$ and $Q_\bullet \xrightarrow{\eta} N \to 0$ a long exact sequence. Given a morphism $\varphi: M \to N$, there exists a homomorphism of chain complexes $\alpha_\bullet: P_\bullet \to Q_\bullet$ such that the following diagram (has exact rows) and commutes:

        \begin{equation*}
            \xymatrix {
                \dots \ar[r] & P_2 \ar[r] \ar[d]_{\alpha_2} & P_1 \ar[r] \ar[d]_{\alpha_1} & P_0 \ar[r]^{\e} \ar[d]_{\alpha_0}& M \ar[r] \ar[d]_{\varphi} & 0 \\
                \dots \ar[r] & Q_2 \ar[r] \ar@{}[ur]|{\circlearrowleft} & Q_1 \ar[r] \ar@{}[ur]|{\circlearrowleft} & Q_0 \ar[r]^{\eta} \ar@{}[ur]|{\circlearrowleft} & N \ar[r] & 0
            }
        \end{equation*}
    \end{proposition}

    \begin{proof}
        We start with the following diagram and build up the ladder:
        \begin{equation*}
        \xymatrix {
            P_1 \ar[r] & P_0 \ar[r]^{\e} \ar[dr]^{\varphi \e} \ar@{.>}[d]_{\alpha_0} & M \ar[r] \ar[d]^{\varphi} & 0 \\
            Q_1 \ar[r] & Q_0 \ar[r]^{\eta} & N \ar[r]& 0
        }
        \end{equation*}
        We get $\alpha_0$ since $P_0$ is projective (via \eqref{proj}) since $\eta: Q_0 \to N$ is surjective. This means the right square commutes. Next we examine the diagram
        \begin{equation*}
        \xymatrix {
            P_2 \ar[r] & P_1 \ar[r]^{d_1^P} \ar@{.>}[d]_{\alpha_1} & P_0 \ar[r]^{\e} \ar[d]_{\alpha_0} & M \ar[r] \ar[d]^{\varphi} & 0 \\
            Q_2 \ar[r] & Q_1 \ar[r]_{d_1^Q} \ar@{}[ur]|{\circlearrowleft} & Q_0 \ar[r]^{\eta} \ar@{}[ur]|{\circlearrowleft} & N \ar[r]& 0
        }
        \end{equation*}

        To construct $\alpha_1: P_1 \to Q_1$ we consider $\im \alpha_0 d_1^P \subseteq \im d_1^Q = \ker \eta$ holds because $\e \circ d_1^P = 0$. From the projectivity of $P_1$ we have constructed $\alpha_1$ to satisfy the following diagram:
        \begin{equation*}
        \xymatrix {
            & P_1 \ar@{.>}[dl]_{\alpha_1} \ar[d]^{\alpha_0 d_1^P} \\
            Q_1 \ar[r]_{d_1^Q}& \im d_1^Q \ar[r] & 0
        }
        \end{equation*}
        So the second square commutes. Repeating this construction gives $\alpha_\bullet: P_\bullet \to Q_\bullet$ which is a homomorphism of chain complexes by construction
    \end{proof}

    We have studied a single module, a homomorphsim (two modules). Next we move to three modules.

    \begin{definition}{}{4.23}
        Let $C_\bullet,C'_\bullet,C''_\bullet$ be three chain complexes. A s.e.s of chain complexes
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & C'_\bullet \ar[r]^{f_\bullet} & C_\bullet \ar[r]^{g_\bullet} & C''_\bullet \ar[r] & 0
        }
        \end{equation*}
        
        consists of maps of chain complexes $f_\bullet, g_\bullet$ such that for any fixed $n$
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & C'_n \ar[r]^{f_n} & C_n \ar[r]^{g_n} & C''_n \ar[r] & 0
        }
        \end{equation*}
        is an exact sequence of $A$-modules.
    \end{definition}
    
    \begin{lemma}{Horseshoe Lemma}{4.24}
        Take a short exact sequence $\xymatrix{0 \ar[r]& X\ar[r]^{f} & Y\ar[r]^g & Z\ar[r] & 0}$ and two projective resolutions $\xymatrix {P_\bullet^X \ar[r]^{\e_X} & X \ar[r] & 0}$, $\xymatrix {P_\bullet^Z \ar[r]^{\e_Z} & Z \ar[r] & 0}$. There there exists a projective resolution $\xymatrix {P_\bullet^Y \ar[r]^{\e_Y} & Y \ar[r] & 0}$ and chain maps $f_\bullet:P_\bullet^X \to P_\bullet^Y, g_\bullet: P_\bullet^Y \to P_\bullet^Z$ such that
        \begin{equation*}
        \xymatrix {
            0 \ar[r]& P_\bullet^X \ar[r]^f & \ar[r]^g P_\bullet^Y & P_\bullet^Z \ar[r] & 0 
        }
        \end{equation*} is a short exact sequence of chain complexes.
    \end{lemma}
    According to this lemma, short exact sequences can be "lifted" to projective resolutions. The name "horseshoe" comes from the proof.
    
    \begin{proof}{Proof of Lemma \ref{lem:4.24}}
        We argue via induction. For the base case  We need:
        \begin{itemize}
            \item a projective module $P_0^Y$
            \item a surjective map $\e_Y: P_0^Y \to Y$
            \item a s.e.s. $0 \to P_0^X \to P_0^Y \to P_0^Z \to 0$
        \end{itemize}
        satisfying the following diagram:

        \begin{equation*}
        \xymatrix {
            & & & 0 \ar[d] & 0 \ar[d] & \\
            \dots \ar[r] & P_2^X \ar[r] & P_1^X \ar[r] & P_0^X \ar[d] \ar[r]^{\e_X} & X \ar[d]^f \ar[r] & 0 \\
            & & & P_0^Y \ar[d] \ar[r]^{\e_Y} \ar@{}[ur]|{\circlearrowleft} & Y \ar[d]^g \ar[r] & 0 \\
            \dots \ar[r] & P_2^Z \ar[r] & P_1^Z \ar[r] & P_0^Z \ar[d] \ar[r]^{\e_Z}\ar@{}[ur]|{\circlearrowleft} & Z \ar[d] \ar[r] & 0 \\
            & & & 0 & 0
        }
        \end{equation*}
       First, considering $(3)$, we have to set $P_0^Y:=P_0^X \oplus P_0^Z$ because of Proposition \ref{prop:4.19}, to have a split s.e.s., thus checking $(1)$ of the list (the sum of projective modules is projective using the characterisation from Proposition \ref{prop:4.19} (3)).
       Next we tackle $(2)$. There already is a map $f \circ \e_X: P_0^X \to Y$. To define $\e_Y: P_0^Z \to Y$ we consider the following diagram

       \begin{equation*}
        \xymatrix {
            & P_0^Z \ar@{.>}[dl]_{\lambda} \ar[d]^{\e_Z} \\
            Y \ar[r]_{g}& Z \ar[r]& 0
        }
       \end{equation*}
       Since $P_0^Z$ is projective, this $\lambda$ exists. Now we define
       \begin{equation*}
        \begin{split}
            \e_Y: P_0^X \oplus P_0^Z &\to Y \\
            (x,z) &\mapsto f \e_X(x) + \lambda(z)
        \end{split}
       \end{equation*}

       It remains to show that $\e_Y$ is surjective and the squares commute.
       
       For the first part, take $y \in Y$ and choose $z \in P_0^Z$ such that $\e_Z(z) = g(y)$. Then $g(y - \lambda (z)) = g(y) - g \lambda (z) = \e_Z(z) - \e_Z(z) = 0$, hence $y - \lambda(z) \in \ker g = \im f$. So we write $y-\lambda(z) = f(\e_X(p))$ for some $p \in P_0^X$ since $\e_X$ is surjective. That is to say $\e_Y(p,z) = y$.

       Secondly, the squares commute, since the maps of the two components commute (and the sum is direct).

        For the induction step consider the following diagram
        \begin{equation*}
        \xymatrix {
                        &                          & 0 \ar[d] \\
            \dots \ar[r] & P_{n}^X \ar[r]^{d_n^X} & \ker d_{n-1}^X \ar[r] \ar[d]& 0 \\
            & & \ker d_{n-1}^Y \ar[d] \\
            \dots \ar[r] & P_{n}^Z \ar[r]^{d_n^Z} & \ker d_{n-1}^Z \ar[r] \ar[d] & 0 \\
            & & \coker d_{n-1}^X = \{0\}
        }
        \end{equation*}
        where the vertical row is an exact sequence given by the Snake lemma. We repeat the base case of the induction to get a sequence
        
        \begin{equation*}
        \xymatrix {
                        &  0 \ar[d] & 0 \ar[d] \\
            \dots \ar[r] & P_{n}^X \ar[d]^{\iota_{X}} \ar[r]^{d_n^X} & \ker d_{n-1}^X \ar[r] \ar[d]& 0 \\
            & P_{n}^Y \ar@{}[ur]|{\circlearrowleft} \ar[r] \ar[d]^{\pi_Z}& \ker d_{n-1}^Y \ar[d] \ar[r] & 0 \\
            \dots \ar[r] & P_{n}^Z \ar[r]^{d_n^Z} \ar@{}[ur]|{\circlearrowleft} \ar[d] & \ker d_{n-1}^Z \ar[r] \ar[d] & 0 \\
            & 0 &  0
        }
        \end{equation*}
        Now we glue this diagram together with the one we get from the induction hypothesis and we are done.
    \end{proof}

    The last question we want to answer is: \textit{What is the relation between different projective resolutions?}

    We want to use projective resolutions to study properties of $M$, so we need something which is independent of the choice of the projective resolution. For this, we introduce the notion of homotopy of chain complexes.

\separline{Week 13}
    \begin{definition}{}{4.25}
        Let $C_\bullet$ and $D_\bullet$ be two chain complexes. Two homomorphisms of chain complexes $\alpha_\bullet,\beta_\bullet: C_\bullet \to D_\bullet$ are called \textbf{homotopic}, if there is a sequence (indexed by $i \in \Z$) of $A$-module homomorphisms $s_i:C_i \to D_{i+1}$ such that:
        \begin{equation*}
            \alpha_i - \beta_i = d_{i+1}^D \circ s_i + s_{i-1} \circ d_i^C
        \end{equation*}
    \end{definition}

    This definition will be clearer if we write the maps in a diagram.
    \begin{equation*}
    \xymatrix@C=40pt @R=40pt{
        \dots \ar[r] & C_{i+2} \ar[r]^{d_{i+2}^C} \ar@<-0.5ex>[d]_{\alpha_{i+2}} \ar@<0.5ex>[d]^{\beta_{i+2}} & C_{i+1} \ar[r]^{d_{i+1}^C} \ar@<-0.5ex>[d]_{\alpha_{i+1}} \ar@<0.5ex>[d]^{\beta_{i+1}} \ar@{.>}[dl]_(.4){s_{i+1}} & C_i \ar[r]^{d_i^C} \ar@<-0.5ex>[d]_{\alpha_{i}} \ar@<0.5ex>[d]^{\beta_{i}} \ar@{.>}[dl]_(.4){s_{i}} & C_{i-1} \ar[r] \ar@<-0.5ex>[d]_{\alpha_{i-1}} \ar@<0.5ex>[d]^{\beta_{i-1}} \ar@{.>}[dl]_(.4){s_{i-1}} & \dots \\
        \dots \ar[r] & D_{i+2} \ar[r]\ar[r]_{d_{i+2}^D} & D_{i+1} \ar[r]\ar[r]_{d_{i+1}^D} & D_i \ar[r]\ar[r]_{d_{i}^D} & D_{i-1} \ar[r] & \dots
    }
    \end{equation*}

    This notion comes from algebraic topology. For us the important insight is the following

    \begin{lemma}{}{4.26}
        If two morphisms of complexes $\alpha_\bullet: C_\bullet \to D_\bullet$ are homotopic, then they induce the same map on the homology group. That is to say:
        \begin{equation*}
            H_i(\alpha_\bullet) = H_i(\beta_\bullet): H_i(C_\bullet) \to H_i(D_\bullet)
        \end{equation*}
    \end{lemma}

    \begin{proof}
        Take $z_i \in Z_i(C_\bullet)$. From the definition \ref{lem:4.12}, we get
        \begin{equation*}
        \begin{split}
            H_i(\alpha_\bullet)([z_i]) &= \eckig{\alpha_i(z_i)} = [\beta_i(z_i) + \underbrace{d_{i+1}^D \circ s_i(z_i)}_{\in B_i(D_\bullet)} + s_{i-1} \circ \underbrace{d_i^C(z_i)}_{=0}]z_i = [\beta_i(z_i)] = H_i(\beta_\bullet)([z_i])
        \end{split}
        \end{equation*}
    \end{proof}

    \begin{proposition}{}{4.27}
        Let $P_\bullet \xrightarrow{\e} M \to 0$ and $Q_\bullet \xrightarrow{\eta} N \to 0$ be two projective resolutions and $\varphi: M \to N$ a homomorphism of $A$-modules. Assume that $\alpha_\bullet$ and $\beta_\bullet: P_\bullet \to Q_\bullet$ are two chain maps, both of which are obtained from Proposition \ref{prop:4.22}. Then $\alpha_\bullet$ and $\beta_\bullet$ are homotopic.
    \end{proposition}

    \begin{equation*}
    \xymatrix@C=40pt @R=40pt{
        \dots \ar[r] & P_{2} \ar[r]^{d_{2}^P} \ar@<-0.5ex>[d]_{\alpha_{2}} \ar@<0.5ex>[d]^{\beta_{2}} & P_{1} \ar[r]^{d_{1}^P} \ar@<-0.5ex>[d]_{\alpha_{1}} \ar@<0.5ex>[d]^{\beta_{1}} \ar@{.>}[dl]_(.4){s_{1}} & P_0 \ar[r]^{\e} \ar@<-0.5ex>[d]_{\alpha_{0}} \ar@<0.5ex>[d]^{\beta_{0}} \ar@{.>}[dl]_(.4){s_{0}} & M \ar[r] \ar[d]^{\varphi} & 0\\
        \dots \ar[r] & Q_{2} \ar[r]\ar[r]_{d_{2}^Q} & Q_1 \ar[r]\ar[r]_{d_{1}^Q} & Q_0 \ar[r]\ar[r]_{\eta} & N \ar[r] & 0
    }
    \end{equation*}

    \begin{proof}[Proof of Proposition \ref{prop:4.27}]
        We need to construct $s_i:P_i \to Q_{i+1}$ such that for $i \geq 1$
        \begin{equation*}
            \alpha_i - \beta_i = d_{i+1}^Q \circ s_i + s_{i-1} \circ d_i^P
        \end{equation*}
        and $\alpha_0 - \beta_ 0 = d_1^Q \circ s_0$. The proof is similar to the previous ones. We start from $\alpha_0 - \beta_0$. Using that $P_0$ is projective, we can apply the characterisation \ref{proj}
        \begin{equation*}
        \xymatrix {
            & P_0 \ar[d]^{\alpha_0 - \beta_0} \ar@{.>}[dl]_{s_0} \\
            Q_1 \ar[r]_{d_1^Q} & \im d_1^Q \ar[r] & 0
        }
        \end{equation*}
        to get an $s_0: P_0 \to Q_1$ such that the above diagram commutes. For this to work, we have to show that $\im (\alpha_0 - \beta_0) \subseteq \im d_1^Q = \ker \eta$, i.e. $\eta(\alpha_0 - \beta_0) = 0$. This follows from the assumption that $\eta \alpha_0 = \varphi \e = \eta \beta_0$.

        Next, we look at $\alpha_1 - \beta_1$. Consider the diagram (via \ref{proj})
        \begin{equation*}
        \xymatrix {
            & P_1 \ar[d]^{\alpha_1 - \beta_1 - s_0 d_1^P} \ar@{.>}[dl]_{s_1} \\
            Q_2 \ar[r]_{d_2^Q} & \im d_2^Q \ar[r] & 0
        }
        \end{equation*}
        Again we verify that
        \begin{equation*}
            \im \round{\alpha_1 - \beta_1 - s_0 d_1^P} \subseteq \im d_2^Q = \ker d_1^Q
        \end{equation*}
        i.e. $d_1^Q(\alpha_1 - \eta_1 - s_0 d_1^P) = 0$. We have:
        \begin{equation*}
            d_1^Q(\alpha_1 - \beta_1) - d_1^Q s_0 d_1^P = (\alpha_0 - \beta_0)d_1^P - (\alpha_0 - \beta_0) d_1^P = 0
        \end{equation*}

        Now we can repeat this procedure to obtain the desired homotopy
    \end{proof}

\subsection{Ext}
    Ext is one of the most important objects in homological algebra. Roughly speaking, it tells how hard one can obtain complicated $A$-modules from easy ones. We will see later that this encodes the notion of the global dimension of an algebra.

    Let $M$ and $N$ be two $A$-modules and $P_\bullet \xrightarrow{\e} M \to 0$ be a projective resolutioon of $M$. Applying the functor $\Hom_A(-,N)$ to this resolution yields a cochain complex
    \begin{equation*}
    \xymatrix@C=20pt {
        0 \ar[r] & \Hom_A(P_0,N) \ar[r]^{d_1^*} & \Hom_A(P_1,N) \ar[r]^{d_2^*} & \Hom_A(P_2,N) \ar[r] & \dots
    }
    \end{equation*}
    which we will denote by $\Hom(P_\bullet,N)$.

    This sequence is usually far away from being exact. We set the degree $0$ part to be $\Hom_A(P_0,N)$.

    \begin{definition}{}{}
        For $n \geq 0$ we define $\Ext^n_A(M,N)$ to be the $n$-th cohomology group $H^n\round{\Hom_A(P_\bullet,N)}$ and call it the \textbf{n-th extension space of $\mathbf{N}$ by $\mathbf{M}$}. It is a $K$-vector space by definition.
    \end{definition}

    \textbf{Examples:}
    \begin{itemize}
        \item $\Ext^0_A(M,N) = \ker d_1^*$
        \item $\Ext^1_A(M,N) = \ker d_2^*/\im d_1^*$
    \end{itemize}

    \begin{exercise}{}{22}
        Show that $\Ext_A^0(M,N) \cong \Hom_A(M,N)$.
    \end{exercise}

    \begin{proof}
        Since the contravariant Homfunctor is left-exact as shown in \ref{prop:4.13}, applying $\Hom(-,N)$ to the exact sequence $P_\bullet \xrightarrow{\e} M \to 0$ yields an exact sequence
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & \Hom_A(M,N) \ar[r]^{\e^*} & \Hom_A(P_0,N) \ar[r]^{d_1^*} & \Hom_A(P_1,N) \ar[r] & \dots
        }
        \end{equation*}
        That means $\Ext^0(M,N) = \ker d_1^* = \im \e^* \cong \Hom_A(M,N)$.
    \end{proof}

    The first problem arising from the definition is where $\Ext_A^n(M,N)$ depends on the choice of the projective resolution $P_\bullet \to M \to 0?$

    \begin{theorem}{}{4.29}
        Let $R_\bullet: P_\bullet \to M \to 0$ and $R_\bullet': Q_\bullet \to M \to 0$ two projective resolutions of $M$. Then for any $A$-module $N$ and $i \geq 0$:
        \begin{equation*}
            H^i(\Hom_A(P_\bullet,N)) \cong H^i(\Hom_A(Q_\bullet,N))
        \end{equation*}
        In particular, up to an isomorphism of vector spaces, the $K$ vector space $\Ext^n_A(M,N)$ does not depend on the choice of the projective resolution of $M$.
    \end{theorem}

    \begin{proof}
        We apply Proposition $\ref{prop:4.22}$ to $\varphi = \id_M$ and the two projective resolutuions $P_\bullet$ and $Q_\bullet$ of $M$. Then we obtain two chain maps $\alpha_\bullet: R_\bullet \to R_\bullet'$ and $\beta_\bullet: R_\bullet' \to R_\bullet$. So we have two chain maps $\beta_\bullet \circ \alpha_\bullet, \id_\bullet: R_\bullet \to R_\bullet$. By Proposition \ref{prop:4.27}, $\beta_\bullet \circ \alpha_\bullet$ and $\id_\bullet$ are homotopic.

        Applying $\Hom_A(-,N)$, we obtain the following diagram
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & \Hom_A(M,N) \ar[r] \ar[d]^{\id_M^*} & \Hom_A(P_0,N) \ar[r] \ar@<-0.5ex>[d]_{\id_0^*} \ar@<0.5ex>[d]^{\alpha_0^* \circ \beta_0^*} & \Hom_A(P_1,N) \ar[r] \ar@<-0.5ex>[d]_{\id_1^*} \ar@<0.5ex>[d]^{\alpha_1^* \circ \beta_1^*} & \dots \\
            0 \ar[r] & \Hom_A(M,N) \ar[r] & \Hom_A(P_0,N) \ar[r] \ar@{.>}[ur]_{s_0^*} & \Hom_A(P_1,N) \ar[r] & \dots
        }
        \end{equation*}
        It follows (since the functor $\Hom_A(-,N)$ is additive) that $\alpha_\bullet^* \circ \beta_\bullet^*$ and $\id_\bullet^*$ are homotopic. So they induce the same map on on cohomology by the ''co''-version (principal of duality) of Lemma \ref{lem:4.26} i.e.
        \begin{equation*}
            H^i(\alpha_\bullet^* \circ \beta_\bullet^*) = H^i(\id): H^i\round{\Hom_A(P_\bullet,N)} \to H^i\round{\Hom_A(P_\bullet,N)}
        \end{equation*}
        Swapping the role of $\alpha$ and $\beta$, and applying Lemma \ref{lem:4.12}(2) we obtain:
        \begin{equation*}
        \begin{split}
            H^i(\alpha_\bullet^*) \circ H^i(\beta_\bullet^*) &= \id_{H^i\round{\Hom_A(P_\bullet,N)}} \\
            H^i(\beta_\bullet^*) \circ H^i(\alpha_\bullet^*) &= \id_{H^i\round{\Hom_A(Q_\bullet,N)}}
        \end{split}
        \end{equation*}
        We have shown that $H^i\round{\Hom_A(P_\bullet,N)} \cong H^i\round{\Hom_A(Q_\bullet,N)}$ and hence $\Ext_A^n(M,N)$ is independent of the choice of the projective resolution.
    \end{proof}

    We finish this section with a description of $\Ext_A^1(M,N)$ as extensions of modules (extensions of $N$ by $M$). This is usually called the Yoneda construction of $\Ext_A^1$. In fact, such a construction exists for $\Ext^n_A$ (see \cite{Weibel.2010}).

    We let $\Ext(X,Z)$ denote the set of all s.e.s $0 \to X \to Y \to Z \to 0$.

    \begin{definition}{}{4.30}
        Two s.e.s. $\zeta: 0 \to X \xrightarrow{f} Y \xrightarrow{g} Z \to 0$ and $\zeta: 0 \to X \xrightarrow{f'} Y' \xrightarrow{g'} Z \to 0$ are called equivalent if there exists a morphism $\varphi: Y \to Y'$ such that the following diagram commutes:
        \begin{equation*}
        \xymatrix {
            0 \ar[r] & X \ar[r] \ar@{=}[d] & Y \ar[r]^f \ar[d]^{\varphi} & Z \ar[r] \ar@{=}[d] & 0 \\
            0 \ar[r] & X \ar[r] \ar[r]^{f'} & Y' \ar[r]^{g'} & Z \ar[r] & 0
        }
        \end{equation*}
        We denote write $\zeta \sim \zeta'$ if $\zeta$ is equivalent to $\zeta'$. This is an equivalence relation (check this!).
    \end{definition}

    \begin{lemma}{}{4.31}
        In the above definition, $\varphi$ is an isomorphism.
    \end{lemma}

    \begin{proof}
        Follows immediately from the $5$-Lemma.
    \end{proof}

    The following theorem gives an explicit description of $\Ext^1$ as exact sequences.

    \begin{theorem}{Yoneda}{4.32}
        There exists a bijection between $\Ext(X,Z)/\sim$ and $\Ext^1(Z,X)$. Moreover, one can define an abelian group structure on $\Ext(X,Z)/\sim$ such that the bijection is an isomorphism of abelian groups. The zero element in $\Ext(X,Z)/\sim$ is the class of split extensions.
    \end{theorem}

    We will not prove this theorem. For a reference, see Mitchell's Theory of Categories \cite{Mitchell.1971} chapter VII (or ask me, Felix, I wrote a seminar essay about this).

\section{Long exact sequences and applications}
    Long exact sequences are tools for the computation of homology or cohomology. Since $\Ext^n$ is a kind of cohomology, long exact sequences are the most important tools of compute $\Ext^n$.

    Recall that for a module homomorphism $f: M \to N$ its \textbf{cokernel} is defined as $\coker f := N/\im f$.

    We start with the famous snake Lemma. The proof of the Lemma is standard diagram chasing.

    \begin{lemma}{Snake Lemma}{snake}
        Given a commutative diagram in $A\Mod$ 
        \begin{equation*}
        \xymatrix {
            & X \ar[r]^f \ar[d]^\alpha & Y \ar[r]^g \ar[d]^\beta & Z \ar[r] \ar[d]^\gamma & 0 \\
            0 \ar[r] & X' \ar[r]^{f'} & Y' \ar[r]^{g'} & Z'
        }
        \end{equation*}
        where the rows are exact, one can construct an exact sequence
        \begin{equation*}
        \xymatrix {
            \ker \alpha \ar[r]^f & \ker \beta \ar[r]^g & \ker \gamma \ar[r]^\partial & \coker \alpha \ar[r]^{f'} & \coker \beta \ar[r]^{g'} & \coker \gamma
        }
        \end{equation*}
    \end{lemma}

    \begin{proof}[Proof by "It's My Turn":]
        \url{https://www.youtube.com/watch?v=etbcKWEKnvg} :)
    \end{proof}

    The map $\partial: \ker \gamma \to \coker \alpha$ is called the connecting homomorphism or connecting map for simplicity.

    With the help of the Snake Lemma \ref{lem:snake}, we can prove the following.

    \begin{proposition}{}{4.34}
        \begin{enumerate}
            \item
                Let $0 \to C_\bullet' \xrightarrow{f_\bullet} C_\bullet \xrightarrow{g_\bullet} C_\bullet'' \to 0$ be a s.e.s. of chain complexes. Then it gives rise to a long exact sequence
                \begin{equation*}
                \xymatrix {
                    \dots \ar[r] & H_{n+1}(C_\bullet'') \ar[r]^{\partial_{n+1}} & H_n(C_\bullet') \ar[r]^{H_n(f_n)} & H_n(C_\bullet) \ar[r]^{H_n(g_n)} & H_n(C_\bullet'') \ar[r]^(.45){\partial_{n}} & H_{n-1}(C_\bullet') \ar[r] & \dots
                }
                \end{equation*}
            \item
                Let $0 \to C^\bullet \xrightarrow{f^\bullet} D^\bullet \xrightarrow{g^\bullet} E^\bullet \to 0 $ be a s.e.s. of cochain complexes. Then it gives rise to a long exact sequence
                \begin{equation*}
                    \xymatrix {
                        \dots \ar[r] & H^{n-1}(E^\bullet) \ar[r]^{\partial^{n-1}} & H^n(C^\bullet) \ar[r]^{H^n(f^n)} & H^n(D^\bullet) \ar[r]^{H^n(g^n)} & H^n(E^\bullet) \ar[r]^{\partial^{n}} & H^{n-1}(C^\bullet) \ar[r] & \dots
                    }
                \end{equation*}
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        We only prove (1). To prove it, we try to find the snake. We find the following commutative diagram with exact rows (verify this!)
        \begin{equation*}
            \xymatrix {
                & \faktor{C_n'}{B_n(C_\bullet')} \ar[r]^{f_n} \ar[d]^{d_n^{C'}} & \faktor{C_n}{B_n(C_\bullet)} \ar[r]^{g_n} \ar[d]^{d_n^C} & \faktor{C_n''}{B_n(C_\bullet'')} \ar[r] \ar[d]^{d_n^{C''}} & 0 \\
                0 \ar[r] & Z_{n-1}(C_\bullet') \ar[r]_{f_{n-1}} \ar@{}[ur]|{\circlearrowleft} & Z_{n-1}(C_\bullet) \ar[r]_{g_{n-1}} \ar@{}[ur]|{\circlearrowleft} & Z_{n-1}(C_\bullet'')
            }
        \end{equation*}
        We have $\ker d_n^{C} = H_n(C_\bullet)$ and $\coker d_n^C = Z_{n-1}(C_\bullet)/\im d_n = Z_{n-1}(C_\bullet)/B_{n-1}(C_\bullet) = H_{n-1}(C_\bullet)$.

        The Snake Lemma \ref{lem:snake} gives an exact sequence

        \begin{equation*}
                \xymatrix {
                    \dots \ar[r] & H_{n+1}(C_\bullet'') \ar[r]^{\partial_{n+1}} & H_n(C_\bullet') \ar[r]^{H_n(f_n)} & H_n(C_\bullet) \ar[r]^{H_n(g_n)} & H_n(C_\bullet'') \ar[r]^(.45){\partial_{n}} & H_{n-1}(C_\bullet') \ar[r] & \dots
                }
                \end{equation*}
    \end{proof}

    Now we can deduce the long exact sequence of $\Ext$.

    \begin{corollary}{}{4.35}
        Let $0 \to X \to Y \to Z \to 0$ be a s.e.s. of $A$-modules. then there exist two long exact sequences of $\Ext$ for any $A$-module $M$:
        \begin{equation*}
        \xymatrix {
        0 \ar[r] & \Hom_A(M,X) \ar[r] & \Hom_A(M,Y) \ar[r] & \Hom_A(M,Z) \ar[r] & \Ext_A^1(M,X) \ar[r] & \dots 
        }
        \end{equation*}
        and
        \begin{equation*}
        \xymatrix{
            0 \ar[r] & \Hom_A(Z,M) \ar[r] & \Hom_A(Y,M) \ar[r] & \Hom_A(X,M) \ar[r] & \Ext_A^1(Z,M) \ar[r] & \dots
        }
        \end{equation*}
    \end{corollary}

    \begin{proof}
        We first show that the first long exact sequence exists. For this, we take a projective resolution $P_\bullet \to M \to 0$. Since the modules in $P_\bullet$ are projective, we have an exact sequence of cochain complexes
        \begin{equation*}
        \xymatrix{
            0 \ar[r] & \Hom(P_\bullet,X) \ar[r] & \Hom(P_\bullet,Y) \ar[r] & \Hom(P_\bullet,Z) \ar[r] & 0
        }
        \end{equation*}
        Applying Proposition \ref{prop:4.34} yields
        \begin{equation*}
        \xymatrix{
            0 \ar[r] & H^0 \round{\Hom(P_\bullet,X)} \ar[r] & H^0 \round{\Hom(P_\bullet,Y)} \ar[r] & H^0 \round{\Hom(P_\bullet,Z)} \ar[r] & H^1 \round{\Hom(P_\bullet,X)} \ar[r]  & \dots
        }
        \end{equation*}

        which by definition of $\Ext$ gives
        \begin{equation*}
        \xymatrix {
        0 \ar[r] & \Hom_A(M,X) \ar[r] & \Hom_A(M,Y) \ar[r] & \Hom_A(M,Z) \ar[r] & \Ext_A^1(M,X) \ar[r] & \dots 
        }
        \end{equation*}

        Now we construct the second one.

        Applying the horseshoe Lemma \ref{lem:4.24} to the exact sequence $0 \to X \to Y \to Z \to 0$ gives rise to an exact sequence of projective resolutions $0 \to P^X_\bullet \to P^Y_\bullet \to P^Z_\bullet \to 0$ where $P^X_\bullet, P^Y_\bullet, P^Z_\bullet$ are the projective resolutions of $X,Y,Z$ respectively. Applying $\Hom(-,M)$ to the resolution gives an exact sequence of cochain complexes
        \begin{equation*}
            \xymatrix{
                0 \ar[r] & \Hom(P_\bullet^Z,M) \ar[r] & \Hom(P_\bullet^Y,M) \ar[r] & \Hom(P_\bullet^X,M) \ar[r] & 0
            }
        \end{equation*}
        Now it suffices to apply Proposition \ref{prop:4.34} to cohomology to obtain the second long exact sequence of $\Ext$.
    \end{proof}

    As a quick application, we obtain the following characterization of projective modules:
    \begin{corollary}{}{4.36}
        The following statements are equivalent:
        \begin{enumerate}
            \item $P$ is a projective $A$-module
            \item for any $A$-module $M$, $\Ext_A^1(P,M) = \{0\}$
            \item for any $A$-module $M$ and any $n \geq 1$, $\Ext_A^n(P,M) = \{0\}$
        \end{enumerate}
    \end{corollary}

    \begin{proof} We show that $1 \Rightarrow 3 \Rightarrow 2 \Rightarrow 1$.
        \begin{itemize}
            \item[(1)$\Rightarrow$(3):]
                If $P$ is projective, $\underbrace{0 \to P}_{P_\bullet} \to P \to 0$ is a projective resolution of $P$. The cochain complex $\Hom(P_\bullet,M)$ is then
                \begin{equation*}
                \xymatrix{
                    \Hom(P,M) \ar[r]^0 & {\underbrace{\Hom(0,M)}_{=\{0\}}} \ar[r] & 0 \ar[r] & \dots
                }
                \end{equation*}
                Therefore, $\Ext_A^n(P,M) = \ker 0 / \im 0 = \{0\}$.
            \item[(3)$\Rightarrow$(2):]
                clear.
            \item[(2)$\Rightarrow$(1):]
                Assume $\Ext_A^1(P,M) = \{0\}$ for any $A$-module $M$. Now take an exact sequence $0 \to X \to Y \to Z \to 0$. Then Proposition \ref{prop:4.34} gives a long exact sequence
                \begin{equation*}
                \xymatrix{
                    0 \ar[r] & \Hom_A(P,X) \ar[r] & \Hom_A(P,Y) \ar[r] & \Hom_A(P,Z) \ar[r] & \Ext_A^1(P,X) = \{0\}
                }
                \end{equation*}
                Therefore $\Hom_A(P,-)$ is exact, hence $P$ is a projective $A$-module.
        \end{itemize}
    \end{proof}
    

    
    
    
    
\printbibliography
\end{document}

        

